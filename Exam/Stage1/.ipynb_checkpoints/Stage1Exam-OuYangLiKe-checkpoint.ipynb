{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习第九期第一阶段考试\n",
    "#### 考试说明:\n",
    "- 起止时间：请同学在2018年7月4日至7月10日期间完成，最晚提交时间（7月10日24时之前）结束，<b>逾期不接受补考,该考试分数计入平时成绩</b>\n",
    "- 考试方式：请同学将该试卷进行复制后，改名为自己姓名后，如State1Exam-WangWei.ipynb，<b>移动</b>至/0.Teacher/Exam/Stage1/目录下后，进行答题。\n",
    "- 注意事项：为确保同学们真正了解自身对本周课程的掌握程度，<font color=red><b>请勿翻阅，移动，更改</b></font>其它同学试卷。如发现按0分处理\n",
    "- 请同学在下方同学姓名处填写自己的姓名，批改人和最终得分不用填写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名:欧阳立珂\n",
    "- 批改人：   \n",
    "- 最终得分:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问答题(共10题，每题10分，共计100分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.请说明线性分类器与非线性分类器有哪些区别，具体应用场景有哪些不同？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "线性的可以认为是1次曲线，比如y=ax+b ,即成一条直线，也可以是z=ax+by，是一个平面，总之最高项是一次项\n",
    "非线性的可以认为是2次以上的曲线，比如y=ax^2+bx+c，(x^2是x的2次方)，二次，三次等函数关系，也可能是没有关系，即不为直线的即可。\n",
    "对于维度很高的特征，选择非线性分类器:knn,决策树,svm(非线性核)等.\n",
    "对于维度较低的特征，则选择线性分类器:lr,svm(线性核)等."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.逻辑回归中为什么常常要对特征进行离散化?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "离散化后,模型会更稳定,具有很强的鲁棒性.举个例子:加入要对公交车上的老人和小孩让座,若不进行离散化,则 y = w*x ,x为被让座人的年龄,w为权重,如果年龄就是一个维度, 则根据等式显然x越大,y就越大(代表越要让座),也就是说根据这个判断,年龄越大被让座的可能性越大? 实际情况中显然不是这样.\n",
    "这里就需要把年龄离散化了,将年龄x扩展为x_1,x_2,..x_n多个特征,假设x_1为0到10岁,x_3为60岁以上,我们只需要对这两个扩展后的特征加大权重即可,即最后的y = w1*x_1 +w2*x_2 +w3*x_3..,模型只需要把年龄的权重w1和w3强化,其他权重wn弱化,这样预测出来的结果才是最接近真实情况.\n",
    "使用离散化特征后，一个特征变成了多个，权重也变为多个，大大的提升了模型的泛化能力.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.随机森林如何处理缺失值？随机森林如何评估特征重要性？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* 随机森林可以预测缺省值，将数据集分成两个数据集：一个数据集没有缺失值，另一个数据集有缺失值。没有缺失值的数据集作为模型的训练集，\n",
    "含有缺失值的数据集作为测试集，含有缺失值的变量作为目标变量。然后基于训练集建立模型，并应用于测试集。\n",
    "\n",
    "* 用随机森林进行特征重要性评估的思想比较简单，主要是看每个特征在随机森林中的每棵树上做了多大的贡献，\n",
    "然后取平均值，最后比较不同特征之间的贡献大小。通常可以用基尼指数（Gini index）或者袋外数据（OOB）\n",
    "错误率作为贡献的评价指标来衡量。 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 请说明梯度下降算法如何实现，以及它与牛顿法的不同？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* 在线性回归中,拟合的函数为(假设数据集为m个样本,n个特征,其中xj为样本的第j个特征,x0为样本增加的等于1的特征):\n",
    "$$h_\\theta(x_1,x_2,...,x_n)= \\theta_0x_0+\\theta_1x_1+...+\\theta_nx_n$$\n",
    "yi为第i个样本的真实label,则目标函数为:\n",
    "$$J(\\theta)=\\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}-y^{(i)})^2$$\n",
    "要使得J(θ)取极小值,可对参数θ求一阶导,然后用梯度下降法(负梯度方向),迭代参数θ(θ0,θ1,...,θj,...θn),α为学习率:\n",
    "\n",
    "$$\\frac {\\partial J(\\theta)}{\\partial θ_j}=\\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}-y^{(i)}){x_j}^{(i)}$$\n",
    "\n",
    "$$θ_j := θ_j - \\alpha\\frac {\\partial J(\\theta)}{\\partial θ_j}$$\n",
    "当J(θ)收敛到最小值时,对θ的一阶导将等于0,而此时的θ不再变化,为最优参数.\n",
    "\n",
    "* 而牛顿法是根据代价函数J(θ)的二阶导数信息，自动计算出了合适的学习率，因此有更快的迭代速度\n",
    "牛顿法：\n",
    "$$\n",
    "θ_j := θ_j- \\frac{\\partial^2 J(\\theta)}{\\partial^2(\\theta_i)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 简要谈下您理解的的机器学习领域的正则化？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "正则化是结构风险最小化策略的实现,是在经验风险上加一个正则化项惩罚项,为了防止模型过拟合而使用,正则化项的值一般是随着模型复杂度的\n",
    "增加而增大.比如,正则化项可以是模型参数向量的L1和L2范数.回归问题中,目标函数为平方损失,使用参数的L1和L2范数作为正则化项."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6 带核的SVM为什么能分类非线性问题？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "对于非线性问题,可以通过核函数将数据的维度转化成更高维度,这样就可以在高维度的特征空间中学习svm,将数据进行线性切分."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7 请举例有哪些常用核函数，以及核函数的条件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "线性核函数  \n",
    "$$k(x_{i},x_{j}) = x_{i}^Tx_{j}$$\n",
    "多项式核函数：  \n",
    "$$k(x_{i},x_{j}) = ((x_{i}^Tx_{j})^{d}$$\n",
    "高斯核函数：  \n",
    "$$k(x_{i},x_{j}) = exp(-\\frac{||x_i-x_j||^2}{2\\delta^2})$$\n",
    "拉普拉斯核:\n",
    "$$k(x_{i},x_{j}) = exp(-\\frac{||x_i-x_j||}{2\\delta})$$\n",
    "sigmoid核函数:  \n",
    "$$k(x_{i},x_{j}) = tanh(\\beta x_{i}^Tx_{j} + \\theta )$$\n",
    "\n",
    "条件:函数是对称函数,并且其对应的核矩阵半正定,就能作为核函数使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.什么是偏差与方差？当你模型受到低偏差和高方差困扰时，如何解决？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "偏差是预测值（估计值）的期望与真实值之间的差距。偏差越大，越偏离真实数据，\n",
    "方差时预测值的变化范围，离散程度，也就是离其期望值的距离。方差越大，数据的分布越分散.\n",
    "\n",
    "当模型出现底偏差和高方差时,说明已经过拟合,解决方法为增大正则化项的值或者减少特征维度或加大数据量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.请说明熵、联合熵、条件熵、相对熵、互信息的定义（要求公式），以及您对这些定义的理解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "熵代表了随机事件信息量的期望,是随机变量不确定性的度量,熵越大,数据信息越不稳定,不确定性就越大,公式如下:\n",
    "$$H(X) = -\\sum_{i=1}^{n}P(x_{i}) logP(x_{i})$$\n",
    "联合熵是事件x和y,一起的不确定性度量,同理也是越大越不稳定:\n",
    "$$H(X,Y) = -\\sum p(x,y)logp(x,y) $$\n",
    "条件熵是在x发生的前提下,y发生的不确定性度量:\n",
    "$$H(X|Y) = H(X,Y) - H(Y)= -\\sum_{x,y}^{ } p(x,y)logp(x|y)$$\n",
    "相对熵是度量x中取值的两个概率分布p(x)和q(x)距离的度量,是p(x)和q(x)不相似程度的度量：  \n",
    "$$D(p||q) = \\sum_{x}^{ }p(x)log\\frac{p(x)}{q(x)} = E_{p(x)}log\\frac {p(x)}{q(x)}$$\n",
    "互信息是在x确定的条件下,对y不确定性的减少,也就是信息增益：  \n",
    "$$I(X,Y) = H(Y)-H(Y|X)=\\sum_{x,y}^{ } p(x,y)log\\frac{p(x,y)}{p(x)p(y)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 请简要说明您对EM算法的理解，并列举有哪些常用的采用EM 算法求解的模型？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* 在求解概率模型的时候，如果需要的变量都是观测变量，不涉及到隐藏变量的话，可以使用极大似然或者贝叶斯估计来求解模型的参数。\n",
    "比如：对于单高斯模型来讲，如果知道观测变量，那么就可以使用极大似然或者最小均方误差来估计高斯模型的均值和方差。\n",
    "如果模型同时包含观察变量和隐藏变量的话，传统的方法不能完成模型的估计，此时就需要引入EM算法。\n",
    "比如：对于混合高斯模型来讲，除了需要估计高斯模型的均值和方差，还涉及到每个高斯模型的权重信息，\n",
    "这个权重信息就可以认为是隐变量，所以一般使用EM来求解GMM的参数。\n",
    "\n",
    "\n",
    "\n",
    "* EM 算法应用于求解高斯混合模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>####答卷结束####</h1></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
