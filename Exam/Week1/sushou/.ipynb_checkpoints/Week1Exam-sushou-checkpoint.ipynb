{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习九期第一周(机器学习基础)考试\n",
    "#### 考试说明:\n",
    "- 起止时间：请同学在2018年6月25日至7月2日期间完成，最晚提交时间（7月1日24时之前）结束，<b>逾期不接受补考,该考试分数计入平时成绩</b>\n",
    "- 考试方式：请同学<font color=red><b>拷贝</b></font>该试卷至自己姓名的目录后，将文件更名，即上同学姓名拼音后进行作答。格式：Week1Exam-WangWei.ipynb\n",
    "- 提交格式：请同学新建自己姓名全拼的文件夹，将该试卷，数据文件，zip文件等相关考试文件，放置此目录下。将该目录<b>移动</b>至/0.Teacher/Exam/1/目录下\n",
    "- 注意事项：为确保同学们真正了解自身对本周课程的掌握程度，<font color=red><b>请勿翻阅，移动，更改</b></font>其它同学试卷。如发现按0分处理\n",
    "- 请同学在下方同学姓名处填写自己的姓名，批改人和最终得分不用填写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名: 苏收\n",
    "- 批改人：   \n",
    "- 最终得分:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问答题(共10题，每题10分，共计100分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.请分别解释统计机器学习中的输入/输出空间，特征空间，假设空间与参数空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A. 输入与输出空间：在监督学习中，将输入与输出所有可能取值的集合，分别称为输入与输出空间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B. 每个具体的输入就是一个实例，通常由特征向量表示，所有的特征向量存在的空间称为特征空间。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C. 监督学习中，输入空间到输出空间的映射（即模型）的集合，称为假设空间。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D. 监督学习过程中，学习到的条件概率分布或决策函数，是假设空间中一组概率分布或决策函数的线性组合，线性组合的参数向量Θ，取值于n维的欧氏空间，这个空间称为参数空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.什么是损失函数，并举例有哪些常用的损失函数？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "损失函数是一种衡量损失和错误程度的函数，在机器学习中用于度量模型一次预测的好坏。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "常见的损失函数有\n",
    "* 0-1 损失函数;     \n",
    "$ L(Y,f(X))=\\begin{cases}\n",
    " &1, \\text{ if } Y\\neq  f(X)\\\\ \n",
    " &0, \\text{ if } Y= f(X)\n",
    "\\end{cases} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 平方损失函数；   \n",
    "$ L(Y,f(X))=(Y - f(X))^2 $\n",
    "* 绝对损失函数；   \n",
    "$ L(Y,f(X))= |Y - f(X)| $\n",
    "* 对数损失函数；   \n",
    "$ L(Y,P(Y|X)) = -log P(Y|X) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3请结合公式进行说明结构风险最小化的含义？"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$ R_{srm}(f)=\\frac{1}{N}\\sum_{i=1}^{n}L(y_{i},f(x_{i}))+\\lambda J(f) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结构化风险最小化，是为了防止过拟合而提出的策略，结构风险在经验风险上加上表示模型复杂度的惩罚项。   \n",
    "假设结构风险的定义如上述公式，J(f）称为模型的复杂度，模型越复杂，复杂度J(f)越大，复杂度表示了对复杂模型的惩罚。   \n",
    "$ \\lambda \\geq 0 $ 是用于权衡经验风险和模型复杂度的系数。   \n",
    "结构风险小要求，经验风险和模型复杂度同时小。   \n",
    "结构风险最小化策略认为结构风险最小的模型，就是最优化模型。所以求最优模型成了求解如下结构风险最小化问题：\n",
    "$ R_{srm}(f)=min_{f\\epsilon F} \\    \\frac{1}{N}\\sum_{i=1}^{n}L(y_{i},f(x_{i}))+\\lambda J(f) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.谈谈什么是生成式模型与判别式模型，以及各自特点？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "生成方法是由数据学习联合概率分布P(X,Y)，然后求出条件概率分布P(Y|X)作为预测模型，即生成模型   \n",
    "$ P(Y|X) = \\frac{P(X,Y)}{P(X)} $   \n",
    "生成方法的特点是，可以还原出联合概率分布P(x,y)，学习收敛速度快，当存在隐变量时，仍可以使用生成方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "判别方法是由数据直接学习决策函数f(X)或者条件概率分布P(X,Y)作为预测的模型，即判别模型。\n",
    "判别方法的特点是，直接学习决策函数f(X)或者条件概率分布P(X,Y)，直接面对预测，往往学习的准确率更高，   \n",
    "由于直接学习P(x,y)或f(X)，可以对数据进行各种程序的抽象，定义特征并使用特征，因此可以简化学习问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.请解释范数，并写出L1,L2范数的定义公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$ \\left \\| W \\right \\|_{1} =\\sum_{i}^{n}\\left | w_{i} \\right |  $   \n",
    "L1 范数为向量中各个元素的绝对值之和。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\left \\| W \\right \\|_{2} =\\sqrt{\\sum_{i}^{n}{ w_{i}^2}}   $   \n",
    "L2范数为向量中各个元素的平方和求根。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 一个是绝对值最小，一个是平方最小：\n",
    "\n",
    "L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. 什么是信息熵，它的公式是？什么是信息增益?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "信息熵用来描述信源的不确定度，其公式为：   \n",
    "$ H(x)=-\\sum_{x\\epsilon X}P(x)\\log P(x) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "信息增益：特征A对训练数据集D的信息增益g(D,A)定义为集合D的信息熵H(D)与特征A条件下D的经验条件熵H(D|A)之差，即   \n",
    "$ g(D,A)=H(D)-H(D|A) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. 贝叶斯公式是什么？您如何理解贝叶斯思想？它与频率派有哪些区别？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "贝叶斯公式：\n",
    "$ P(B_{i}|A)=\\frac{P(B_{i})P(A|B_{i})}{\\sum_{j}^{n}P(B_{j})P(A|B_{j})} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "贝叶斯学派并不从试图刻画「事件」本身，而从「观察者」角度出发，是通过先验概率和条件概率，来推断联合概率分布，在获得了新的知识后（先验概率和条件概率），可以更新\"观察者\"对事物发生概率的推断。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "频率学派从「自然」角度出发，试图直接为「事件」本身建模，即事件A在独立重复试验中发生的频率趋于极限p，那么这个极限就是该事件的概率。举例而言，想要计算抛掷一枚硬币时正面朝上的概率，我们需要不断地抛掷硬币，当抛掷次数趋向无穷时正面朝上的频率即为正面朝上的概率。类似于蒙特卡洛方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.请从最大似然的角度去解释逻辑回归,以及逻辑回归的损失函数是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "最大似然法的主要思想是，已知一个概率分布（但参数未知）的情况下，根据现有的数据样本来估计概率分布的参数，估计的目标是使似然函数最大化。   \n",
    "逻辑回归的本质，就是根据数据样本估计逻辑斯蒂分布函数的参数，使得似然函数最大化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不考虑正则化的情况下，损失函数为：   \n",
    "$ cost(h_{\\theta }(x),y)=-\\sum_{i=1}^{m}[y_{i}\\log h_{\\theta }(x) + (1-y_{i})\\log(1-h_{\\theta }(x))] $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.试析Min-Max与Z-Score这两种数据缩放各自特点，和为什么树形结构不需要做缩放？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 一、min-max标准化（Min-Max Normalization）\n",
    "\n",
    "也称为离差标准化，是对原始数据的线性变换，使结果值映射到[0 - 1]之间。   \n",
    "$ x*=\\frac{x-min}{max-min} $   \n",
    "其中max为样本数据的最大值，min为样本数据的最小值。这种方法有个缺陷就是当有新数据加入时，可能导致max和min的变化，需要重新定义。   \n",
    "#### 二、Z-score标准化方法\n",
    "这种方法给予原始数据的均值（mean）和标准差（standard deviation）进行数据的标准化。经过处理的数据符合标准正态分布，即均值为0，标准差为1，转化函数为：\n",
    "$ x*=\\frac{x-\\mu }{\\sigma } $   \n",
    "其中μ为所有样本数据的均值，$ \\sigma $为所有样本数据的标准差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当模型算法里面有没关于对距离的衡量，没有关于对变量间标准差的衡量时，比如决策树，采用算法里面没有涉及到任何和距离等有关的，所以在做决策树模型时，通常是不需要将变量做标准化的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. 请解释下随机森林和Xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### 随机森林\n",
    "随机森林通过对样本和特征进行有放回的多次抽样，随机产生多个训练数据的子集，根据这些训练数据子集训练出多个决策树模型，称为随机森林。   \n",
    "最终根据森林中多个数的预测结果进行投票、平均值等方式产生预测结果。   \n",
    "##### XGboost\n",
    "Gradient boosting 是 boosting 的其中一种方法，所谓 Boosting ，就是将弱分离器 f_i(x) 组合起来形成强分类器 F(x) 的一种方法。   \n",
    "Gradient boosting 就是通过加入新的弱学习器，来努力纠正前面所有弱学习器的残差，最终这样多个学习器相加在一起用来进行最终预测，准确率就会比单独的一个要高。之所以称为 Gradient，是因为在添加新模型时使用了梯度下降算法来最小化的损失。   \n",
    "XGBoost 就是对 gradient boosting decision tree 的实现，但是一般来说，gradient boosting 的实现是比较慢的，因为每次都要先构造出一个树并添加到整个模型序列中。\n",
    "而 XGBoost 的特点就是计算速度快，模型表现好，这两点也正是这个项目的目标。\n",
    "表现快是因为它具有这样的设计：\n",
    "\n",
    "    Parallelization：\n",
    "    训练时可以用所有的 CPU 内核来并行化建树。\n",
    "    Distributed Computing ：\n",
    "    用分布式计算来训练非常大的模型。\n",
    "    Out-of-Core Computing：\n",
    "    对于非常大的数据集还可以进行 Out-of-Core Computing。\n",
    "    Cache Optimization of data structures and algorithms：\n",
    "    更好地利用硬件。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>####答卷结束####</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本周课程意见反馈(非必答)\n",
    "请同学围绕以下两点进行回答：\n",
    "- 自身总结：您自己在本周课程的学习，收获，技能掌握等方面进行总结，包括自身在哪些方面存在哪些不足，欠缺，困惑。作为将来回顾学习路径时的依据。\n",
    "- 课程反馈：也可以就知识点，进度，难易度，教学方式，考试方式等等进行意见反馈，督促我们进行更有效的改进，为大家提供更优质的服务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$\\phi \\Phi \\emptyset$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
