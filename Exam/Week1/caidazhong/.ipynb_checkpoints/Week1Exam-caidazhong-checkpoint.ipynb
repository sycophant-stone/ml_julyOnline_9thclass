{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习九期第一周(机器学习基础)考试\n",
    "#### 考试说明:\n",
    "- 起止时间：请同学在2018年6月25日至7月2日期间完成，最晚提交时间（7月1日24时之前）结束，<b>逾期不接受补考,该考试分数计入平时成绩</b>\n",
    "- 考试方式：请同学<font color=red><b>拷贝</b></font>该试卷至自己姓名的目录后，将文件更名，即上同学姓名拼音后进行作答。格式：Week1Exam-WangWei.ipynb\n",
    "- 提交格式：请同学新建自己姓名全拼的文件夹，将该试卷，数据文件，zip文件等相关考试文件，放置此目录下。将该目录<b>移动</b>至/0.Teacher/Exam/1/目录下\n",
    "- 注意事项：为确保同学们真正了解自身对本周课程的掌握程度，<font color=red><b>请勿翻阅，移动，更改</b></font>其它同学试卷。如发现按0分处理\n",
    "- 请同学在下方同学姓名处填写自己的姓名，批改人和最终得分不用填写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名:蔡大忠\n",
    "- 批改人：   \n",
    "- 最终得分:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问答题(共10题，每题10分，共计100分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.请分别解释统计机器学习中的输入/输出空间，特征空间，假设空间与参数空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "输入空间（X）：输入的所有可能的取值的空间。\n",
    "\n",
    "输出空间（Y）：输出的所有可能的取值的空间。\n",
    "\n",
    "特征空间（X）：每个输入由若干个特征表示，他们合起来叫做特征向量。特征向量组成的空间叫特征空间。特征空间在某些时候就是输入空间。\n",
    "\n",
    "假设空间(F):输入到输出的映射是一个模型。输入空间到输出空间的所有映射/模型组成了一个空间，这个空间称为假设空间。假设空间包含所有的模型。\n",
    "\n",
    "参数空间(θ):设（X1，……，Xn)为来自总体X的样本，（x1,…xn）为相应的样本值，θ是总体分布的未知参数，θ∈Θ， Θ表示θ的取值范围，称Θ为参数空间。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### 2.什么是损失函数，并举例有哪些常用的损失函数？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "损失函数用来评价真实值和预测值的差异程度，一般作为机器学习的目标函数，通过求损失函数的最小值，来优化模型。损失函数分为经验风险损失函数和结构风险损失函数。经验风险损失函数指预测结果和实际结果的差别，结构风险损失函数是指经验风险损失函数加上正则项。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0-1损失函数： \n",
    "\n",
    "L(Y,f(X))= $\\begin{cases}\n",
    "0 & \\text{ if } \\left | y-f(x)) \\right |\\geq T \\\\ \n",
    "1 & \\text{ if } \\left | y-f(x)) \\right |<  T \\\\ \n",
    "\\end{cases}$\n",
    "\n",
    "绝对值损失函数为：\n",
    "\n",
    "L(Y,f(X)=|Y−f(X)|\n",
    "\n",
    "log对数损失函数 ：\n",
    "\n",
    "L(Y,P(Y|X))=−logP(Y|X) \n",
    "\n",
    "平方损失函数:\n",
    "\n",
    "L(Y|f(X))=$\\sum (y-f(x))^{2}$\n",
    "\n",
    "指数损失函数:\n",
    "\n",
    "L(Y|f(X))=$e^{-x*f(x)}$\n",
    "\n",
    "Hinge损失函数：\n",
    "\n",
    "$min_{w,b} \\sum_{i}^{N}(1-y_{i}(\\omega x_{i}+b))+\\lambda \\left \\|\\omega ^{2}  \\right \\|$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3请结合公式进行说明结构风险最小化的含义？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$\\theta ^{*}=argmin\\frac{1}{N}\\sum_{i=1}^{N}L(y_{i},f(x_{i};\\theta _{i}))+\\lambda \\Phi (\\theta )$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 当样本容量很小时，经验风险最小化学习的效果不好，会产生过拟合现象，而结构风险最小化是为了防止过拟合而提出的策略。结构风险最小化等价于正则化。\n",
    "在假设空间、损失函数以及训练集确定的情况下，结构风险的定义是$\\frac{1}{N}\\sum_{i=1}^{N}L(y_{i},f(x_{i};\\theta _{i}))+\\lambda \\Phi (\\theta )$。其中，$ \\Phi (\\theta )$为模型的复杂度，是定义在假设空间上的泛函。模型f越复杂，复杂度$ \\Phi (\\theta )$就越大。也就是说，复杂度表示了对复杂模型的惩罚。结构风险小的模型往往对训练数据和未知的测试数据都有较好的预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.谈谈什么是生成式模型与判别式模型，以及各自特点？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "生成式模型定义：\n",
    "\n",
    "生成模型：无穷样本==》概率密度模型 = 产生模型==》预测\n",
    "\n",
    "生成方法由数据学习联合概率分布P(X,Y)，然后求出条件概率分布P(Y|X)=P(X,Y)/P(X)作为预测的模型。这样的方法之所以成为生成方法，是因为模型表示了给定输入X产生输出Y的生成关系。用于随机生成的观察值建模，特别是在给定某些隐藏参数情况下。典型的生成模型有：朴素贝叶斯法、马尔科夫模型、高斯混合模型。这种方法一般建立在统计学和Bayes理论的基础之上。\n",
    "\n",
    "生成方法特点：\n",
    "\n",
    "1，从统计的角度表示数据的分布情况，能够反映同类数据本身的相似度;\n",
    "\n",
    "2，生成方法还原出联合概率分布，而判别方法不能；\n",
    "\n",
    "3，生成方法的学习收敛速度更快、即当样本容量增加的时候，学到的模型可以更快地收敛于真实模型；\n",
    "\n",
    "4，当存在隐变量时，扔可以用生成方法学习，此时判别方法不能用\n",
    "\n",
    "判别式模型定义：\n",
    "\n",
    "判别模型：有限样本==》判别函数 = 预测模型==》预测\n",
    "\n",
    "判别方法由数据直接学习决策函数f(X)或者条件概率分布P(Y|X)作为预测的模型，即判别模型。判别方法关心的是对给定的输入X，应该预测什么样的输出Y。典型的判别模型包括：k近邻法、感知机、决策树、逻辑斯蒂回归模型、最大熵模型、支持向量机、boosting方法和条件随机场等。判别模型利用正负例和分类标签，关注在判别模型的边缘分布。\n",
    "\n",
    "判别方法的特点：\n",
    "\n",
    "1，判别方法寻找不同类别之间的最优分类面，反映的是异类数据之间的差异;\n",
    "\n",
    "2，判别方法利用了训练数据的类别标识信息，直接学习的是条件概率P(Y|X)或者决策函数f(X)，直接面对预测，往往学习的准确率更高；\n",
    "\n",
    "3，由于直接学习条件概率P(Y|X)或者决策函数f(X)，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。\n",
    "\n",
    "4，缺点是不能反映训练数据本身的特性\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.请解释范数，并写出L1,L2范数的定义公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "范数，在机器学习中通常用于衡量一个向量的大小\n",
    "\n",
    "L1范数：\n",
    "\n",
    "$\\sum_{i=1}^{N}\\left | x_{i} \\right |$\n",
    "\n",
    "L2范数：\n",
    "\n",
    "$\\sqrt{\\sum_{i=1}^{N}x_{i}^{2}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. 什么是信息熵，它的公式是？什么是信息增益?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "信息熵是信息论中用于度量信息量的一个概念。一个系统越是有序，信息熵就越低；反之，一个系统越是混乱，信息熵就越高。所以，信息熵也可以说是系统有序化程度的一个度量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$h(x)=-\\sum_{i=1}^{n}p(x_{i})log(2,p(x_{i}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "信息增益：以某特征划分数据集前后的熵的差值，在熵的理解那部分提到了，熵可以表示样本集合的不确定性，熵越大，样本的不确定性就越大。因此可以使用划分前后集合熵的差值来衡量使用当前特征对于样本集合D划分效果的好坏。划分前样本集合D的熵是一定的 ，entroy(前)，使用某个特征A划分数据集D，计算划分后的数据子集的熵 entroy(后)，信息增益 =  entroy(前) -  entroy(后)\n",
    "\n",
    "公式：g(D,A)=H(D)-H(D|A)\n",
    "\n",
    "做法：计算使用所有特征划分数据集D，得到多个特征划分数据集D的信息增益，从这些信息增益中选择最大的，因而当前结点的划分特征便是使信息增益最大的划分所使用的特征。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### 7. 贝叶斯公式是什么？您如何理解贝叶斯思想？它与频率派有哪些区别？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$P(B|A)=\\frac{P(B)P(A|B)}{P(A)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "频率学派与贝叶斯学派探讨「不确定性」这件事时的出发点与立足点不同。频率学派从「自然」角度出发，试图直接为「事件」本身建模，即事件A在独立重复试验中发生的频率趋于极限p，那么这个极限就是该事件的概率。\n",
    "然而，贝叶斯学派并不从试图刻画「事件」本身，而从「观察者」角度出发。贝叶斯学派并不试图说「事件本身是随机的」，或者「世界的本体带有某种随机性」，这套理论根本不言说关于「世界本体」的东西，而只是从「观察者知识不完备」这一出发点开始，构造一套在贝叶斯概率论的框架下可以对不确定知识做出推断的方法。\n",
    "频率学派下说的「随机事件」在贝叶斯学派看来，并不是「事件本身具有某种客观的随机性」，而是「观察者不知道事件的结果」而已，只是「观察者」知识状态中尚未包含这一事件的结果。\n",
    "但是在这种情况下，观察者又试图通过已经观察到的「证据」来推断这一事件的结果，因此只能靠猜。贝叶斯概率论就想构建一套比较完备的框架用来描述最能服务于理性推断这一目的的「猜的过程」。因此，在贝叶斯框架下，同一件事情对于知情者而言就是「确定事件」，对于不知情者而言就是「随机事件」，随机性并不源于事件本身是否发生，而只是描述观察者对该事件的知识状态。\n",
    "总的来说，贝叶斯概率论为人的知识（knowledge）建模来定义「概率」这个概念。频率学派试图描述的是「事物本体」，而贝叶斯学派试图描述的是观察者知识状态在新的观测发生后如何更新。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.请从最大似然的角度去解释逻辑回归,以及逻辑回归的损失函数是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "逻辑回归是使用sigmoid函数将输入X的线性函数映射成输出结果发生的概率，\n",
    "由于概率函数可以写成P(Y|X)=$h_{\\omega }(x)^{y}(1-h_{\\omega }(x))^{1-y}$,试验采样的似然函数为$\\prod_{1}^{m}p(y|x)$,由于样本的结果已成事实，要使结果更合理就要使似然函数最大，由于log函数是严格递增函数，所以最大化log等价于最大化原函数，损失函数可以通过最小化负的对数似然函数得到,$J(\\omega )=-\\frac{1}{m}\\sum_{i=1}^{m}[y_{i}logh_{\\omega }(x)+(1-y_{i})log(1-h_{\\omega }(x))]$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.试析Min-Max与Z-Score这两种数据缩放各自特点，和为什么树形结构不需要做缩放？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "归一化能提高梯度下降法求解最优解的速度，在需要计算样本距离的场景中有可能提高精度\n",
    "\n",
    "线性归一化：\n",
    "$x^{*}=\\frac{x-min}{max-min}$ 这种方法有个缺陷，如果max和min不稳定，很容易使得归一化结果不稳定，使得后续使用效果也不稳定。实际使用中可以用经验常量值来替代max和min。所以这种方法鲁棒性较差，只适合传统精确小数据场景。\n",
    "\n",
    "标准化：\n",
    "$x^{*}=\\frac{x-\\mu }{\\delta }$该种归一化方式要求原始数据的分布可以近似为高斯分布，否标准化的效果会变得很糟糕。它们可以通过现有样本进行估计。在已有样本足够多的情况下比较稳定，适合现代嘈杂大数据场景。\n",
    "\n",
    "树型模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、rf。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. 请解释下随机森林和Xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "随机森林是由多个决策树构成(M个）， 每个决策树单独对数据进行预测， 最终结果取这M个决策树中类别最多的那一类。不同分类器是相互独立的。 \n",
    "在随机森林中，对每个决策树，加入了两个随机特征。对于N个样本的训练集，采取有放回抽样， boostraping N个样本。这样可以保证 \n",
    "M个决策树的样本不同，防止over-fitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost（eXtreme Gradient Boosting）是工业界逐渐风靡的基于GradientBoosting算法的一个优化的版本，可以给预测模型带来能力的提升。回归树的分裂结点对于平方损失函数，拟合的就是残差；对于一般损失函数（梯度下降），拟合的就是残差的近似值，分裂结点划分时枚举所有特征的值，选取划分点。 最后预测的结果是每棵树的预测结果相加。 xgboost算法的步骤和GB基本相同，都是首先初始化为一个常数，gb是根据一阶导数ri，xgboost是根据一阶导数gi和二阶导数hi，迭代生成基学习器，相加更新学习器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>####答卷结束####</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本周课程意见反馈(非必答)\n",
    "请同学围绕以下两点进行回答：\n",
    "- 自身总结：您自己在本周课程的学习，收获，技能掌握等方面进行总结，包括自身在哪些方面存在哪些不足，欠缺，困惑。作为将来回顾学习路径时的依据。\n",
    "- 课程反馈：也可以就知识点，进度，难易度，教学方式，考试方式等等进行意见反馈，督促我们进行更有效的改进，为大家提供更优质的服务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$\\phi \\Phi \\emptyset$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
