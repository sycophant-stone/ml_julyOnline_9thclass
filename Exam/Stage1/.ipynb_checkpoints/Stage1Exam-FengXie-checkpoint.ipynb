{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习第九期第一阶段考试\n",
    "#### 考试说明:\n",
    "- 起止时间：请同学在2018年7月4日至7月10日期间完成，最晚提交时间（7月10日24时之前）结束，<b>逾期不接受补考,该考试分数计入平时成绩</b>\n",
    "- 考试方式：请同学将该试卷进行复制后，改名为自己姓名后，如State1Exam-WangWei.ipynb，<b>移动</b>至/0.Teacher/Exam/Stage1/目录下后，进行答题。\n",
    "- 注意事项：为确保同学们真正了解自身对本周课程的掌握程度，<font color=red><b>请勿翻阅，移动，更改</b></font>其它同学试卷。如发现按0分处理\n",
    "- 请同学在下方同学姓名处填写自己的姓名，批改人和最终得分不用填写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名: 谢锋\n",
    "- 批改人：   \n",
    "- 最终得分:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问答题(共10题，每题10分，共计100分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.请说明线性分类器与非线性分类器有哪些区别，具体应用场景有哪些不同？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* 线性分类器即用一个超平面将正负样本分割开，非线性分类器对划分平面没有限制，可能是一个曲面或多个超平面的组合。\n",
    "* 典型的线性分类器有感知机，LDA，逻辑斯特回归，SVM（线性核），朴素贝叶斯；\n",
    "* 典型的非线性分类器有kNN，决策树，SVM（非线性核）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.逻辑回归中为什么常常要对特征进行离散化?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. 便于模型的快速迭代。\n",
    "2. 稀疏向量的内积运算速度快。\n",
    "3. 简化特征，让计算机更易于识别。比如，年龄大于20岁则为1，否则为0，没有离散化的特征将对模型造成很大干扰。\n",
    "4. 简化逻辑回归模型，降低过拟合的风险。\n",
    "5. 特征离散化后进行训练得到的模型更稳定。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.随机森林如何处理缺失值？随机森林如何评估特征重要性？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 如何处理缺失值？\n",
    "1. 把数值型变量的缺失值用所对应类别的中位数替换，把描述型变量的缺失值用所对应类别的众数替换。\n",
    "2. 引入权重，用于替换的数与方法一相同，不同的是对需要替换的数据先和其他数据做相似度测量。\n",
    "\n",
    "#### 如何评估特征重要性？\n",
    "1. 基尼指数\n",
    "2. 计算袋外数据误差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 请说明梯度下降算法如何实现，以及它与牛顿法的不同？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* 梯度下降算法，又称最速下降法，是一种优化算法，可以通过迭代计算找到函数的局部最小值，若是凸函数，则是全局最小值。每次迭代根据当前的梯度和步长找到一个新的位置。迭代公式为：\n",
    "![](https://img-blog.csdn.net/20150810170743679)\n",
    "* 与牛顿法的不同：\n",
    "    1. 梯度下降法：对函数进行一阶逼近，寻找函数下降最快的方向。\n",
    "    2. 牛顿法：对函数的二阶逼近，直接估计函数的极小值点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 简要谈下您理解的的机器学习领域的正则化？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* 为损失函数增加一个惩罚项，在保留特征数量的同时调整参数值，有效防止过拟合。\n",
    "* 常见的正则化项有L1范数和L2范数。L1正则化可以产生稀疏权值矩阵，用于特征选择。L2正则化可以防止模型过拟合，L1在一定程度上也可以防止过拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6 带核的SVM为什么能分类非线性问题？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "核函数可以将数据映射到高维空间，从而可以用超平面将正负样本分开，变成线性问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7 请举例有哪些常用核函数，以及核函数的条件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. 线性核函数。主要用于线性可分的情况。\n",
    "2. 多项式核函数。可以实现将低维的输入空间映射到高维的特征空间。\n",
    "3. 高斯核函数。高斯径向基函数是一种局部性强的核函数，其可以将一个样本映射到一个更高维的空间内，该核函数是应用最广的一个，无论大样本还是小样本都有比较好的性能，而且其相对于多项式核函数参数要少。\n",
    "4. sigmoid 核函数。采用sigmoid核函数，支持向量机实现的就是一种多层神经网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.什么是偏差与方差？当你模型受到低偏差和高方差困扰时，如何解决？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Error = Bias + Variance。这里的Error大概可以理解为模型的预测错误率，是有两部分组成的，一部分是由于模型太简单而带来的估计不准确的部分（Bias），朴素贝叶斯是高偏差低方差的算法;另一部分是由于模型太复杂而带来的更大的变化空间和不确定性（Variance）\n",
    "* 怎么处理高偏差和高方差问题：  \n",
    "    高偏差：训练误差很大，训练误差与测试误差差距小，随着样本数据增多，训练误差增大。解决方法：\n",
    "\n",
    "    1.寻找更好的特征（具有代表性的）\n",
    "\n",
    "    2.用更多的特征（增大输入向量的维度）\n",
    "\n",
    "    高方差：过拟合，模型过于复杂，训练误差小，训练误差与测试误差差距大，可以通过增大样本集合来减小差距。随着样本数据增多，测试误差会减小。解决方案：\n",
    "\n",
    "    1.增大数据集合（使用更多的数据）\n",
    "\n",
    "    2.减少数据特征（减小数据维度）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.请说明熵、联合熵、条件熵、相对熵、互信息的定义（要求公式），以及您对这些定义的理解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$H(X) = -\\sum_{i=1}^{n}P(X_{k}) logP(X_{k})$\n",
    "$H(X,Y) = $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 请简要说明您对EM算法的理解，并列举有哪些常用的采用EM 算法求解的模型？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>####答卷结束####</h1></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
