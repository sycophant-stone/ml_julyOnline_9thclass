{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习第九期第一阶段考试\n",
    "#### 考试说明:\n",
    "- 起止时间：请同学在2018年7月4日至7月10日期间完成，最晚提交时间（7月10日24时之前）结束，<b>逾期不接受补考,该考试分数计入平时成绩</b>\n",
    "- 考试方式：请同学将该试卷进行复制后，改名为自己姓名后，如State1Exam-WangWei.ipynb，<b>移动</b>至/0.Teacher/Exam/Stage1/目录下后，进行答题。\n",
    "- 注意事项：为确保同学们真正了解自身对本周课程的掌握程度，<font color=red><b>请勿翻阅，移动，更改</b></font>其它同学试卷。如发现按0分处理\n",
    "- 请同学在下方同学姓名处填写自己的姓名，批改人和最终得分不用填写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名:陈宇\n",
    "- 批改人：   \n",
    "- 最终得分:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问答题(共10题，每题10分，共计100分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.请说明线性分类器与非线性分类器有哪些区别，具体应用场景有哪些不同？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "线性分类器：模型是参数的线性函数，分界面是超平面。\n",
    "\n",
    "非线性分类器：模型分界面是曲面或者超平面的组合。\n",
    "\n",
    "线性分类器判别简单，易实现，且需要的计算量和存储量都比较小；而如果需要解决比较复杂的线性不可分问题，则要用非线性分类器。\n",
    "\n",
    "具体的线性分类器包括：感知机，$Logistic$回归，带线性核的$SVM$等。\n",
    "\n",
    "集体的非线性分类器包括：决策树，带非线性核的$SVM$，$KNN$等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.逻辑回归中为什么常常要对特征进行离散化?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "在工业界，很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列$0、1$特征交给逻辑回归模型，这样做的优势有以下几\n",
    "\n",
    "点：\n",
    "\n",
    "1.离散特征的增加和减少都很容易，易于模型的快速迭代；\n",
    "\n",
    "2.稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；\n",
    "\n",
    "3.离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄$>30$是$1$，否则$0$。如果特征没有离散化，一个异常数据“年龄$300$岁”会给\n",
    "\n",
    "模型造成很大的干扰；\n",
    "\n",
    "4.逻辑回归属于广义线性模型，表达能力受限；单变量离散化为$N$个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能\n",
    "\n",
    "力，加大拟合；\n",
    "\n",
    "5.离散化后可以进行特征交叉，由$M+N$个变量变为$M*N$个变量，进一步引入非线性，提升表达能力；\n",
    "\n",
    "6.特征离散化后，模型会更稳定，比如如果对用户年龄离散化，$20-30$作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然\n",
    "\n",
    "处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；\n",
    "\n",
    "7.特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.随机森林如何处理缺失值？随机森林如何评估特征重要性？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "随机森林处理缺失值：首先将有缺失值的样本集合和无缺失值的样本集合分开。对于无缺失值的样本集合，将缺失值对应特征作为标签列，其他特征数\n",
    "\n",
    "据作为数据集，利用随机森林进行模型训练，再用该模型对有缺失值的样本集合进行预测，最终得出缺失值的预测值，完成缺失值的处理。\n",
    "\n",
    "随机森林可用基尼系数来评估特征重要性，具体方法如下：\n",
    "\n",
    "我们将特征重要性评分用$VIM(variable importance measures)$表示，将$Gini$指数用$GI$来表示，假设有$c$个特征$X_1，X_2，X_3，...，X_c$，现\n",
    "\n",
    "在要计算出每个特征$X_j$的Gini指数评分$VIM_j^(Gini)$，亦即第$j$个特征在$RF$所有决策树中节点分裂不纯度的平均改变量。\n",
    "\n",
    "$Gini$指数的计算公式为\n",
    "$$\n",
    "GI_m=\\sum_{k=1}^{|K|}\\sum_{k^{'}≠k}p_{mk}p_{mk^{'}}=1-\\sum_{k=1}^{|K|}p_{mk}^2\n",
    "$$\n",
    "\n",
    "其中，$K$表示有$K$个类别，$p_{mk}$表示节点$m$中类别k所占的比例。\n",
    "\n",
    "直观地说，就是随便从节点$m$中随机抽取两个样本，其类别标记不一致的概率。\n",
    "\n",
    "特征$X_j$在节点$m$的重要性，即节点$m$分枝前后的$Gini$指数变化量为\n",
    "$$\n",
    "VIM_{jm}^{Gini}=GI_m-GI_l-GI_r\n",
    "$$\n",
    "\n",
    "其中，$GI_l$和$GI_r$分别表示分枝后两个新节点的$Gini$指数。\n",
    "\n",
    "如果，特征$X_j$在决策树$i$中出现的节点在集合$M$中，那么$X_j$在第$i$颗树的重要性为\n",
    "$$\n",
    "VIM_{ij}^{Gini}=\\sum_{m∈M}VIM_{jm}^{Gini}\n",
    "$$\n",
    "\n",
    "假设$RF$中共有$n$颗树，那么\n",
    "$$\n",
    "VIM_j^{Gini}=\\sum_{i=1}^nVIM_{ij}^{Gini}\n",
    "$$\n",
    "\n",
    "最后，把所有求得的重要性评分做一个归一化处理即可。\n",
    "$$\n",
    "VIM_j=\\frac{VIM_j}{\\sum_{j=1}^cVIM_j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 请说明梯度下降算法如何实现，以及它与牛顿法的不同？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "梯度下降法：$f(θ)$在$θ_0$处的梯度表示$f(θ)$在点$θ_0$处函数值变化最快的方向。对于凸函数$f(θ)$来说，沿着负梯度方向寻找可以找到函数\n",
    "\n",
    "的极小值：\n",
    "$$\n",
    "θ^{k+1}=θ^k-ηf^{'}(θ^k)\n",
    "$$\n",
    "\n",
    "$k$表示第$k$次迭代，$η$表示修正因子(步长)。如果对于多维情形，表示为：\n",
    "$$\n",
    "θ^{k+1}=θ^k-η\\nabla f(θ^k)\n",
    "$$\n",
    "\n",
    "牛顿法：同样考虑凸函数$f(θ)$的极小值最优化问题，按照泰勒展开，用二次曲面去拟合函数的局部曲面：\n",
    "$$\n",
    "f(θ+\\triangleθ)=f(θ)+f^{'}(θ)\\triangleθ+\\frac{1}{2}f^{''}(θ)(\\triangleθ)^2\n",
    "$$\n",
    "\n",
    "要使左式最小，则对右边求导为0：\n",
    "$$\n",
    "0=f^{'}(θ)+f^{''}(θ)\\triangleθ\n",
    "$$\n",
    "$$\n",
    "\\triangleθ=-\\frac{f^{'}(θ)}{f^{''}(θ)}\n",
    "$$\n",
    "\n",
    "从而迭代过程：\n",
    "$$\n",
    "θ^{k+1}=θ^k-\\frac{f^{'}(θ)}{f^{''}(θ)}\n",
    "$$\n",
    "\n",
    "对于多维情形，一阶导变成梯度向量，二阶导变成海森矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 简要谈下您理解的的机器学习领域的正则化？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "正则化是为了解决过拟合问题而引入的，可以分为$L1$和$L2$正则化。其主要是通过修正损失函数，加入模型复杂性评估实现。正则化也符合奥科姆剃\n",
    "\n",
    "刀的思想：在所有可能的模型中，能够很好地解释已知数据并且十分简单的才是最好的模型。基本公式如下：\n",
    "$$\n",
    "J(ω)\\Rightarrow J(ω)+λ||ω||_p\n",
    "$$\n",
    "\n",
    "$p$表示范数，$p=1$和$p=2$分别应用于$L1$和$L2$正则。\n",
    "\n",
    "$L1$正则化：向量中各元素绝对值之和，又叫稀疏规则算子。关键在于能够实现特征的自动选择，参数稀疏可以避免非必要的特征引入的噪声。\n",
    "\n",
    "$L2$正则化：使得每个元素都尽可能地小，但是都不为零。\n",
    "\n",
    "$L1$正则化会趋向于产生少量的特征，而其他的特征都是0；而$L2$会选择更多的特征，这些特征会接近于0，但是总体模型解释性差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6 带核的SVM为什么能分类非线性问题？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "假设$φ$是一个从低维的输入空间$χ$(欧式空间的子集或离散集合)到高维的希尔伯特空间$H$的映射，如果存在函数$K(x,z)$，对于任意$x,z∈χ$,都\n",
    "\n",
    "有$K(x,z)=φ(x)φ(z)$，称$K(x,z)$为核函数，$φ(x)$为映射函数。\n",
    "\n",
    "核函数虽然也是将特征进行低维到高维的转换，但在低维上进行计算，而将实质上的分类效果(利用内积)表现在了高维上，避免了直接在高维空间中的\n",
    "\n",
    "复杂计算量，真正解决了$SVM$线性不可分的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7 请举例有哪些常用核函数，以及核函数的条件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "常用的核函数包括：\n",
    "\n",
    "①线性核函数$K(x,z)=xz$\n",
    "\n",
    "②多项式核函数$K(x,z)=(γxz+r)^d$， 其中$γ,r,d$都需要自己调参定义\n",
    "\n",
    "③高斯核函数(径向基函数)$K(x,z)=\\exp(-γ||x-z||^2)$， 其中$γ>0$，需要自己调参定义\n",
    "\n",
    "④$Sigmoid$函数$K(x,z)=tanh(γxz+r)$， 其中$γ,r$需要自己调参定义\n",
    "\n",
    "通常所说的核函数都是正定核函数。一个函数要想成为正定核函数，必须满足它里面任意点的集合形成的$Gram$矩阵是半正定的，即：\n",
    "\n",
    "对于任意的$x_i∈χ,i=1,2,3,...,N$，$K(x,z)$对应的$Gram$矩阵$K=[K(x_i,x_j)]$是半正定矩阵，则$K(x,z)$是正定核函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.什么是偏差与方差？当你模型受到低偏差和高方差困扰时，如何解决？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "基于偏差的误差：基于偏差的误差是我们模型预期的预测与我们将要预测的真实值之间的差值。偏差是用来衡量我们的模型的预测同真实值的差异。\n",
    "\n",
    "基于方差的误差：基于方差的误差描述了一个模型对给定的数据进行预测的可变性。比如，当你多次重复构建完 整模型的进程时，方差是在预测在模型\n",
    "\n",
    "的不同关系间变化的多少。\n",
    "\n",
    "当我们用一个参数多的复杂的模型进行预测，会得到高方差，低偏差的结果，通常出现过拟合。这时候可以通过添加正则化项或者获取更多数据解决。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.请说明熵、联合熵、条件熵、相对熵、互信息的定义（要求公式），以及您对这些定义的理解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "熵是信息量的期望，是表示随机变量不确定性的度量\n",
    "\n",
    "假设$X,Y$均是具有有限个值的离散型随机变量，其中$X=\\{x_1,x_2,...,x_n\\},Y=\\{y_1,y_2,...,y_m\\}$\n",
    "\n",
    "则$X$的熵为$H(X)=-\\sum_{i=1}^np(x_i)logp(x_i)$\n",
    "\n",
    "联合熵$H(X,Y)=-\\sum_{i=1}^n\\sum_{j=1}^mp(x_i,y_j)logp(x_i,y_j)$，表示$H(X)$和$H(Y)$的并集，描述一对随机变量平均所需要的信息量\n",
    "\n",
    "条件熵$H(Y|X)=\\sum_{i=1}^np(x_i)H(Y|X=x_i)=-\\sum_{i=1}^n\\sum_{j=1}^mp(x_i)p(y_j|x_i)logp(y_j|x_i)=-\\sum_{i=1}^n\\sum_{j=1}^mp(x_i,y_j)logp(y_j|x_i)=$\n",
    "\n",
    "$\\sum_{i=1}^n\\sum_{j=1}^mp(x_i,y_j)log\\frac{p(x_i)}{p(x_i,y_j)}$，表示已知随机变量$X$的取值条件下随机变量$Y$的不确定性\n",
    "\n",
    "互信息$I(X;Y)=-\\sum_{i=1}^n\\sum_{j=1}^mp(x_i,y_j)log\\frac{p(x_i,y_j)}{p(x_i)p(y_j)}$\n",
    "\n",
    "由$I(X;Y)=H(Y)-H(Y|X)$可知，互信息反映的是已知随机变量$X$的取值条件下随机变量$Y$的不确定性的减少量，即为机器学习概念中的信息增益\n",
    "\n",
    "相对熵又称为$KL$散度，是衡量相同事件空间里两个概率分布相对差距的测度，假设$p,q$为两个概率分布，则\n",
    "\n",
    "$p,q$的相对熵为$D(p||q)=\\sum_{i=1}^np(x_i)log\\frac{p(x_i)}{q(x_i))}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 请简要说明您对EM算法的理解，并列举有哪些常用的采用EM 算法求解的模型？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "概率模型有时既含有显变量(观测变量),又含有隐变量(潜在变量)。如果概率模型的变量都是显变量，那么给定数据，可以直接用极大似然估计法，或贝\n",
    "\n",
    "叶斯估计法估计模型参数。但是，当模型含有隐变量时，就不能简单地使用这些估计方法。$EM$算法就是含有隐变量的概率模型参数的极大似然估计\n",
    "\n",
    "法，或极大后验概率估计法。这里仅讨论极大似然估计。\n",
    "\n",
    "设显变量$Y=\\{y_1,y_2,...,y_N\\}$，隐变量$Z=\\{z_1,z_2,...,z_N\\}$\n",
    "\n",
    "则观测数据的似然函数为$L(θ)=\\prod_{j=1}^NP(y_j|θ)=\\prod_{j=1}^N\\sum_ZP(y_j|z,θ)P(z|θ)$\n",
    "\n",
    "取对数似然$l(θ)=logL(θ)=\\sum_{j=1}^Nlog\\sum_ZP(y_j|z,θ)P(z|θ)$\n",
    "\n",
    "当对数中有求和项时难以求到解析解，故欲求$l(θ)$的最大值，应使用迭代法求近似最优解\n",
    "\n",
    "设迭代第$n$次时的参数为$θ_n$，则\n",
    "\n",
    "$l(θ)-l(θ_n)=\\sum_{j=1}^Nlog\\sum_ZP(y_j|z,θ)P(z|θ)-\\sum_{j=1}^Nlog\\sum_ZP(y_j|z,θ_n)P(z|θ_n)$\n",
    "\n",
    "$=\\sum_{j=1}^N[log\\sum_ZP(y_j|z,θ)P(z|θ)-log\\sum_ZP(y_j|z,θ_n)P(z|θ_n)]$\n",
    "\n",
    "将$y_j$泛化为$y$，且有$log\\sum_ZP(y_j|z,θ_n)P(z|θ_n)=logP(y_j|θ_n)$，原式可化为\n",
    "\n",
    "$l(θ)-l(θ_n)=log\\sum_ZP(y|z,θ)P(z|θ)-logP(y|θ_n)$\n",
    "\n",
    "利用琴生不等式，$l(θ)-l(θ_n)=log\\sum_ZP(z|y,θ_n)\\frac{P(y|z,θ)P(z|θ)}{P(z|y,θ_n)}-logP(y|θ_n)$\n",
    "\n",
    "$≥\\sum_ZP(z|y,θ_n)log\\frac{P(y|z,θ)P(z|θ)}{P(z|y,θ_n)}-logP(y|θ_n)=\\sum_ZP(z|y,θ_n)log\\frac{P(y|z,θ)P(z|θ)}{P(z|y,θ_n)P(y|θ_n)}$\n",
    "\n",
    "令$B(θ,θ_n)=l(θ_n)+\\sum_ZP(z|y,θ_n)log\\frac{P(y|z,θ)P(z|θ)}{P(z|y,θ_n)P(y|θ_n)}$，则$l(θ)≥B(θ,θ_n)$\n",
    "\n",
    "即$B(θ,θ_n)$是$l(θ)$的一个下界，且有$B(θ,θ_n)=l(θ_n)$\n",
    "\n",
    "因此，任何可以使$B(θ,θ_n)$增大的$θ$，也可以使$l(θ)$增大。为了使$l(θ)$有尽可能大的增长，应选择$θ_{n+1}$使$B(θ,θ_n)$达到极大，即\n",
    "\n",
    "$θ_{n+1}=\\mathop{\\arg\\max}_{\\theta}B(θ,θ_n)=\\mathop{\\arg\\max}_{\\theta}[l(θ_n)+\\sum_ZP(z|y,θ_n)log\\frac{P(y|z,θ)P(z|θ)}{P(z|y,θ_n)P(y|θ_n)}]$\n",
    "\n",
    "$=\\mathop{\\arg\\max}_{\\theta}[\\sum_ZP(z|y,θ_n)logP(y|z,θ)P(z|θ)]=\\mathop{\\arg\\max}_{\\theta}[\\sum_ZP(z|y,θ_n)logP(y,z|θ)]=\\mathop{\\arg\\max}_{\\theta}Q(θ,θ_n)$\n",
    "\n",
    "上式等价于$EM$算法的一次迭代，即求$Q$函数及其极大化。$EM$算法是通过不断求解下界的极大化逼近求解对数似然极大化的算法。\n",
    "\n",
    "常用的采用$EM$算法求解的模型包括：高斯混合模型，$K-Means$等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>####答卷结束####</h1></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
