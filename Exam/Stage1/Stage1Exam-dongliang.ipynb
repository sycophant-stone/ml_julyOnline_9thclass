{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习第九期第一阶段考试\n",
    "#### 考试说明:\n",
    "- 起止时间：请同学在2018年7月4日至7月10日期间完成，最晚提交时间（7月10日24时之前）结束，<b>逾期不接受补考,该考试分数计入平时成绩</b>\n",
    "- 考试方式：请同学将该试卷进行复制后，改名为自己姓名后，如State1Exam-WangWei.ipynb，<b>移动</b>至/0.Teacher/Exam/Stage1/目录下后，进行答题。\n",
    "- 注意事项：为确保同学们真正了解自身对本周课程的掌握程度，<font color=red><b>请勿翻阅，移动，更改</b></font>其它同学试卷。如发现按0分处理\n",
    "- 请同学在下方同学姓名处填写自己的姓名，批改人和最终得分不用填写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名:董亮\n",
    "- 批改人：   \n",
    "- 最终得分:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问答题(共10题，每题10分，共计100分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.请说明线性分类器与非线性分类器有哪些区别，具体应用场景有哪些不同？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "线性分类器：模型是参数的线性函数，分类平面是（超）平面；\n",
    "非线性分类器：模型分界面可以是曲面或者超平面的组合。  \n",
    "典型的线性分类器有感知机，LDA，逻辑斯特回归，SVM（线性核）；\n",
    "典型的非线性分类器有朴素贝叶斯 kNN，决策树，SVM（非线性核）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.逻辑回归中为什么常常要对特征进行离散化?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "在工业界，很少直接将连续值作为特征喂给逻辑回归模型，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点：  \n",
    "1. 稀疏向量内积乘法运算速度快，计算结果方便存储，容易scalable（扩展）。 \n",
    "2. 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰。 \n",
    "3. 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合。  \n",
    "4. 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力  \n",
    "5. 特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问。  \n",
    "\n",
    "大概的理解：\n",
    "\n",
    "1）计算简单\n",
    "\n",
    "2）简化模型\n",
    "\n",
    "3）增强模型的泛化能力，不易受噪声的影响"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.随机森林如何处理缺失值？随机森林如何评估特征重要性？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "随机森林如何处理缺失值  \n",
    "方法一（na.roughfix）简单粗暴，对于训练集,同一个class下的数据，如果是分类变量缺失，用众数补上，如果是连续型变量缺失，用中位数补。\n",
    "\n",
    "方法二（rfImpute）这个方法计算量大，至于比方法一好坏？不好判断。先用na.roughfix补上缺失值，然后构建森林并计算proximity matrix，再回头看缺失值，如果是分类变量，则用没有缺失的观测实例的proximity中的权重进行投票。如果是连续型变量，则用proximity矩阵进行加权平均的方法补缺失值。然后迭代4-6次，这个补缺失值的思想和KNN有些类似。  \n",
    "随机森林如何评估特征重要性  \n",
    "1) Decrease GINI： 对于回归问题，直接使用argmax(VarVarLeftVarRight)作为评判标准，即当前节点训练集的方差Var减去左节点的方差VarLeft和右节点的方差VarRight  \n",
    "2) Decrease Accuracy：对于一棵树Tb(x)，我们用OOB样本可以得到测试误差1；然后随机改变OOB样本的第j列：保持其他列不变，对第j列进行随机的上下置换，得到误差2。至此，我们可以用误差1-误差2来刻画变量j的重要性。基本思想就是，如果一个变量j足够重要，那么改变它会极大的增加测试误差；反之，如果改变它测试误差没有增大，则说明该变量不是那么的重要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 请说明梯度下降算法如何实现，以及它与牛顿法的不同？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "梯度下降法用来求解目标函数的极值。这个极值是给定模型给定数据之后在参数空间中搜索找到的。迭代过程为  \n",
    "$\\theta _{j}:=\\theta _{j}-\\alpha \\frac{\\partial J(\\theta )}{\\partial \\theta }$  \n",
    "首先计算目标函数在当前参数值的斜率（梯度），然后乘以步长因子后带入更新公式，如图点所在位置（极值点右边），此时斜率为正，那么更新参数后参数减小，更接近极小值对应的参数。    \n",
    "如果更新参数后，当前参数值仍然在极值点右边，那么继续上面更新，效果一样。  \n",
    "如果更新参数后，当前参数值到了极值点的左边，然后计算斜率会发现是负的，这样经过再一次更新后就会又向着极值点的方向更新。  \n",
    "梯度下降的目的是直接求解目标函数极小值，而牛顿法则变相地通过求解目标函数一阶导为零的参数值，进而求得目标函数最小值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 简要谈下您理解的的机器学习领域的正则化？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "正则化是结构风险最小化策略的实现，是在经验风险上加上一个正则项(regularizer)或罚项(penalty  term)。是模型选择的典型方法。正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值越大。比较常用的正则化项有模型参数向量的范数，l1-norm、l2-norm......  \n",
    "正则化就是对最小化经验误差函数上加约束，这样的约束可以解释为先验知识(正则化参数等价于对参数引入先验分布)。  \n",
    "约束有引导作用，在优化误差函数的时候倾向于选择满足约束的梯度减少的方向，使最终的解倾向于符合先验知识(如一般的l-norm先验，表示原问题更可能是比较简单的，这样的优化倾向于产生参数值量级小的解，一般对应于稀疏参数的平滑解)  \n",
    "同时正则化，解决了逆问题的不适定性，产生的解是存在，唯一同时也依赖于数据的，噪声对不适定的影响就弱，解就不会过拟合，而且如果先验(正则化)合适，则解就倾向于是符合真解(更不会过拟合了)，即使训练集中彼此间不相关的样本数很少。  \n",
    "正则化大概有两个功能：   \n",
    "1，从模型修正上看，起了一个trade-off作用，用于平衡学习过程中两个基本量，名字诸如bias-variance、拟合能力-泛化能力、损失函数-推广能力、经验风险-结构风险等等；  \n",
    "2，从模型求解上看，正则化提供了一种唯一解的可能，众所周知，光用最小二乘拟合可能出现无数组解，加个L1或L2正则化项能有唯一解，即不适定性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6 带核的SVM为什么能分类非线性问题？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "使用核函数，使原始输入空间映射到新的特征空间让原先象形不可分的样本变得线性可分。这个时候，我们就可以使用原来的推导来进行计算，只是所有的推导现在是在新的空间，而不是原始空间中进行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7 请举例有哪些常用核函数，以及核函数的条件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "多项式核  $K(x,z) =(x*z+r)^{d}$  对应的SVM是一个d次多项式分类器。  \n",
    "高斯核函数 $K(x,z) =exp(-\\frac{\\gamma \\left \\| x-z \\right \\|^{^{2}}}{2\\sigma ^{2}})$  \n",
    "高斯核就是最开始提到过的会将原始空间映射为无穷维空间的那个家伙。不过，如果选得很大的话，高次特征上的权重实际上衰减得非常快，所以实际上（数值上近似一下）相当于一个低维的子空间；反过来，如果选得很小，则可以将任意的数据映射为线性可分——当然，这并不一定是好事，因为随之而来的可能是非常严重的过拟合问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.什么是偏差与方差？当你模型受到低偏差和高方差困扰时，如何解决？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "偏差：描述的是预测值（估计值）的期望与真实值之间的差距。偏差越大，越偏离真实数据  \n",
    "方差：描述的是预测值的变化范围，离散程度，也就是离其期望值的距离。方差越大，数据的分布越分散  \n",
    "我们可以使用bagging算法（如随机森林），以解决高方差问题。bagging算法把数据集分成重复随机取样形成的子集。然后，这些样本利用单个学习算法生成一组模型。接着，利用投票（分类）或平均（回归）把模型预测结合在一起  \n",
    "使用正则化技术，惩罚更高的模型系数，从而降低了模型的复杂性  \n",
    "使用可变重要性图表中的前n个特征。可以用于当一个算法在数据集中的所有变量里很难寻找到有意义信号的时候。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.请说明熵、联合熵、条件熵、相对熵、互信息的定义（要求公式），以及您对这些定义的理解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "熵（entropy）就用来衡量整个系统的总体信息量  \n",
    "$H_{s}=\\sum_{i=1}^{n}P_{i}I_{e}=-\\sum_{i=1}^{n}P_{i}log_{2}P_{i}$  \n",
    "联合熵 对于服从联合分布p(x,y)的一对离散型随机变量 联合熵定义为  \n",
    "$H_{X,Y}=\\sum_{x\\in X}\\sum_{y\\in Y}P_{x,y}logp_{x,y}=-ElogP(X,Y)$  \n",
    "大于每个独立的熵\n",
    "\n",
    "2个变量的联合熵大于或等于这2个变量中任一个的独立熵。    \n",
    "少于独立熵的和  \n",
    "2个变量的联合熵少于或等于2个变量的独立熵之和。这是次可加性的一个例子。该不等式有且只有在和均为统计独立的时候相等  \n",
    "这表明，两个变量关联之后不确定性会增大，但是又由于相互有制约关系，不确定小于单独两个变量的不确定度之和。  \n",
    "条件熵 就是在事件X的前提下，事件Y的熵。   \n",
    "$H_{(X|Y)}=\\sum_{x\\in X,y\\in Y}p(x,y)logP(X|Y)$  \n",
    "用处：决策树的特征选择，实际上使用的信息增益，就是用G(D,A)=H(Y)-H(Y|X)。可以看出在X的条件下，Y的不确定度下降了多少  \n",
    "相对熵（KL距离）两个概率密度函数p(x)和q(x)之间的相对熵定义为  \n",
    "$D(p||q)=\\sum_{x\\in X}p(x)log\\frac{p(x)}{q(x)} =E_{p}log\\frac{p(x)}{q(x)}$  \n",
    "描述两个随机变量距离的度量也叫交叉熵。\n",
    "\n",
    "相对熵越大，两个函数差异越大；反之，相对熵越小，两个函数差异越小。\n",
    "\n",
    "用处：在聚类算法中，使用相对熵代替欧几里得距离，计算连个节点的相关度，度量两个随机变量的差异性。  \n",
    "互信息：给定两个随机变量X，Y，联合密度函数为p(x,y)，其边际函数分别为p(x),q(x)，则互信息为p(x,y) 与p(x)q(x)之间的相对熵。  \n",
    "$I(X,Y) =\\sum_{x\\in X}\\sum_{y\\in Y}log\\frac{p(x,y)}{q(x)p(y)}-H(X)-H(X|Y)$    \n",
    "了解Y的前提下，消除X的不确定度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 请简要说明您对EM算法的理解，并列举有哪些常用的采用EM 算法求解的模型？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "解决什么问题\n",
    "在求解概率模型的时候，如果需要的变量都是观测变量，不涉及到隐藏变量的话，可以使用极大似然或者贝叶斯估计来求解模型的参数。比如：对于单高斯模型来讲，如果知道观测变量，那么就可以使用极大似然或者最小均方误差来估计高斯模型的均值和方差。\n",
    "通俗角度的话，求极大肯定是求似然函数的极大了，而且一般都是对数似然。我们一般解决模型参数求解问题，都是在给定数据的情况下，求解使得似然函数最大的参数的取值。用公式表示就是：   \n",
    "θ^=arg maxθ log P(X|θ)\n",
    "\n",
    "通常的做法是对似然函数求偏导，然后令偏导等于零，参数取得的数值就是近似最优值。但是，有些含有隐变量的模型没办法直接进行似然函数的偏导，但是如果假设已经知道隐变量的值，就可以将似然函数简化进行下一步的求偏导。\n",
    "因此，我们需要引入一个隐变量，求这个隐变量的期望就成了这种理解角度下E步骤。然后将隐变量的期望代入到经过隐变量改写的对数似然函数中，就可以按照通常的极大似然估计求解参数了。不过需要不断迭代才能达到近似最优。\n",
    "\n",
    "总结起来，这种角度的EM算法框架如下：\n",
    "\n",
    "loop\n",
    "\n",
    "E-step：求在观测数据的前提下隐变量的期望；\n",
    "M-step：求经过隐变量改写的似然函数的极大；\n",
    "end\n",
    "\n",
    "这种角度的好处是由一般的极大似然估计自然地引入到EM方法，比较容易理解；但是缺点是一般很难写出引入隐变量的似然函数的改写。实际上，这种改写很多情况下是依据EM算法的另一个理解角度而直接写出来的。\n",
    "\n",
    "如果模型同时包含观察变量和隐藏变量的话，传统的方法不能完成模型的估计，此时就需要引入EM算法。比如：对于混合高斯模型来讲，除了需要估计高斯模型的均值和方差，还涉及到每个高斯模型的权重信息，这个权重信息就可以认为是隐变量，所以一般使用EM来求解GMM的参数。\n",
    "\n",
    "1、蒙特卡罗算法（该算法又称随机性模拟算法,是通过计算机仿真来解决问题的算 法,同时可以通过模拟可以来检验自己模型的正确性,是比赛时必用的方法） 2、数据拟合、参数估计、插值等数据处理算法（比赛中通常会遇到大量的数据需要处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>####答卷结束####</h1></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
