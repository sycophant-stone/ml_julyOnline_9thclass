{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习第九期第一阶段考试\n",
    "#### 考试说明:\n",
    "- 起止时间：请同学在2018年7月4日至7月10日期间完成，最晚提交时间（7月10日24时之前）结束，<b>逾期不接受补考,该考试分数计入平时成绩</b>\n",
    "- 考试方式：请同学将该试卷进行复制后，改名为自己姓名后，如State1Exam-WangWei.ipynb，<b>移动</b>至/0.Teacher/Exam/Stage1/目录下后，进行答题。\n",
    "- 注意事项：为确保同学们真正了解自身对本周课程的掌握程度，<font color=red><b>请勿翻阅，移动，更改</b></font>其它同学试卷。如发现按0分处理\n",
    "- 请同学在下方同学姓名处填写自己的姓名，批改人和最终得分不用填写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名:余晗\n",
    "- 批改人：   \n",
    "- 最终得分:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问答题(共10题，每题10分，共计100分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.请说明线性分类器与非线性分类器有哪些区别，具体应用场景有哪些不同？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "线性分类器是对于线性可分的数据进行分类的模型,对于二维维的数据是条线,三维是平面,在多维数据里面是超平面\n",
    "非线性分类器则处理一条直线,平面,超平面都切的不是特别漂亮的场景,对于二维数据就是曲线,三维是曲面,多维则是超平面的组合\n",
    "所以线性分类器作用的场景就是一下就能切开的很漂亮的场景,也即是线性可分的场景\n",
    "而非线性分类器则作用于不是那么容易一刀切开的场景,也即是非线性可分的场景"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.逻辑回归中为什么常常要对特征进行离散化?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1.如果不做离散化容易受极值的影响,比如一个游戏调查数据,大部分的人都是15-25岁,突然出现个70岁的人,可能会对分类结果产生影响\n",
    "2.离散化也可以避免连续型数据分的太细造成过拟合的风险\n",
    "3.对于连续特征,往往会因为这个特征变的线性不可分,而离散化无疑能缓和这种不可分的趋势,对轻微的非线性数据能做到漂亮的分割"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.随机森林如何处理缺失值？随机森林如何评估特征重要性？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "缺失值根据该特征的众位数(出现最多的数字)和均值来填充,或者在两者的基础上根据实际项目情况加一个权重\n",
    "随机森林如果是决策树模式的话,ID3则是根据信息增益来评估,C4.5则是根据信息增益率来评估,cart则是gini系数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 请说明梯度下降算法如何实现，以及它与牛顿法的不同？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "梯度下降是根据损失函数求导得到导函数,而根据下降的情况把斜率具体算出来乘以一个学习率从而进行下降,\n",
    "因为它的斜率会越来越小直到逼近0,所以最终会得到一个近似解\n",
    "牛顿法则是用原函数除以导函数得出值,而根据这个值做迭代从而使结果收敛到导函数为0也即是近似解\n",
    "同时牛顿法也不需要学习率来约束斜率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 简要谈下您理解的的机器学习领域的正则化？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "正则化是一个约束,把得到的theta值进行一定程度的降低.从而避免下降过快,过拟合或者步子过大难收敛到最优解.不过如果正则化系数选择的不当也会导致欠拟合的情况"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6 带核的SVM为什么能分类非线性问题？ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "带核的svm可以把不可线性分割的数据映射到高维进行线性分割,而核函数又处理了维度高而计算复杂的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7 请举例有哪些常用核函数，以及核函数的条件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "常见的核函数有线性核,高斯核,sigmoid核函数\n",
    "###条件指的是什么?选用的条件吗？####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.什么是偏差与方差？当你模型受到低偏差和高方差困扰时，如何解决？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "偏差是因为数据等关系造成数据偏向某一个方面不太全面,实际训练出来的模型可能会欠拟合\n",
    "而方差则是模型的学习能力太强,导致学习的非常好,但实际预测却不行\n",
    "加大采样,正则化方法都可以解决偏差的问题\n",
    "减少一些特征,正则化方法都可以解决方差的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.请说明熵、联合熵、条件熵、相对熵、互信息的定义（要求公式），以及您对这些定义的理解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$H(X) = -\\sum_{k=1}^{n}P(X_{k}) logP(X_{k})$\n",
    "\n",
    "$H(X,Y) = -\\sum_{i=1}^{n}\\sum_{j=1}^{n}P(X_{i},Y_{j})logP(X_{i},Y_{j})$\n",
    "\n",
    "$H(Y|X) = -\\sum_{i=1}^{n}\\sum_{j=1}^{n}P(X_{i},Y_{j})logP(Y_{j}|X_{i})$\n",
    "\n",
    "$D(P||Q) = -\\sum_{i=1}^{n}P(X_{i})logP(X_{i})/Q(X_{i})$\n",
    "\n",
    "$I(X,Y) = \\sum_{i=1}^{n}\\sum_{j=1}^{n}P(X_{i},Y_{j})logP(X_{i},Y_{j})/P(X_{i})P(Y_{j}) $\n",
    "\n",
    "熵是对信息的不确定性描述,或者说,是对信息不确定的一个期望,概率分布越均匀熵越大.H(X)这个公式,刚好好在1/2的时候值最大,然后在1/2处随着概率变大或者变小,熵都会变小,也即是越来越确定\n",
    "\n",
    "而联合熵是对两者联合的不确定描述,所以把概率变成联合概率即可\n",
    "\n",
    "条件熵是对X确定时候Y的不确定性描述的期望,把条件概率带进去,应该是\n",
    "\n",
    "$H(Y|X) = -\\sum_{i=1}^{n}\\sum_{j=1}^{n}P(Y_{j}|X_{i})logP(Y_{j}|X_{i})$\n",
    "\n",
    "但是这里的x不是一个x所以还需要乘以一个P(xi)所以前面变成了联合概率,就成了上述公式\n",
    "\n",
    "相对熵又称kl散度,是衡量两个分布的差异的,需要注意的是D(P||Q)是以P为主体衡量两者的差异,而D(Q||P)是以Q为主体衡量两者差异..这是不一样的\n",
    "\n",
    "互信息是衡量两个熵的相似性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 请简要说明您对EM算法的理解，并列举有哪些常用的采用EM 算法求解的模型？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>####答卷结束####</h1></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
