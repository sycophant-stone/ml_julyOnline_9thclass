{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习九期第一周(机器学习基础)考试\n",
    "#### 考试说明:\n",
    "- 起止时间：请同学在2018年6月25日至7月2日期间完成，最晚提交时间（7月1日24时之前）结束，<b>逾期不接受补考,该考试分数计入平时成绩</b>\n",
    "- 考试方式：请同学<font color=red><b>拷贝</b></font>该试卷至自己姓名的目录后，将文件更名，即上同学姓名拼音后进行作答。格式：Week1Exam-WangWei.ipynb\n",
    "- 提交格式：请同学新建自己姓名全拼的文件夹，将该试卷，数据文件，zip文件等相关考试文件，放置此目录下。将该目录<b>移动</b>至/0.Teacher/Exam/1/目录下\n",
    "- 注意事项：为确保同学们真正了解自身对本周课程的掌握程度，<font color=red><b>请勿翻阅，移动，更改</b></font>其它同学试卷。如发现按0分处理\n",
    "- 请同学在下方同学姓名处填写自己的姓名，批改人和最终得分不用填写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名:<u></u>  \n",
    "- 批改人：   \n",
    "- 最终得分:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问答题(共10题，每题10分，共计100分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.请分别解释统计机器学习中的输入/输出空间，特征空间，假设空间与参数空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "输入空间：\n",
    "对于机器学习需要的数据，它们是由一批记录组成，这些记录组成为一个数据集，其中每条记录是关于一个事件或对象的描述，称为样本，这些样本描述了事件或对象在某方面的表现或性质，这些属性形成的空间就叫做输入空间。\n",
    "\n",
    "输出空间:\n",
    "一般地, 若$$({x_{i}},y_{i})$$ 表示第i个样例.其中$$y_{i}\\epsilon Y$$是xi的标记，Y是所有标记的集合，成为输出空间\n",
    "\n",
    "假设空间：\n",
    "\n",
    "设组成的空间。也可以说是所有在表达形式上符合任务要求的假设函数的集合。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.什么是损失函数，并举例有哪些常用的损失函数？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "损失函数，是评价模型的预测值和真实值之间的差距的方法。它是一个非负实数。\n",
    "\n",
    "常见损失函数有：\n",
    "\n",
    "1 log对数损失函数。\n",
    "\n",
    "用于逻辑回归。直接求导比较麻烦，先去对数然后再求导取极值点。\n",
    "$$ J(\\theta ) = -\\frac{1}{m}[ \\sum (y^{(i)} logh_{\\theta}(x^{(i)}) + (1-y^{(i)})log(1-h_{\\theta}(x^{(i)}))]$$\n",
    "\n",
    "\n",
    "2 平方损失函数，最小二乘法。\n",
    "\n",
    "最优拟合直线应该是使各点到回归直线的距离和最小的直线，即平方和最小。距离是欧氏距离。\n",
    "$$L(Y,f(X)) = \\sum (Y = f(X))^{2}$$\n",
    "\n",
    "\n",
    "3 指数损失函数\n",
    "\n",
    "$$L(Y,f(X)) = \\frac{1}{n}\\sum (exp(-y_{i}f(x_{i}))$$\n",
    "\n",
    "4 折叶损失， hinge损失函数，svm\n",
    "\n",
    "$$\\frac{1}{m}\\sum (l(w*x_{i} + b, y_{i})) + ||w||^{2}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "参考：\n",
    "[机器学习中的损失函数](https://blog.csdn.net/shenxiaoming77/article/details/51614601)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3请结合公式进行说明结构风险最小化的含义？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "结构风险包含两部分： 经验风险和置信风险。\n",
    "\n",
    "经验风险，代表了分类器在给定样本上的误差；\n",
    "置信风险，代表了在多大程度上可以信任分类器在未知文本上分类的结果。很显然，第二部分是没有办法精确计算的，因此只能给出一个估计的区间，也使得整个误差只能计算上界，而无法计算准确的值（所以叫做泛化误差界，而不叫泛化误差）。\n",
    "\n",
    "其中，置信风险与两个量有关，一是样本数量，显然给定的样本数量越大，学习结果越有可能正确，此时置信风险越小；二是分类函数的VC维，显然VC维越大，推广能力越差，置信风险会变大。\n",
    "\n",
    "泛化误差界的公式为:\n",
    "$$R(w) \\leq R_{emp}(w) + \\phi (\\frac{n}{h})$$\n",
    "\n",
    "公式中R(w)就是真实风险，Remp(w)就是经验风险，Ф(n/h)就是置信风险。统计学习的目标从经验风险最小化变为了寻求经验风险与置信风险的和最小，即结构风险最小。\n",
    "\n",
    "\n",
    "参考：\n",
    "1. [Vc维，结构风险最小化](http://www.cnblogs.com/yinheyi/p/8028416.html)\n",
    "\n",
    "2. [vc维含义个人理解](http://www.cnblogs.com/wuyuegb2312/archive/2012/12/03/2799893.html)\n",
    "\n",
    "\n",
    "感觉就是把一个空间可分的最大的向量个数（类似支持向量的感觉，但是含义不同）\n",
    "对于一个二位空间，最大的向量可分的个数是3，不是4。\n",
    "且，只要能够找到一组位置，对于m空间，满足n个向量，可以摆出m*n个可以被线性可分的组合就可以说明当前的VC是n。\n",
    "\n",
    "> 3个点的情况，存在一种位置关系，使得它的所有标签分配方式（2^3=8种）都是二维线性分类器可分的，因此结论是VC维至少为3\n",
    "即使存在另一种位置关系的某种标签分配方式不可分，也不影响这个结论\n",
    "4个点的情况，对于任意的某个位置关系，都存在一种标签分配方式使得二维线性分类器不可分（第三个图中右边的图），一共是2^4=16种标签分配方式，不过为了举反例，只有两个图。如果还不理解，可以任意画一个4个点的图，总能找到一个标签分配方式使得二维线性分类器不可分\n",
    "分散的概念可以再好好理解下\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.谈谈什么是生成式模型与判别式模型，以及各自特点？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "生成式模型：无穷样本 -> 概率密度模型 = 产生式模型 -> 预测\n",
    "\n",
    "估计的是联合概率分布，P(class, context)=P(class|context)×P(context)。\n",
    "用于随机生成的观察值建模，特别是在给定某些隐藏参数情况下。在机器学习中，或用于直接对数据建模，或作为生成条件概率密度函数的中间步骤。通过使用贝叶斯规则可以从生成模型中得到条件分布。\n",
    "主要特点：\n",
    "其一， 一般主要是对后验概率建模，从统计的角度表示数据的分布情况，能够反映同类数据本身的相似度\n",
    "其二， 只关注自己的 inclass 本身，不关心到底 decision boundary 在哪\n",
    "\n",
    "\n",
    "判别式模型\n",
    "\n",
    "有限样本 -> 判别函数 = 判别式模型 -> 预测\n",
    "\n",
    "利用正负例和分类标签，关注在判别模型的边缘分布。更多用来直接解决给定的问题，而不侧重于建模。\n",
    "\n",
    "主要特点：\n",
    "\n",
    "寻找不同类别之间的最优分类面，反映的是异类数据之间的差异\n",
    "\n",
    "\n",
    "\n",
    "区别：\n",
    "产生式模型(Generative Model)与判别式模型(Discrimitive Model)是分类器常遇到的概念，它们的区别在于：\n",
    "\n",
    "对于输入x，类别标签y：\n",
    "产生式模型估计它们的联合概率分布P(x,y)\n",
    "判别式模型估计条件概率分布P(y|x)\n",
    "\n",
    "产生式模型可以根据贝叶斯公式得到判别式模型，但反过来不行。\n",
    "\n",
    "\n",
    "参考：\n",
    "1. [判别式模型与生成式模型](http://www.voidcn.com/article/p-kyvkpcrx-xa.html)\n",
    "\n",
    "2. [判别式模型与生成式模型](https://blog.csdn.net/wolenski/article/details/7985426)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.请解释范数，并写出L1,L2范数的定义公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "范数：\n",
    "我们知道距离的定义是一个宽泛的概念，只要满足非负、自反、三角不等式就可以称之为距离。\n",
    "范数是一种强化了的距离概念，它在定义上比距离多了一条数乘的运算法则。有时候为了便于理解，我们可以把范数当作距离来理解。\n",
    "在数学上，范数包括向量范数和矩阵范数，向量范数表征向量空间中向量的大小，矩阵范数表征矩阵引起变化的大小。\n",
    "一种非严密的解释就是，对应向量范数，向量空间中的向量都是有大小的，这个大小如何度量，就是用范数来度量的，不同的范数都可以来度量这个大小，就好比米和尺都可以来度量远近一样；\n",
    "对于矩阵范数，学过线性代数，我们知道，通过运算AX=B，可以将向量X变化为B，矩阵范数就是来度量这个变化大小的。\n",
    "\n",
    "L1 范数：\n",
    "$$||x||_{1} = \\sum _{i}|x_{i}|$$\n",
    "\n",
    "L2 范数\n",
    "$$||x||_{2} = \\sqrt{\\sum _{i}x_{i}^{2}}$$\n",
    "\n",
    "参考：\n",
    "[几种范数的简单介绍](https://blog.csdn.net/shijing_0214/article/details/51757564)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. 什么是信息熵，它的公式是？什么是信息增益?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "信息熵描述了信源的不确定度。\n",
    "\n",
    "$$H(x) = -\\sum_{i->n} p_{xi}log_{2}p_{xi}$$\n",
    "\n",
    "信息增益描述了一个特征带来的信息量的多少，往往用于特征选择\n",
    "\n",
    "信息增益 = 信息熵 - 条件熵\n",
    "\n",
    "一个特征往往会使一个随机变量Y的信息量减少，减少的部分就是信息增益\n",
    "\n",
    "参考：\n",
    "[信息熵、条件熵、信息增益](https://blog.csdn.net/xtingjie/article/details/71305409)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. 贝叶斯公式是什么？您如何理解贝叶斯思想？它与频率派有哪些区别？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "贝叶斯公式，描述两个条件概率之间的关系：\n",
    "\n",
    "$$P(B|A) = \\frac{P(A|B)*P(B)}{P(A)}$$\n",
    "\n",
    "\n",
    "\n",
    "频率学派与贝叶斯学派探讨「不确定性」这件事时的出发点与立足点不同。频率学派从「自然」角度出发，试图直接为「事件」本身建模，即事件A在独立重复试验中发生的频率趋于极限p，那么这个极限就是该事件的概率。举例而言，想要计算抛掷一枚硬币时正面朝上的概率，我们需要不断地抛掷硬币，当抛掷次数趋向无穷时正面朝上的频率即为正面朝上的概率。然而，贝叶斯学派并不从试图刻画「事件」本身，而从「观察者」角度出发。贝叶斯学派并不试图说「事件本身是随机的」，或者「世界的本体带有某种随机性」，这套理论根本不言说关于「世界本体」的东西，而只是从「观察者知识不完备」这一出发点开始，构造一套在贝叶斯概率论的框架下可以对不确定知识做出推断的方法。频率学派下说的「随机事件」在贝叶斯学派看来，并不是「事件本身具有某种客观的随机性」，而是「观察者不知道事件的结果」而已，只是「观察者」知识状态中尚未包含这一事件的结果。但是在这种情况下，观察者又试图通过已经观察到的「证据」来推断这一事件的结果，因此只能靠猜。贝叶斯概率论就想构建一套比较完备的框架用来描述最能服务于理性推断这一目的的「猜的过程」。因此，在贝叶斯框架下，同一件事情对于知情者而言就是「确定事件」，对于不知情者而言就是「随机事件」，随机性并不源于事件本身是否发生，而只是描述观察者对该事件的知识状态。\n",
    "\n",
    "参考：\n",
    "\n",
    "[贝叶斯学派与频率学派有何不同](https://www.zhihu.com/question/20587681/answer/17435552)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.请从最大似然的角度去解释逻辑回归,以及逻辑回归的损失函数是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "用最大似然估计，目标函数就是对数似然函数。，它是关于(w,b)的高阶连续可到凸函数。可以通过工程商的一些算法求解，比如梯度下降法，牛顿法。\n",
    "\n",
    "逻辑回归的损失函数：\n",
    "$$ J(\\theta ) = -\\frac{1}{m}[ \\sum (y^{(i)} logh_{\\theta}(x^{(i)}) + (1-y^{(i)})log(1-h_{\\theta}(x^{(i)}))]$$\n",
    "\n",
    "参考：\n",
    "[逻辑回归损失函数为什么使用最大似然估计而不用最小二乘法？](https://www.zhihu.com/question/65350200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.试析Min-Max与Z-Score这两种数据缩放各自特点，和为什么树形结构不需要做缩放？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "一、min-max标准化（Min-Max Normalization）\n",
    "\n",
    "也称为离差标准化，是对原始数据的线性变换，使结果值映射到[0 - 1]之间。转换函数如下：\n",
    "\n",
    "$$x^{*} = \\frac{x-min}{max-min}$$\n",
    "\n",
    "其中max为样本数据的最大值，min为样本数据的最小值。这种方法有个缺陷就是当有新数据加入时，可能导致max和min的变化，需要重新定义。\n",
    "\n",
    "二、Z-score标准化方法\n",
    "\n",
    "这种方法给予原始数据的均值（mean）和标准差（standard deviation）进行数据的标准化。经过处理的数据符合标准正态分布，即均值为0，标准差为1，转化函数为：\n",
    "\n",
    "$$x^{*} = \\frac{x-\\mu }{\\sigma }$$\n",
    "\n",
    "其中u为所有样本数据的均值，sigma为所有样本数据的标准差\n",
    "\n",
    "\n",
    "树形结构不需要归一化的原因：\n",
    "数值缩放，不影响分裂点位置。因为第一步都是按照特征值进行排序的，排序的顺序不变，那么所属的分支以及分裂点就不会有不同。对于线性模型，比如说LR，我有两个特征，一个是(0,1)的，一个是(0,10000)的，这样运用梯度下降时候，损失等高线是一个椭圆的形状，这样我想迭代到最优点，就需要很多次迭代，但是如果进行了归一化，那么等高线就是圆形的，那么SGD就会往原点迭代，需要的迭代次数较少。\n",
    "另外，注意树模型是不能进行梯度下降的，因为树模型是阶跃的，阶跃点是不可导的，并且求导没意义，所以树模型（回归树）寻找最优点事通过寻找最优分裂点完成的。\n",
    "\n",
    "参考\n",
    "1. [数据归一化和两种常用的归一化方法](https://www.cnblogs.com/chaosimple/p/3227271.html)\n",
    "\n",
    "2. [关于XGBOOST疑问和解决](https://www.jianshu.com/p/629edf6412de)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. 请解释下随机森林和Xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "随机森林顾名思义，是用随机的方式建立一个森林，森林里面有很多的决策树组成，随机森林的每一棵决策树之间是没有关联的。在得到森林之后，当有一个新的输 入样本进入的时候，就让森林中的每一棵决策树分别进行一下判断，看看这个样本应该属于哪一类（对于分类算法），然后看看哪一类被选择最多，就预测这个样本 为那一类。\n",
    "\n",
    "xgboost是在GBDT的基础上对boosting算法进行的改进，内部决策树使用的是回归树\n",
    "\n",
    "\n",
    "参考：\n",
    "[xgboost 算法原理](https://blog.csdn.net/a1b2c3d4123456/article/details/52849091)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>####答卷结束####</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本周课程意见反馈(非必答)\n",
    "请同学围绕以下两点进行回答：\n",
    "- 自身总结：您自己在本周课程的学习，收获，技能掌握等方面进行总结，包括自身在哪些方面存在哪些不足，欠缺，困惑。作为将来回顾学习路径时的依据。\n",
    "- 课程反馈：也可以就知识点，进度，难易度，教学方式，考试方式等等进行意见反馈，督促我们进行更有效的改进，为大家提供更优质的服务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
