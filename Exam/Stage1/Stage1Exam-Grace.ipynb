{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习第九期第一阶段考试\n",
    "#### 考试说明:\n",
    "- 起止时间：请同学在2018年7月4日至7月10日期间完成，最晚提交时间（7月10日24时之前）结束，<b>逾期不接受补考,该考试分数计入平时成绩</b>\n",
    "- 考试方式：请同学将该试卷进行复制后，改名为自己姓名后，如State1Exam-WangWei.ipynb，<b>移动</b>至/0.Teacher/Exam/Stage1/目录下后，进行答题。\n",
    "- 注意事项：为确保同学们真正了解自身对本周课程的掌握程度，<font color=red><b>请勿翻阅，移动，更改</b></font>其它同学试卷。如发现按0分处理\n",
    "- 请同学在下方同学姓名处填写自己的姓名，批改人和最终得分不用填写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名: Grace\n",
    "- 批改人：   \n",
    "- 最终得分:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问答题(共10题，每题10分，共计100分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.请说明线性分类器与非线性分类器有哪些区别，具体应用场景有哪些不同？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "线性非线性分类器都是有监督学习，但有以下区别：\n",
    "* 线性分类器的分界函数是线性函数，分界面是直线（二维）、平面（三维）、超平面（多维）。\n",
    "* 非线性分类器点分解函数是非线性函数，分界面不是超平面，形状变化多端，可能是超平面、曲面等的组合。\n",
    "* 线性分类器比非线性数学模型简单、易理解和实现、计算量相对较小，但不能解决线性不可分的问题。\n",
    "常用的线性分类器：Linear Regression, Logistic Regression, 线性核的SVM\n",
    "常用的线性分类器：Decision Tree，非线性核的SVM，KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.逻辑回归中为什么常常要对特征进行离散化?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展。\n",
    "\n",
    "2. 离散化后的特征对异常数据有很强的鲁棒性\n",
    "\n",
    "3. 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合。\n",
    "\n",
    "4. 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力。\n",
    "\n",
    "5. 特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.随机森林如何处理缺失值？随机森林如何评估特征重要性？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "处理缺失值：\n",
    "1. 均值/中位数等统计值填补\n",
    "2. 预测值。将无缺失值的样本作为训练数据，缺失值对应的特征作为标签列构建随机森林，用对有缺失值的样本，用该随机森林预测的值用于填补。\n",
    "\n",
    "评估特征重要性：\n",
    "我们将特征重要性评分用$VIM$ (variable importance measures)表示，将$Gini$指数用$GI$来表示，假设有$c$个特征$X_1，X_2，X_3，...，X_c$，现在要计算出每个特征$X_j$的Gini指数评分$VIM_j^(Gini)$，\n",
    "\n",
    "$Gini$指数的计算公式为\n",
    "$$\n",
    "GI_m=\\sum_{k=1}^{|K|}\\sum_{k^{'}≠k}p_{mk}p_{mk^{'}}=1-\\sum_{k=1}^{|K|}p_{mk}^2\n",
    "$$\n",
    "\n",
    "其中，$K$表示有$K$个类别，$p_{mk}$表示节点$m$中类别k所占的比例。\n",
    "\n",
    "直观地说，就是随便从节点$m$中随机抽取两个样本，其类别标记不一致的概率。\n",
    "\n",
    "特征$X_j$在节点$m$的重要性，即节点$m$分枝前后的$Gini$指数变化量为\n",
    "$$\n",
    "VIM_{jm}^{Gini}=GI_m-GI_l-GI_r\n",
    "$$\n",
    "\n",
    "其中，$GI_l$和$GI_r$分别表示分枝后两个新节点的$Gini$指数。\n",
    "\n",
    "如果，特征$X_j$在决策树$i$中出现的节点在集合$M$中，那么$X_j$在第$i$颗树的重要性为\n",
    "$$\n",
    "VIM_{ij}^{Gini}=\\sum_{m∈M}VIM_{jm}^{Gini}\n",
    "$$\n",
    "\n",
    "假设$RF$中共有$n$颗树，那么\n",
    "$$\n",
    "VIM_j^{Gini}=\\sum_{i=1}^nVIM_{ij}^{Gini}\n",
    "$$\n",
    "\n",
    "最后，把所有求得的重要性评分做一个归一化处理即可。\n",
    "$$\n",
    "VIM_j=\\frac{VIM_j}{\\sum_{j=1}^cVIM_j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 请说明梯度下降算法如何实现，以及它与牛顿法的不同？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "GD:\n",
    "$$\n",
    "θ_i := θ_i - \\alpha\\nabla \\frac{\\partial J(\\theta)}{\\partial θ_i}\n",
    "$$\n",
    "\n",
    "牛顿法：\n",
    "$$\n",
    "θ_i := θ_i- \\frac{\\partial^2 J(\\theta}{\\partial^2(\\theta_i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 简要谈下您理解的的机器学习领域的正则化？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "在机器学习模型中，当参数较多或模型过于复杂时，可能出现过拟合，模型泛化能力较差。为解决这个问题，我们  \n",
    "在损失函数中加入正则化项，防止模型的参数过于复杂或参数值过大。\n",
    "$$\n",
    "J(ω)\\Rightarrow J(ω)+λ||ω||_p\n",
    "$$\n",
    "$p$可以取1,2,3,..等不同值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6 带核的SVM为什么能分类非线性问题？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "核函数将特征进行低维到高维的转换，但在低维上进行计算，而将实质上的分类效果(利用内积)表现在了高维上，避免了直接在高维空间中的\n",
    "\n",
    "复杂计算量。在高维空间非线性问题转化为线性问题, SVM得到超平面是高维空间的线性分类平面。解决了$SVM$线性不可分的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7 请举例有哪些常用核函数，以及核函数的条件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "常用的核函数包括：\n",
    "\n",
    "- 线性核函数$K(x,z)=xz$\n",
    "\n",
    "- 多项式核函数$K(x,z)=(γxz+r)^d$， 其中$γ,r,d$都需要自己调参定义\n",
    "\n",
    "- 高斯核函数(径向基函数)$K(x,z)=\\exp(-γ||x-z||^2)$， 其中$γ>0$，需要自己调参定义\n",
    "\n",
    "- $Sigmoid$函数$K(x,z)=tanh(γxz+r)$， 其中$γ,r$需要自己调参定义\n",
    "\n",
    "\n",
    "通常所说的核函数都是正定核函数。一个函数要想成为正定核函数，必须满足它里面任意点的集合形成的$Gram$矩阵是半正定的，即：\n",
    "\n",
    "对于任意的$x_i∈χ,i=1,2,3,...,N$，$K(x,z)$对应的$Gram$矩阵$K=[K(x_i,x_j)]$是半正定矩阵，则$K(x,z)$是正定核函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.什么是偏差与方差？当你模型受到低偏差和高方差困扰时，如何解决？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- 偏差：预测值与真实值之间的差值\n",
    "\n",
    "- 方差：预测值的变化范围和离散程度\n",
    "\n",
    "- 低偏差高方差如何解决：模型过拟合时会出现这种情况，可以通过\n",
    "* 正则化\n",
    "* 采集更多数据\n",
    "* 特征选择\n",
    "* 差异化模型融合\n",
    "来解决"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.请说明熵、联合熵、条件熵、相对熵、互信息的定义（要求公式），以及您对这些定义的理解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- 熵Entropy：不确定性或信息量的度量，不确定性越大熵越大，信息纯度越搞，熵越小。\n",
    "-- 对单个概率的熵: $E(p) = -p\\log p$\n",
    "-- 对随机变量X的熵: $E(X) = -\\sum_{x \\in X} p(x)\\log p(x)$ \n",
    "\n",
    "假设$X,Y$均是具有有限个值的离散型随机变量，其中$X=\\{x_1,x_2,...,x_n\\},Y=\\{y_1,y_2,...,y_m\\}$\n",
    "- 联合熵$H(X, Y)$代表$X$,$Y$同时发生的不确定性\n",
    "$$H(X, Y) = -\\sum_{x \\in X, y \\in Y}p(x, y)\\log p(x, y)$$\n",
    "\n",
    "- 条件熵$H(Y|X)$代表$X$发生时,$Y$发生的不确定性\n",
    "$$H(Y|X) = -\\sum_{x \\in X}\\sum_{y \\in Y}p(x)p(y|x)\\log p(y|x) = -\\sum_{x \\in X}\\sum_{y \\in Y}p(x, y)\\log p(y|x)$$\n",
    "\n",
    "- 互信息$I(X;Y)=H(Y)-H(Y|X)$，互信息反映的是已知随机变量$X$的取值条件下随机变量$Y$的不确定性的减少量，即为机器学习概念中的信息增益.\n",
    "$$I(X;Y)=-\\sum_{x \\in X}\\sum_{y \\in Y} \\log\\frac{p(x, y)}{p(x)p(y)}$$\n",
    "\n",
    "- 相对熵又称为$KL$散度，是衡量相同事件空间里两个概率分布相对差距的测度，假设$p,q$为两个概率分布，则\n",
    "\n",
    "$p,q$的相对熵为$$D(p||q)=\\sum_{i=1}^n p(x_i)\\log\\frac{p(x_i)}{q(x_i))}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 请简要说明您对EM算法的理解，并列举有哪些常用的采用EM 算法求解的模型？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>####答卷结束####</h1></center>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
