{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习九期第一周(机器学习基础)考试\n",
    "#### 考试说明:\n",
    "- 起止时间：请同学在2018年6月25日至7月2日期间完成，最晚提交时间（7月1日24时之前）结束，<b>逾期不接受补考,该考试分数计入平时成绩</b>\n",
    "- 考试方式：请同学<font color=red><b>拷贝</b></font>该试卷至自己姓名的目录后，将文件更名，即上同学姓名拼音后进行作答。格式：Week1Exam-WangWei.ipynb\n",
    "- 提交格式：请同学新建自己姓名全拼的文件夹，将该试卷，数据文件，zip文件等相关考试文件，放置此目录下。将该目录<b>移动</b>至/0.Teacher/Exam/1/目录下\n",
    "- 注意事项：为确保同学们真正了解自身对本周课程的掌握程度，<font color=red><b>请勿翻阅，移动，更改</b></font>其它同学试卷。如发现按0分处理\n",
    "- 请同学在下方同学姓名处填写自己的姓名，批改人和最终得分不用填写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名:王清\n",
    "- 批改人：   \n",
    "- 最终得分:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问答题(共10题，每题10分，共计100分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.请分别解释统计机器学习中的输入/输出空间，特征空间，假设空间与参数空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- 在模型学习中，将输入与输出的所有可能取值的集合分别称为输入空间（input space）与输出空间（output space）。\n",
    "- 每个具体的输入是一个实例（instance），通常由特征向量（feature vector）表示。此时，所有的特征向量存在的空间称为特征空间（feature space）。\n",
    "- 模型属于输入空间到输出空间的映射的集合，这个集合就是假设空间（hypothesis space）。\n",
    "- 假设空间通常是由一个参数向量所决定的函数族，参数向量取值与n维欧式空间，称为参数空间（parameter space）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.什么是损失函数，并举例有哪些常用的损失函数？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "对于给定的X，由假设函数得到相应的输出Y，这个输出值与真实值可能一致也可能不一致，用损失函数（loss function）来度量预测错误的程度， 损失函数度量模型一次预测的好坏，损失函数值越小，模型就越好。\n",
    "常见的损失函数：\n",
    "1. 0-1损失函数（0-1 loss function）$$ L(Y, f(X)) = \\left\\{  \\begin{array}{lr} 1, Y \\neq f(X) &\\\\ 0, Y = f(X)  \\end{array} \\right.$$\n",
    "2. 平方损失函数（quadratic loss function）$$ L(Y, f(X)) = (Y - f(X))^2 $$\n",
    "3. 绝对损失函数（absolute lossfunction）$$ L(Y, f(X)) = |Y - f(X)| $$\n",
    "4. 对数损失函数（logarithmic loss function）$$ L(Y, f(X)) = - logP(Y|X) $$\n",
    "5. 指数损失函数（exponent loss function）$$ L(Y, f(X)) = exp[-yf(X)] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3请结合公式进行说明结构风险最小化的含义？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "结构风险最小化（structural risk minimization， SRM）是为了防止过拟合而提出来的策略，其等价于正则化。结构风险在经验风险上加上表示模型复杂度的正则化项：$$ R_{srm}(f) = \\frac{1}{N}\\sum_{i=1}^NL(y_i, f(x_i)) + \\lambda J(f) $$其中$ \\frac{1}{N}\\sum_{i=1}^NL(y_i, f(x_i)) $表示经验风险；$ J(f) $表示模型的复杂度，模型f越复杂，复杂度$ J(f) $就越大；$ \\lambda \\ge 0 $是系数，用以权衡经验风险和模型复杂度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.谈谈什么是生成式模型与判别式模型，以及各自特点？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "生成方法由数据学习联合概率分布$ P(X, Y) $，然后求出条件概率分布$ P(Y|X) $作为预测的模型，即：$$ P(Y|X) = \\frac{P(X, Y)}{P(X)} $$模型表示了给定输入X产生输出Y的生成关系，则称为生成模型。典型的生成模型有：朴素贝叶斯和隐马尔科夫模型。\n",
    "\n",
    "判别方法由数据直接学习决策函数f(X)或者条件概率分布P(Y|X)作为预测的模型，即判别模型。判别方法关心的是对给定的输入X，应该预测什么样的输出Y。典型的判别模型有：K近邻、感知机、决策树等。\n",
    "\n",
    "生成式模型特点：生成方法可以还原出联合概率分布P(X, Y)，而判别方法则不能；生成方法的学习收敛速度更快，即当样本容量增加的时候，学习到的模型可以更快地收敛真是模型；当存在隐变量时，只能使用生成方法学习。\n",
    "\n",
    "判别式模型特点：判别方法直接学习条件概率P(Y|X)或决策函数f(X)，直接面对预测，往往学习的准确率更高；由于直接学习条件概率P(Y|X)或决策函数f(X)，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.请解释范数，并写出L1,L2范数的定义公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "距离的定义是一个宽泛的概念，只要满足非负、自反、三角不等式就可以称之为距离。范数是一种强化了的距离概念，它在定义上比距离多了一条数乘的运算法则。有时候为了便于理解，我们可以把范数当作距离来理解。\n",
    "\n",
    "L1范数：$$ ||x||_1 = \\sum_i x_i $$表示向量x中非零元素的绝对值之和\n",
    "\n",
    "L2范数：$$ ||x||_2 = \\sqrt{\\sum_i x_i^2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. 什么是信息熵，它的公式是？什么是信息增益?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "信息熵（entropy）：表示随机变量不确定性的度量，用来度量样本集合纯度的一种常用指标。假设当前样本集合D中第K类样本所占的比例为$p_k(k = 1, 2, \\cdots, |y|$，则D的信息熵定义为$$ Ent(D) = -\\sum_{k=1}^{|y|}p_klog_2p_k $$Ent(D)的值越小，则D的纯度越高。\n",
    "\n",
    "信息增益（information gain）：$$ Gain(D, a) = Ent(D) - \\sum_{v=1}^V\\frac{D^v}{D}Ent(D^v) $$信息增益越大，表示使用属性a来进行划分所获得的“纯度提升“越大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. 贝叶斯公式是什么？您如何理解贝叶斯思想？它与频率派有哪些区别？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "贝叶斯公式：$$ P(B_i|A) = \\frac{P(B_i)P(A|B_i)}{\\sum_{j=1}^nP(B_j)P(A|B_j)} $$频率学派从「自然」角度出发，试图直接为「事件」本身建模，即事件A在独立重复试验中发生的频率趋于极限p，那么这个极限就是该事件的概率。贝叶斯学派并不从试图刻画「事件」本身，而从「观察者」角度出发。贝叶斯学派并不试图说「事件本身是随机的」，或者「世界的本体带有某种随机性」，这套理论根本不言说关于「世界本体」的东西，而只是从「观察者知识不完备」这一出发点开始，构造一套在贝叶斯概率论的框架下可以对不确定知识做出推断的方法。频率学派试图描述的是「事物本体」，而贝叶斯学派试图描述的是观察者知识状态在新的观测发生后如何更新。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.请从最大似然的角度去解释逻辑回归,以及逻辑回归的损失函数是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "逻辑回归的假设函数$$ h_\\theta(x) = g(X^T\\theta) = \\frac{1}{1 + e^{-X^T\\theta}}$$如果像线性回归一样利用最小二乘法做代价函数，将会发现最终得到的函数并不是一个凸函数，数学性质不好导致常用的梯度下降法就无法使用。\n",
    "\n",
    "因为$h_\\theta(x)$代表样本X属于标签Y=1时候的概率，即$P(Y_i=1|X_i,\\theta)=h_\\theta(x)$,$P(Y_i=0|X_i,\\theta)=1-h_\\theta(x)$，做出似然函数：$$ P(Y_i|X_i,\\theta) = (h_\\theta(x_i))^{Y_i}(1-h_\\theta(x_i))^{1-Y_i}$$样本之间独立的，则：$$ P(Y|X, \\theta) = \\prod_{1}^{m}P(Y_i|X_i,\\theta) $$取对数：$$ P(Y|X, \\theta) = \\sum_{1}^{m}Y_ilog((h_\\theta(x_i))+(1-Y_i)log(1-h_\\theta(x_i)) $$则逻辑回归的损失函数为：$$ cost(h_\\theta(x_i),y_i) = \\left\\{ \\begin{array}{lr} -log(h_\\theta(x_i))\\qquad y_i=1 &\\\\ -log(1-h_\\theta(x_i))\\quad y_i=0 \\end{array} \\right.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.试析Min-Max与Z-Score这两种数据缩放各自特点，和为什么树形结构不需要做缩放？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Min-Max数据缩放：$$ x^{'} = \\frac{x - x_{min}}{x_{max} - x_{min}} $$min-max标准化方法是对原始数据进行线性变换，设x_min和x_max分别为属性A的最小值和最大值，将A的一个原始值x通过min-max标准化映射成在区间\\[0,1\\]中的值x’\n",
    "\n",
    "Z-Score数据缩放：$$ x^{'} = \\frac{x - \\mu}{\\delta}\\qquad\\mu\\mbox{为数据均值}\\quad\\delta\\mbox{为数据方差} $$Z-Score基于原始数据的均值(mean)和标准差(standard deviation)进行数据的标准化,将A的原始值x使用z-score标准化到x’。z-score标准化方法适用于属性A的最大值和最小值未知的情况，或有超出取值范围的离群数据的情况。将数据按其属性(按列进行)减去其均值，然后除以其方差。最后得到的结果是，对每个属性/每列来说所有数据都聚集在0附近，方差值为1。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. 请解释下随机森林和Xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "随机森林（Romdom Forest，简称RF）是Bagging的一个扩展变体，RF在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入随机属性的选择。传统决策树在选择划分属性时是在当前节点的属性集合（假设有有d个属性）中选择一个最优属性；而在RF中，对基决策树的每个节点，先从该节点的属性集合中随机选择一个包含k个属性的子集，然后再从这个子集中选择一个最优属性用于划分。\n",
    "\n",
    "Xgboost是GBDT的一种高效实现，Xgboost中的基学习器除了可以是CART（gbtree）也可以是线性分类器（gblinear）。GBDT是一种迭代的决策树算法，该算法由多颗决策树组成，所有树的结论累加起来作为最终答案。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>####答卷结束####</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本周课程意见反馈(非必答)\n",
    "请同学围绕以下两点进行回答：\n",
    "- 自身总结：您自己在本周课程的学习，收获，技能掌握等方面进行总结，包括自身在哪些方面存在哪些不足，欠缺，困惑。作为将来回顾学习路径时的依据。\n",
    "- 课程反馈：也可以就知识点，进度，难易度，教学方式，考试方式等等进行意见反馈，督促我们进行更有效的改进，为大家提供更优质的服务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$\\phi \\Phi \\emptyset$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
