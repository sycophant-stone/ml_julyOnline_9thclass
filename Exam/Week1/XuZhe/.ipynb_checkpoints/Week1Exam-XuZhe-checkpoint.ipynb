{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习九期第一周(机器学习基础)考试\n",
    "#### 考试说明:\n",
    "- 起止时间：请同学在2018年6月25日至7月2日期间完成，最晚提交时间（7月1日24时之前）结束，<b>逾期不接受补考,该考试分数计入平时成绩</b>\n",
    "- 考试方式：请同学<font color=red><b>拷贝</b></font>该试卷至自己姓名的目录后，将文件更名，即上同学姓名拼音后进行作答。格式：Week1Exam-WangWei.ipynb\n",
    "- 提交格式：请同学新建自己姓名全拼的文件夹，将该试卷，数据文件，zip文件等相关考试文件，放置此目录下。将该目录<b>移动</b>至/0.Teacher/Exam/1/目录下\n",
    "- 注意事项：为确保同学们真正了解自身对本周课程的掌握程度，<font color=red><b>请勿翻阅，移动，更改</b></font>其它同学试卷。如发现按0分处理\n",
    "- 请同学在下方同学姓名处填写自己的姓名，批改人和最终得分不用填写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名:许喆\n",
    "- 批改人：   \n",
    "- 最终得分:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问答题(共10题，每题10分，共计100分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.请分别解释统计机器学习中的输入/输出空间，特征空间，假设空间与参数空间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "输入‘X’可能取值的集合就是输入空间\n",
    "输出‘Y’可能取值的集合就是输出空间\n",
    "‘X’的特征构成的集合就是特征空间\n",
    "机器学习中可能的函数构成的空间称为假设空间\n",
    "函数的参数构成的空间就是参数空间\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.什么是损失函数，并举例有哪些常用的损失函数？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "机器学习模型关于单个样本的预测值与真实值的差称为损失。损失越小，模型越好，如果预测值与真实值相等，就是没有损失。 \n",
    "用于计算损失的函数称为损失函数。常用的损失有：0-1损失函数，平方损失函数，绝对损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3请结合公式进行说明结构风险最小化的含义？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$R_{srm}(f) = \\frac{1}{N}\\sum_{i=1}^{N}(L(y_i, f(x_i)))+\\lambda J(f)$$结构风险是在经验风险的基础上加上表示模型复杂度的正则项。J(f) 是模型的复杂度，模型f越复杂，J(f)值就越大，模型越简单，J(f)值就越小。也就是说，J(f) 是对复杂模型的惩罚。求解的目标是整体达到最小值，经验风险越小，模型越复杂，J(f) 的值越大，就迫使模型不要太复杂。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.谈谈什么是生成式模型与判别式模型，以及各自特点？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "生成式模型(generative model)会对x和y的联合分布p(x,y)进行建模,然后通过贝叶斯公式来求得p(y|x), 最后选取使得p(y|x)最大的yi。\n",
    "判别式模型(discriminative model)则会直接对p(y|x)进行建模.\n",
    "通常来说, 因为生成式模型要对类条件密度p(x|yi)进行建模, 而判别式模型只需要对类后验密度进行建模, 前者通常会比后者要复杂, 更难以建模\n",
    "1.生成式模型都会对数据的分布做一定的假设，当数据满足这些假设时, 生成式模型通常需要较少的数据就能取得不错的效果, 但是当这些假设不成立时, 判别式模型会得到更好的效果.\n",
    "2.生成式模型最终得到的错误率会比判别式模型高, 但是其需要更少的训练样本就可以使错误率收敛\n",
    "3.生成式模型更容易拟合, 比如在朴素贝叶斯中只需要计下数就可以, 而判别式模型通常都需要解决凸优化问题\n",
    "4.当添加新的类别时, 生成式模型不需要全部重新训练,而判别式模型则需要全部重新训练.\n",
    "5.生成式模型可以更好地利用无标签数据,而判别式模型不可以.\n",
    "6.生成式模型可以生成x, 因为判别式模型是对p(x,y)进行建模,而判别式模型不可以生成x.\n",
    "7.判别式模型可以对输入数据x进行预处理, 而生成式模型不是很方便进行替换."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.请解释范数，并写出L1,L2范数的定义公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "范数（Norm）是一个函数，其赋予某个向量空间（或矩阵）中的每个向量以长度或大小。\n",
    "$$||x||_{1}=\\sum_{i=1}^{n}|x_{i}|$$\n",
    "$$||x||_{2}=\\sqrt{\\sum_{i=1}^{n}x_{i}^{2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. 什么是信息熵，它的公式是？什么是信息增益?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "信息熵是度量样本集合纯度的一种指标$$Ent(D) = -\\sum_{k=1}^{|y|}p_{k}log_2p_{k}$$\n",
    "考虑到不同的分支结点所包含的样本数不同，给分支结点赋予权重，即样本数越多的分支的影响越大，可计算出用属性a对样本D进行划分所获得的\"信息增益\"$$Gain(D, a) = Ent(D) - \\sum_{v=1}^{V}\\frac{|D^v|}{|D|}Ent(D^v)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. 贝叶斯公式是什么？您如何理解贝叶斯思想？它与频率派有哪些区别？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "贝叶斯公式\n",
    "$$P(A|B)= \\frac{P(B|A)*P(A)}{P(B)}$$\n",
    "频率派把需要推断的参数$\\theta$看做是固定的未知常数，即概率虽然是未知的，但最起码是确定的一个值，同时，样本X 是随机的，所以频率派重点研究样本空间，大部分的概率计算都是针对样本X 的分布.\n",
    "贝叶斯派的观点则截然相反，他们认为参数是随机变量，而样本X 是固定的，由于样本是固定的，所以他们重点研究的是参数的分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.请从最大似然的角度去解释逻辑回归,以及逻辑回归的损失函数是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "二值分类，则$$P(y=1|x;\\theta)=h_{\\theta}(x)$$ $$P(y=0|x;\\theta)=1-h_{\\theta}(x)$$写成概率的一般形式$$p(y|x;\\theta)=(h_{\\theta}(x))^{y}(1-h_{\\theta}(x))^{1-y}$$由最大似然估计原理，我们可以通过m个训练样本值，来估计出$\\theta$值，使得似然函数值最大，对$L(\\theta)$求log可得$$l(\\theta)=\\sum_{i=1}^{m}y^{(i)}log(h(x^{(i)}))+(1-y^{(i)})log(1-h(x^{(i)}))$$也就是对数损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.试析Min-Max与Z-Score这两种数据缩放各自特点，和为什么树形结构不需要做缩放？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Min-max 标准化数据缩放对原始数据进行线性变换。设minA和maxA分别为属性A的最小值和最大值，将A的一个原始值x通过min-max标准化映射成在区间[0,1]中的值$$x^{'}=\\frac{x - x_{min}}{x_{max}-x_{min}}$$\n",
    "z-score方法基于原始数据的均值(mean)和标准差(standard deviation)进行数据的标准化。将A的原始值x使用z-score标准化到x’。z-score标准化方法适用于属性A的最大值和最小值未知的情况，或有超出取值范围的离群数据的情况。将数据按其属性(按列进行)减去其均值，然后除以其方差。最后得到的结果是，对每个属性/每列来说所有数据都聚集在0附近，方差值为1$$x^{`}=\\frac{x-\\mu}{\\sigma}$$\n",
    "决策树生成过程中使用的是熵或基尼系数，根据数据的不一致程度或者混乱程度来计算，不依赖数据本身的绝对大小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. 请解释下随机森林和Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "随机森林是由多个决策树构成(M个）， 每个决策树单独对数据进行预测， 最终结果取这M个决策树中类别最多的那一类。不同分类器是相互独立的。 \n",
    "随机森林对输入样本进行有放回采样，对特征进行采样。\n",
    "GBDT是以决策树（CART）为基学习器的GB算法，核心在于每一棵树学的是之前所有树结论和的残差。\n",
    "xgboos也是以（CART）为基学习器的GB算法**，但是扩展和改进了GDBT。\n",
    "xgboost在代价函数里自带加入了正则项，用于控制模型的复杂度。\n",
    "xgboost在进行节点的分裂时，支持各个特征多线程进行增益计算，因此算法更快，准确率也相对高一些。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>####答卷结束####</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本周课程意见反馈(非必答)\n",
    "请同学围绕以下两点进行回答：\n",
    "- 自身总结：您自己在本周课程的学习，收获，技能掌握等方面进行总结，包括自身在哪些方面存在哪些不足，欠缺，困惑。作为将来回顾学习路径时的依据。\n",
    "- 课程反馈：也可以就知识点，进度，难易度，教学方式，考试方式等等进行意见反馈，督促我们进行更有效的改进，为大家提供更优质的服务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$\\phi \\Phi \\emptyset$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
