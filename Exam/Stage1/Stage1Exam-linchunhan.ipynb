{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习第九期第一阶段考试\n",
    "#### 考试说明:\n",
    "- 起止时间：请同学在2018年7月4日至7月10日期间完成，最晚提交时间（7月10日24时之前）结束，<b>逾期不接受补考,该考试分数计入平时成绩</b>\n",
    "- 考试方式：请同学将该试卷进行复制后，改名为自己姓名后，如State1Exam-WangWei.ipynb，<b>移动</b>至/0.Teacher/Exam/Stage1/目录下后，进行答题。\n",
    "- 注意事项：为确保同学们真正了解自身对本周课程的掌握程度，<font color=red><b>请勿翻阅，移动，更改</b></font>其它同学试卷。如发现按0分处理\n",
    "- 请同学在下方同学姓名处填写自己的姓名，批改人和最终得分不用填写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名:蔡大忠\n",
    "- 批改人：   \n",
    "- 最终得分:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问答题(共10题，每题10分，共计100分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.请说明线性分类器与非线性分类器有哪些区别，具体应用场景有哪些不同？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "线性分类器：模型输出和特征是线性函数，类别可以用超平面分割；\n",
    "非线性分类器：模型的类别分割平面是曲面或者超平面组合；\n",
    "相对于线性分类器，非线性分类器的学习能力更强；\n",
    "\n",
    "线性分类器应用场景：线性回归，逻辑回归，线性核svm\n",
    "非线性分类器：knn，树模型\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.逻辑回归中为什么常常要对特征进行离散化?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "因为逻辑回归使用了y=wx+b作为g(z)，对于某些特征，特征与最终输出并不是线性关系，有可能是完全不同的趋势，所以需要对特征进行离散化处理，不同的段赋予不同的权重进行拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.随机森林如何处理缺失值？随机森林如何评估特征重要性？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "随机森林处理缺失值：首先将有缺失值的样本集合和无缺失值的样本集合分开。对于无缺失值的样本集合，将缺失值对应特征作为标签列，其他特征数\n",
    "据作为数据集，利用随机森林进行模型训练，再用该模型对有缺失值的样本集合进行预测，最终得出缺失值的预测值，完成缺失值的处理。\n",
    "使用gini指数或者decrese accuarcy评估特征重要性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 请说明梯度下降算法如何实现，以及它与牛顿法的不同？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1、梯度下降法释沿着负梯度方向，即函数变化最快的方向寻找函数的极值\n",
    "2、牛顿法使用了泰勒展开，寻找自变量变化的最优方向和步长"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 简要谈下您理解的的机器学习领域的正则化？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "正则化是机器学习为了防止算法过拟合引入的，主要目的是提高模型的泛化能力，控制模型的复杂度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6 带核的SVM为什么能分类非线性问题？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "因为核函数将特征从低维空间映射到了高维空间，低维空间线性不可分的问题很大概率在高维空间是线性可分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7 请举例有哪些常用核函数，以及核函数的条件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "常用的核函数包括：\n",
    "①线性核函数 K(x,z)=xz<br>\n",
    " \n",
    "②多项式核函数 K(x,z)=(γxz+r)d<br>\n",
    "③高斯核函数(径向基函数) K(x,z)=exp(−γ||x−z||2)<br>\n",
    "④ Sigmoid<br>\n",
    " 函数 K(x,z)=tanh(γxz+r)<br>\n",
    "    \n",
    "通常所说的核函数都是正定核函数。一个函数要想成为正定核函数，必须满足它里面任意点的集合形成的 Gram\n",
    " 矩阵是半正定的，即：\n",
    "对于任意的 xi∈χ,i=1,2,3,...,N\n",
    " ， K(x,z)\n",
    " 对应的 Gram\n",
    " 矩阵 K=[K(xi,xj)]\n",
    " 是半正定矩阵，则 K(x,z)\n",
    " 是正定核函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.什么是偏差与方差？当你模型受到低偏差和高方差困扰时，如何解决？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "偏差：偏差是模型期望预测值与真实值之间的差异，描述了算法本身的学习能力。<br>\n",
    "方差：方差是模型预测值与模型期望预测的差异（期望预测是在不同数据训练得到的模型在同一测试样本的预测值的平均），描述了数据带来的影响。<br>\n",
    "低偏差高方差意味着过拟合，需要加正则化项或者加大数据量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.请说明熵、联合熵、条件熵、相对熵、互信息的定义（要求公式），以及您对这些定义的理解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "熵在信息论中代表随机变量不确定度的度量。\n",
    "$$H(X)=-\\sum_{x\\varepsilon \\chi }p(x)logp(x)$$\n",
    " \n",
    "联合熵的定义，代表X,Y同时发生的不确定性.\n",
    "$$H(X,Y)=-\\sum_{x\\varepsilon \\chi, y\\varepsilon \\gamma }^{x,y}p(x,y)logp(x,y)$$\n",
    "\n",
    "条件熵是已知随机变量X的条件下随机变量Y的不确定性。\n",
    "$$H(Y|X)=\\sum_{x\\varepsilon X}\\sum_{y\\varepsilon Y}p(x,y)log(y|x)$$\n",
    " \n",
    "相对熵是X取值的两个概率分布，则p对q的相对熵为：\n",
    "$$D(p||q)=\\sum_{i=1}^{n}p(x)log\\frac{p(x)}{q(x)}$$\n",
    " \n",
    "互信息是指两个事件集合之间的相关性，互信息反映的是已知随机变量 X的取值条件下随机变量 Y的不确定性的减少量：\n",
    "$$I(X,Y)=\\sum_{x\\varepsilon X}\\sum_{y\\varepsilon Y}p(x,y)log\\frac{p(x,y)}{p(x)p(y)}$$\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 请简要说明您对EM算法的理解，并列举有哪些常用的采用EM 算法求解的模型？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "EM算法是一种迭代优化算法，通过观测数据（已知量）的求出隐变量（比如说类别）和分布参数，算法推到使用了极大似然估计和琴声不等式，算法分为E步和M步，E步是根据假设的分布参数求出隐变量的值，M步是通过隐变量求出概率分布，再极大化这个分布函数求出新的分布参数，直至收敛就可以求出近似最优的隐变量和数据的分布<br>\n",
    "常用的EM算法模型：K-means，LDA，GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>####答卷结束####</h1></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
