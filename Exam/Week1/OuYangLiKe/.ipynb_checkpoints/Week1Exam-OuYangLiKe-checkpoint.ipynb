{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习九期第一周(机器学习基础)考试\n",
    "#### 考试说明:\n",
    "- 起止时间：请同学在2018年6月25日至7月2日期间完成，最晚提交时间（7月1日24时之前）结束，<b>逾期不接受补考,该考试分数计入平时成绩</b>\n",
    "- 考试方式：请同学<font color=red><b>拷贝</b></font>该试卷至自己姓名的目录后，将文件更名，即上同学姓名拼音后进行作答。格式：Week1Exam-WangWei.ipynb\n",
    "- 提交格式：请同学新建自己姓名全拼的文件夹，将该试卷，数据文件，zip文件等相关考试文件，放置此目录下。将该目录<b>移动</b>至/0.Teacher/Exam/1/目录下\n",
    "- 注意事项：为确保同学们真正了解自身对本周课程的掌握程度，<font color=red><b>请勿翻阅，移动，更改</b></font>其它同学试卷。如发现按0分处理\n",
    "- 请同学在下方同学姓名处填写自己的姓名，批改人和最终得分不用填写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名:<u></u>  \n",
    "- 批改人：   \n",
    "- 最终得分:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问答题(共10题，每题10分，共计100分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.请分别解释统计机器学习中的输入/输出空间，特征空间，假设空间与参数空间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "输入空间：即输入的X的所有可能取的值的集合\n",
    "输出空间：即经过算法计算输入的值后，输出所有可能取值的集合\n",
    "特征空间：输入的n维向量x，的每一个维度被称为特征或属性，而所有特征（维度）组合起来所张成的空间就是特征空间，其实和输入空间差不多\n",
    "假设空间：由输入空间到输出空间的映射（函数f）的集合，即所有可能的函数所构成的空间\n",
    "参数空间：即所求模型的目标函数（损失函数）的所有可能取的参数所构成的空间\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.什么是损失函数，并举例有哪些常用的损失函数？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "即模型的预测值f(X)与真实Y之间的误差程度,记作L(Y,f(X))\n",
    "常用的损失函数有:\n",
    "0-1损失函数 L(Y,f(X)) = 1 或 0  若Y不等于f(X)为1, 等于则为0\n",
    "平方损失函数 L(Y,f(X)) =(Y-f(x))的平方 , 常用于线性回归\n",
    "绝对损失函数 L(Y,f(X)) =|Y-f(x))|\n",
    "对数损失函数 L(Y,f(X)) =-logP(Y|X), 用于逻辑回归\n",
    "合页损失函数 L(Y,f(X)) =max(0,h) ,其中h=1-Y(W·X+b), 用于svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3请结合公式进行说明结构风险最小化的含义？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "结构风险最小化是为了防止模型过拟合而提出的策略,等价于正则化,即在损失函数后面加上表示模型复杂度的正则化项,如下所示:\n",
    "    R(f) = 1/N∑L(y,f(x)) + λJ(f)  其中,1/N∑L(y,f(x)) 为经验损失最小化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.谈谈什么是生成式模型与判别式模型，以及各自特点？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "生成模型计算出的是联合分布概率P(c,x),从而推出各类别的条件概率P(c|x),公式为:P(c|x)=P(c,x)/P(x),因为所以类别的P(x)都一样,\n",
    "所以要比较各类别P(c|x)的大小,只需比较P(c,x)的大小即可,哪个类别的P(c,x)最大,模型就把该样本判定为那个类. 虽然该模型能提供更多的信息,\n",
    "但里面的概率是根据大数定理,近似的看成是频数(所以需要用到的样本量很大,样本越大频数才越接近于真实的概率).\n",
    "\n",
    "判别模型是计算的是条件概率P(c|x),通常模型会设定一个阀值当P(c|x)大于阀值时 判定为正例,小于时判定为负例.\n",
    "需要用到的样本数要比生成模型少得多.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.请解释范数，并写出L1,L2范数的定义公式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "范数是一种强化了的距离概念.\n",
    "L1是向量中各元素的绝对值之和  \n",
    "L2是向量中各元素的平方之和再开发,又称欧式距离 \n",
    "公式如下:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " #### $$ L2 = \\sum_{i=1}^{n}(x_i)^2$$\n",
    " #### $$L1 = \\sum_{i=1}^{n}|x_i|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. 什么是信息熵，它的公式是？什么是信息增益?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "信息熵是度量样本纯度的一种指标,越大样本集纯度越小(即样本集的类别数目越多),越小则样本集越纯\n",
    "信息增益是某个属性a对样本集划分,对纯度的提升,即对信息熵的减少\n",
    "公式分别如下:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 信息熵 ,其中n为样本集类别数目: $$Ent(D)= -\\sum_{i=1}^{n}p_ilog_2p_i$$\n",
    "#### 信息增益,其中V为属性a可能取值的个数,D^V为D中所有在属性a上取值为a^v的样本集:\n",
    "$$Gain(D,a)= Ent(D)-\\sum_{i=1}^{V}\\frac{|D^V|}{|D|}Ent(D^V)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. 贝叶斯公式是什么？您如何理解贝叶斯思想？它与频率派有哪些区别？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "贝叶斯公式:P(c|x)=P(x|c)P(c)/P(x) , 思想是根据条件概率(先给出x的值)来计算样本属于哪个类别\n",
    "\n",
    "频率学派最重要的就是不断的重复(越多越 好, 趋近于无限),而贝叶斯学派讲的都是抽样和分布\n",
    "频率学派认为任何模型都不存在先验;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.请从最大似然的角度去解释逻辑回归,以及逻辑回归的损失函数是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "最大似然估计是手里有很多样本,这些样本已经实现,最大似然估计就是去找一组参数,使得手上这组样本的实现发生的概率是最大的,这样才最切合实际.\n",
    "\n",
    "而逻辑回归里面的函数是Sigmoid函数g(z)=1/(1+e^-z),其中z=w·x+b, w为参数,x为样本特征值,g(z)返回的是一个0到1的值,就是概率,当g(z)>阀值时,\n",
    "样本判为正例y=1,概率为P(Y=1|x)=g(z)=π(x), 当g(z)小与阀值时,样本判为负例y=0,概率为P(Y=0|x)=1-g(z)=1-π(x).\n",
    "\n",
    "每个样本的概率为(其中样本为正例时yi=1,为负例时yi=0):$$\\pi(x_i)^{y_i}(1-\\pi(x_i))^{1-y_i}$$\n",
    "\n",
    "这样样本中所有的正负例的概率连乘之后,就是手上这套样本集实现发生的总概率(似然函数):\n",
    "   $$ \\prod_{i=1}^{N}\\pi(x_i)^{y_i}(1-\\pi(x_i))^{1-y_i}$$\n",
    "由于概率P都是小于1的数,所以连乘后是个很小的数,计算机会低溢出,所以对数化似然函数(连乘变连加):\n",
    " $$ L(w)=\\sum_{i=1}^{N}[y_i\\ln\\pi(x_i)+(1-y_i)\\ln(1-\\pi(x_i))]=\\sum_{i=1}^{N}[y_i(w·x_i+b)-\\ln(1+exp(w·x_i+b))]$$\n",
    "当L(w)取极大值时,即目标函数-L(w)取最小值时的w,就是所求的那组参数\n",
    "其中:\n",
    " $$ z=w·x_i+b , g(z) = \\frac{1}{(1+e^{-z})}$$\n",
    "\n",
    "损失函数:\n",
    "     $$ cost(g(z),y) = -\\ln{g(z)}    　 if 　y = 1$$\n",
    "     $$ cost(g(z),y) = -ln{(1-g(z))}　  if 　y=0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.试析Min-Max与Z-Score这两种数据缩放各自特点，和为什么树形结构不需要做缩放？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Min-Max是把数据缩放到[0,1]内,即输出范围是0到1，公式为: $$x_i = \\frac{x_i - x_{min} }{ x_{max} - x_{min}}$$\n",
    "z-score是标准正态化,是把数据缩放到均值为0,标准差为1的空间内,输出范围是负无穷到正无穷， 公式为:$$x_i = \\frac{x_i-\\mu}{\\sigma}$$\n",
    "\n",
    "两种方法都能提升模型的收敛速度和精度。但如对数据输出有要求，且数据相对稳定，不存在极端的最大最小值，用min-max缩放；\n",
    "若数据存在异常值和较多噪音，分布不稳定，用标准化，这样通过中心化可以避免异常值对模型的影响。\n",
    "\n",
    "因为缩放完之后，数据集的顺序不会改变，而每个属性的信息熵和信息增益等指标也不会改变，而树形结构是根据这些指标的大小进行分裂的，所以不需要做缩放\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. 请解释下随机森林和Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "随机森林是一种集成模型, 其基学习器是决策树, 由多棵决策树集成而得, 而每个决策树都是随机抽取部分样本集和部分特征训练学习而成,故名随机森林.\n",
    "对新样本的预测是采取投票(分类)的方式 和 取均值(回归)的方式.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>####答卷结束####</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本周课程意见反馈(非必答)\n",
    "请同学围绕以下两点进行回答：\n",
    "- 自身总结：您自己在本周课程的学习，收获，技能掌握等方面进行总结，包括自身在哪些方面存在哪些不足，欠缺，困惑。作为将来回顾学习路径时的依据。\n",
    "- 课程反馈：也可以就知识点，进度，难易度，教学方式，考试方式等等进行意见反馈，督促我们进行更有效的改进，为大家提供更优质的服务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
