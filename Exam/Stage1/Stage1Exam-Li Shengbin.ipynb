{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习第九期第一阶段考试\n",
    "#### 考试说明:\n",
    "- 起止时间：请同学在2018年7月4日至7月10日期间完成，最晚提交时间（7月10日24时之前）结束，<b>逾期不接受补考,该考试分数计入平时成绩</b>\n",
    "- 考试方式：请同学将该试卷进行复制后，改名为自己姓名后，如State1Exam-WangWei.ipynb，<b>移动</b>至/0.Teacher/Exam/Stage1/目录下后，进行答题。\n",
    "- 注意事项：为确保同学们真正了解自身对本周课程的掌握程度，<font color=red><b>请勿翻阅，移动，更改</b></font>其它同学试卷。如发现按0分处理\n",
    "- 请同学在下方同学姓名处填写自己的姓名，批改人和最终得分不用填写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名:李晟彬\n",
    "- 批改人：   \n",
    "- 最终得分:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问答题(共10题，每题10分，共计100分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.请说明线性分类器与非线性分类器有哪些区别，具体应用场景有哪些不同？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1. 线性linear，指量与量之间按比例、成直线的关系，在数学上可以理解为一阶导数为常数的函数；\n",
    "   非线性non-linear则指不按比例、不成直线的关系，一阶导数不为常数。\n",
    "2. 线性的可以认为是1次曲线，比如y=ax+b ,即成一条直线\n",
    "   非线性的可以认为是2次以上的曲线，比如y=ax^2+bx+c，(x^2是x的2次方)，即不为直线的即可\n",
    "3. 两个变量之间的关系是一次函数关系的——图象是直线，这样的两个变量之间的关系就是“线性关系”；\n",
    "   如果不是一次函数关系的——图象不是直线，就是“非线性关系\n",
    "4. “线性”与“非线性”，常用于区别函数y = f (x)对自变量x的依赖关系。线性函数即一次函数，其图像为一条直线。其它函数则为非线性函数，其图像不是直线。\n",
    "　　线性，指量与量之间按比例、成直线的关系，在空间和时间上代表规则和光滑的运动；而非线性则指不按比例、不成直线的关系，代表不规则的运动和突变。 \n",
    "5. 在数学上，线性关系是指自变量x与因变量yo之间可以表示成y=ax+b ,（a,b为常数），即说x与y之间成线性关系。\n",
    "   不能表示成y=ax+b ,（a,b为常数），即非线性关系，非线性关系可以是二次，三次等函数关系，也可能是没有关系。\n",
    "线性算法的优点是训练和预测的效率比较高，但最终效果对特征的依赖程度较高，需要数据在特征层面上是线性可分的。\n",
    "非线性分类器效果拟合能力较强，不足之处是数据量不足容易过拟合、计算复杂度高、可解释性不好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.逻辑回归中为什么常常要对特征进行离散化?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；(哑变量)\n",
    "特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.随机森林如何处理缺失值？随机森林如何评估特征重要性？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "一般的数据中，常见的数据缺失情况有单变量缺失和多变量缺失两种情况。单变量缺失就是样本的某一特征有缺失，其它特征的值都是完整的；而多变量缺失是样本的多个特征都有不同程度的缺失。下面针对这两种情况分别介绍如何用随机森林进行填补\n",
    "单变量缺失的数据填补：\n",
    "用未缺失的特征数据构成的向量建立随机森林模型，对确实变量就行预测。\n",
    "多变量缺失的数据填补：\n",
    "首先，需要通过简单的填补(比如均值填补)对矩阵X进行初始填补，然后按照缺失数据的从少到多对矩阵X的列进行顺序调整(调整列顺序后的矩阵X第一列的缺失数据应该是最少的，最后一列的缺失数据是最多的)\n",
    "用随机森林进行特征重要性评估的思想其实很简单，说白了就是看看每个特征在随机森林中的每颗树上做了多大的贡献，然后取个平均值，最后比一比特征之间的贡献大小。 \n",
    "通常可以用基尼指数（Gini index）或者袋外数据（OOB）错误率作为评价指标来衡量。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 请说明梯度下降算法如何实现，以及它与牛顿法的不同？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "梯度下降法用来求解目标函数的极值。这个极值是给定模型给定数据之后在参数空间中搜索找到的。迭代过程为：\n",
    "$$\\theta _{j}:=\\theta _{j}-\\alpha \\frac{\\partial }{\\partial \\theta }J(\\theta )$$\n",
    "这个与梯度下降不同，梯度下降的目的是直接求解目标函数极小值，而牛顿法则变相地通过求解目标函数一阶导为零的参数值，进而求得目标函数最小值。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 简要谈下您理解的的机器学习领域的正则化？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项或罚项。正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值就越大。比如，正则化项可以是模型参数向量的范数。\n",
    "不管是L1还是L2，都是针对模型中参数过大的问题引入惩罚项。而在深度学习中，要优化的变成了一个个矩阵，参数变得多出了几个数量级，过拟合的可能性也相应的提高了。而要惩罚的是神经网络中每个神经元的权重大小，从而避免网络中的神经元走极端抄近路。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6 带核的SVM为什么能分类非线性问题？ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "核函数的本质是两个函数的內积，通过核函数将其隐射到高维空间，在高维空间非线性问题转化为线性问题, SVM得到超平面是高维空间的线性分类平面。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7 请举例有哪些常用核函数，以及核函数的条件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "线性核函数：\n",
    "$$k(x,x_{i})=x\\cdot x_{i}$$\n",
    "多项式核函数：\n",
    "$$k(x,x_{i})=((x\\cdot x_{i})+1)^{d}$$\n",
    "高斯（RBF）核函数：\n",
    "$$k(x,x_{i})=exp(-\\frac{\\left \\| x -x_{i}\\right \\|^{2}}{\\delta ^{2}})$$\n",
    "sigmoid核函数:\n",
    "$$k(x,x_{i})=tanh(\\eta <x,x_{i} > +\\theta )$$\n",
    "核函数的充要条件是K矩阵是半正定的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.什么是偏差与方差？当你模型受到低偏差和高方差困扰时，如何解决？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "偏差：描述的是预测值（估计值）的期望与真实值之间的差距。偏差越大，越偏离真实数据。\n",
    "方差：描述的是预测值的变化范围，离散程度，也就是离其期望值的距离。方差越大，数据的分布越分散。\n",
    "当模型低偏差和高方差时，属于过拟合。\n",
    "当模型处于过拟合状态时，根本的办法是降低模型的复杂度。我们一般有以下一些办法：\n",
    "1）获取更多的数据：训练数据集和验证数据集是随机选取的，它们有不同的特征，以致在验证数据集上误差很高。更多的数据可以减小这种随机性的影响。\n",
    "2）减少特征数量\n",
    "3）增加正则化权重：方差很高时，模型对训练集的拟合很好。实际上，模型很有可能拟合了训练数据集的噪声，拿到验证集上拟合效果就不好了。我们可以增加正则化权重，减小模型的复杂度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.请说明熵、联合熵、条件熵、相对熵、互信息的定义（要求公式），以及您对这些定义的理解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "熵在信息论中代表随机变量不确定度的度量。\n",
    "$$H(X)=-\\sum_{x\\varepsilon \\chi }p(x)logp(x)$$\n",
    "联合熵的定义，代表X,Y同时发生的不确定性.\n",
    "$$H(X,Y)=-\\sum_{x\\varepsilon \\chi ,y\\varepsilon \\gamma }^{x,y}p(x,y)lnp(x,y)$$\n",
    "条件熵是已知随机变量X的条件下随机变量Y的不确定性。\n",
    "$$H(Y|X)=\\sum_{x\\varepsilon X}\\sum_{y\\varepsilon \\gamma }p(x,y)log(y|x)$$\n",
    "相对熵是X取值的两个概率分布，则p对q的相对熵为：\n",
    "$$D(p||q)=\\sum_{i=1}^{n}p(x)log\\frac{p(x)}{q(x)}$$\n",
    "互信息是指两个事件集合之间的相关性，由于事件A发生与事件B发生相关联而提供的信息量：\n",
    "$$I(A,B)=log_{2}\\frac{p(B|A)}{P(B)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 请简要说明您对EM算法的理解，并列举有哪些常用的采用EM 算法求解的模型？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EM算法主要是两步,E步选择出合适的隐变量分布（一个以观测变量为前提条件的后验分布），使得参数的似然函数与其下界相等；M步：极大化似然函数的下界，拟合出参数.\n",
    "采用EM算法求解的模型有： 高斯混合模型 ，混合朴素贝叶斯模型，因子分析模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>####答卷结束####</h1></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
