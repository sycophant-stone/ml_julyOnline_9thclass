{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习第九期第一阶段考试\n",
    "#### 考试说明:\n",
    "- 起止时间：请同学在2018年7月4日至7月10日期间完成，最晚提交时间（7月10日24时之前）结束，<b>逾期不接受补考,该考试分数计入平时成绩</b>\n",
    "- 考试方式：请同学将该试卷进行复制后，改名为自己姓名后，如State1Exam-WangWei.ipynb，<b>移动</b>至/0.Teacher/Exam/Stage1/目录下后，进行答题。\n",
    "- 注意事项：为确保同学们真正了解自身对本周课程的掌握程度，<font color=red><b>请勿翻阅，移动，更改</b></font>其它同学试卷。如发现按0分处理\n",
    "- 请同学在下方同学姓名处填写自己的姓名，批改人和最终得分不用填写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名:\n",
    "- 批改人：   \n",
    "- 最终得分:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问答题(共10题，每题10分，共计100分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.请说明线性分类器与非线性分类器有哪些区别，具体应用场景有哪些不同？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "区别:  \n",
    "线性分类器：可解释性好，计算复杂度较低，不足之处是模型的拟合效果相对弱些，比如欠拟合。  \n",
    "常见：LR,贝叶斯分类，单层感知机、线性回归\n",
    "\n",
    "非线性分类器：效果拟合能力较强，不足之处是数据量不足容易过拟合、计算复杂度高、可解释性不好。  \n",
    "常见： 决策树、RF、GBDT、多层感知机\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.逻辑回归中为什么常常要对特征进行离散化?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. 稀疏向量內积乘法运算速度快，计算结果方便存储，容易扩展\n",
    "2. 离散化后的特征对异常数据具有很强的鲁棒性。\n",
    "3. 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合。\n",
    "4. 离散化后可以进行特征交叉，由M+N变量变成M*N个变量，进一步引入非线性，提升表达能力\n",
    "5. 特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。\n",
    "6. 离散特征的增加和减少都很容易计算，易于模型的快速迭代\n",
    "7. 特征离散化以后，起到了简化逻辑回归模型的作用，降低了模型过拟合的风险"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.随机森林如何处理缺失值？随机森林如何评估特征重要性？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "处理缺失值方法有2：  \n",
    "1. 对于训练集，同一个class下的数据，如果分类变量缺失，用众数补上，如果是连续型变量缺失，用中位数补。  \n",
    "2. 先用na.roughfix补上缺失值，然后构建森林并计算proximity matrix，再回头看缺失值，如果是分类变量，则用没有缺失的观测实例的proximity中的权重进行投票。如果是连续型变量，则用proximity矩阵进行加权平均的方法补缺失值。然后迭代4-6次。\n",
    "\n",
    "衡量变量重要性的方法有两种，Decrease GINI 和 Decrease Accuracy：  \n",
    "1. Decrease GINI： 对于回归问题，直接使用argmax(VarVarLeftVarRight)作为评判标准，即当前节点训练集的方差Var减去左节点的方差VarLeft和右节点的方差VarRight。\n",
    "2. Decrease Accuracy：对于一棵树Tb(x)，我们用OOB样本可以得到测试误差1；然后随机改变OOB样本的第j列：保持其他列不变，对第j列进行随机的上下置换，得到误差2。至此，我们可以用误差1-误差2来刻画变量j的重要性。基本思想就是，如果一个变量j足够重要，那么改变它会极大的增加测试误差；反之，如果改变它测试误差没有增大，则说明该变量不是那么的重要。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 请说明梯度下降算法如何实现，以及它与牛顿法的不同？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "对每个变量求偏单，设定步长和迭代次数，进行迭代，计算出区间内的一个极值点。  \n",
    "与牛顿法相比，收敛速度相对慢一些，计算简单些。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 简要谈下您理解的的机器学习领域的正则化？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "正则化是为了防止过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6 带核的SVM为什么能分类非线性问题？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "核函数的本质是两个函数的內积，通过核函数将其映射到高维空间，在高维空间转换为线性问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7 请举例有哪些常用核函数，以及核函数的条件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "线性核函数  \n",
    "$k(x,x_{i}) = x\\cdot x_{i}$\n",
    "多项式核函数：  \n",
    "$k(x,x_{i}) = ((x\\cdot x_{i})+1)^{d}$\n",
    "高斯核函数：  \n",
    "$k(x,x_{i}) = exp(-\\frac{\\left \\| x-x_{i} \\right \\|^{2}}{\\delta^{2}})$\n",
    "sigmoid核函数:  \n",
    "$k(x,x_{i}) = tanh(\\eta < x,x_{i}> + \\theta )$\n",
    "\n",
    "条件：  \n",
    "对应正定核函数，其充分必要条件是对任意的x属于X，要求K对应的Gram矩阵要是半正定矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.什么是偏差与方差？当你模型受到低偏差和高方差困扰时，如何解决？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "偏差：期望输出与真实标记的差别，即：$bias^{2}(x) = $\n",
    "方差: 描述预测值的变化范围，离散程度，也就是离其期望值的距离。\n",
    "对于高方差，一般采用平均值法，比如bagging，或者模型简化/降维方法，来降低方差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.请说明熵、联合熵、条件熵、相对熵、互信息的定义（要求公式），以及您对这些定义的理解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$H(X) = -\\sum_{i=1}^{n}P(X_{k}) logP(X_{k})$  \n",
    "联合熵：  \n",
    "$H(X,Y) = -\\sum p(X,Y)lnp(X,Y) $  \n",
    "条件熵：\n",
    "$H(X|Y) = H(X,Y) - H(Y)= -\\sum_{x,y}^{ } p(x,y)logp(x|y)$  \n",
    "相对熵：  \n",
    "$D(p||q) = \\sum_{x}^{ }p(x)log\\frac{p(x)}{q(x)} = E_{p(x)}log\\frac {p(x)}{q(x)}$  \n",
    "互信息：  \n",
    "$I(X,Y) = \\sum_{x,y}^{ } p(x,y)log\\frac{p(x,y)}{p(x)p(y)}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 请简要说明您对EM算法的理解，并列举有哪些常用的采用EM 算法求解的模型？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "当样本集中含有隐含变量，而求解模型参数时一般采用最大似然估计，由于含有隐含变量，所以对似然函数参数求导是求解不出来，这时就需要用EM算法加入隐含变量求模型参数。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>####答卷结束####</h1></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
