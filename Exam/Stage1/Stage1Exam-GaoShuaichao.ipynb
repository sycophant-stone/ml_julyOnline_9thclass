{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习第九期第一阶段考试\n",
    "#### 考试说明:\n",
    "- 起止时间：请同学在2018年7月4日至7月10日期间完成，最晚提交时间（7月10日24时之前）结束，<b>逾期不接受补考,该考试分数计入平时成绩</b>\n",
    "- 考试方式：请同学将该试卷进行复制后，改名为自己姓名后，如State1Exam-WangWei.ipynb，<b>移动</b>至/0.Teacher/Exam/Stage1/目录下后，进行答题。\n",
    "- 注意事项：为确保同学们真正了解自身对本周课程的掌握程度，<font color=red><b>请勿翻阅，移动，更改</b></font>其它同学试卷。如发现按0分处理\n",
    "- 请同学在下方同学姓名处填写自己的姓名，批改人和最终得分不用填写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名:高帅超\n",
    "- 批改人：   \n",
    "- 最终得分:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问答题(共10题，每题10分，共计100分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.请说明线性分类器与非线性分类器有哪些区别，具体应用场景有哪些不同？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "线性分类器主要是指样本可以通过超平面进行分开；代表有：LDA，逻辑回归，Linear SVM等。  \n",
    "非线性分类器则是通过曲面或者多个超平面的组合将样本区分开来；代表有：决策树、KNN，非线性SVM等。  \n",
    "对于特征维度较低的数据常选用非线性分类器，而特征维度较高时常用线性分类器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.逻辑回归中为什么常常要对特征进行离散化?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "离散数据易于处理，计算简单；  \n",
    "简化模型；  \n",
    "通过将连续数据离散化，可引入非线性，提高模型的泛化能力，同时也可增强对噪声的抗干扰能力；\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.随机森林如何处理缺失值？随机森林如何评估特征重要性？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "处理缺失值：  \n",
    "方法1:对于训练集,同一个类别下的数据，分类变量缺失，用众数补上；连续型变量缺失，用中位数补。  \n",
    "方法2：先用方法1补，然后构建森林并计算proximity matrix，再回头看缺失值，如果是分类变量，则用没有缺失的观测实例的proximity中的权重进行\n",
    "投票。如果是连续型变量，则用proximity矩阵进行加权平均的方法补缺失值，然后迭代。  \n",
    "评估特征重要性：  \n",
    "Decrease GINI： 对于回归问题，当前节点训练集的方差Var减去左节点的方差$Var_{Left}$和右节点的方差$Var_{Right}$，$\\arg\\max(Var−Var_{Left}−Var_{Right})$作为评判标准。  \n",
    "Decrease Accuracy：如果一个变量足够重要，那么改变它会极大的增加测试误差。对于一棵树$T_{b}(x)$，我们用OOB样本可以得到测试误差1；然后保持其他列不变,随机改变OOB样本的第j列，得到误差2。用误差1-误差2来刻画变量j的重要性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 请说明梯度下降算法如何实现，以及它与牛顿法的不同？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "梯度下降法是一种迭代算法，常用于求解凸函数的极小值。首先计算各个自变量的偏导数，计算梯度。随机选择一初始点，计算该点的梯度，沿负梯度方向进行更新，直至达到最大迭代次数或函数变化值小于某个范围。  \n",
    "梯度下降法属于一阶逼近，而牛顿法涉及二阶导数，属于二阶逼近。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 简要谈下您理解的的机器学习领域的正则化？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "正则化主要起到防止过拟合，降低模型复杂度的作用。  \n",
    "$L_{1}$正则化是指在损失函数后加上模型参数的1范数（$||w||_{1}=\\sum_{i=1}^{n}|w_{i}|$）  \n",
    "$L_{2}$是在损失函数后面加上模型参数的2范数（$||w||_{2}=\\sqrt{\\sum_{i=1}^{n}|w_{i}|^{2}})$  \n",
    "$L_{1}$正则化会产生稀疏的特征；$L_{2}$会产生更多接近于0的特征。  \n",
    "两者都可用于防止过拟合，同时$L_{1}$也常用于特征选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6 带核的SVM为什么能分类非线性问题？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "通过核函数将原本在低维空间中线性不可分的样本映射到了高维空间中使其变得线性可分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7 请举例有哪些常用核函数，以及核函数的条件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "线性核：$K(x,z)=x^{T}z$   \n",
    "多项式核函数：$K(x,z)=(x\\cdot z+1)^{p}$  \n",
    "高斯核：$K(x,Z)=exp(-\\frac{||x-z||^{2}}{2\\sigma^{2}})$  \n",
    "条件：函数为半正定\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.什么是偏差与方差？当你模型受到低偏差和高方差困扰时，如何解决？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "偏差：预测值和真实值之间的差别。衡量偏离程度，即算法本身的拟合能力；  \n",
    "方差：在不同训练集上产生的方差，刻画离散程度，数据扰动造成的影响；  \n",
    "低偏差高方差即为过拟合，解决方法：增加数据量；添加正则项；特征选择；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.请说明熵、联合熵、条件熵、相对熵、互信息的定义（要求公式），以及您对这些定义的理解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "熵：$H(X) = -\\sum_{i=1}^{n}P(X_{i})\\log{P(X_{i}})$  衡量随机变量X的不确定性  \n",
    "联合熵：$H(X,Y) = -\\sum_{i=i}^{n}\\sum_{j=1}^{m}p(x_{i},y_{j})\\log{p(x_{i},y_{j})}$  X Y同时发生的不确定性  \n",
    "条件熵：$H(Y|X)=-\\sum_{i=i}^{n}\\sum_{j=1}^{m}p(x_{i},y_{j})\\log{\\frac{p(x_{i},y_{j})}{p(x_{i})}}$  在X已发生的前提下，Y发生的不确定性  \n",
    "相对熵：$D_{KL}(p||q)=\\sum_{i=1}^{n}p(x_{i})\\log\\frac{p(x_{i})}{q(x_{i})}$  p和q分布之间的差异性  \n",
    "互信息：$I(X,Y)=\\sum_{i=i}^{n}\\sum_{j=1}^{m}p(x_{i},y_{j})\\log{\\frac{p(x_{i},y_{j})}{p(x_{i})p(y_{j})}}$ 已知X的信息使得Y的信息的不确定性减少的程度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 请简要说明您对EM算法的理解，并列举有哪些常用的采用EM 算法求解的模型？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EM算法属于迭代优化，主要包括两个步骤：E步求期望，M步求极大似然，不断迭代，直至收敛。  \n",
    "常用EM算法求解的模型有：混合高斯模型；k均值聚类；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>####答卷结束####</h1></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
