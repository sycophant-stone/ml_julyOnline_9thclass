{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习第九期第一阶段考试\n",
    "#### 考试说明:\n",
    "- 起止时间：请同学在2018年7月4日至7月10日期间完成，最晚提交时间（7月10日24时之前）结束，<b>逾期不接受补考,该考试分数计入平时成绩</b>\n",
    "- 考试方式：请同学将该试卷进行复制后，改名为自己姓名后，如State1Exam-WangWei.ipynb，<b>移动</b>至/0.Teacher/Exam/Stage1/目录下后，进行答题。\n",
    "- 注意事项：为确保同学们真正了解自身对本周课程的掌握程度，<font color=red><b>请勿翻阅，移动，更改</b></font>其它同学试卷。如发现按0分处理\n",
    "- 请同学在下方同学姓名处填写自己的姓名，批改人和最终得分不用填写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名:王清\n",
    "- 批改人：   \n",
    "- 最终得分:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问答题(共10题，每题10分，共计100分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.请说明线性分类器与非线性分类器有哪些区别，具体应用场景有哪些不同？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "线性分类器：模型是参数的线性函数，分类平面是（超）平面。非线性分类器：模型是参数的非线性函数，模型的分界面可以是曲面或（超）平面的组合。线性分类器应用场景：感知机、逻辑斯特回归、SVM（线性核）。非线性分类器应用场景：kNN、决策树、SVM（非线性核）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.逻辑回归中为什么常常要对特征进行离散化?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. 离散特征的增加和减少都很容易，易于模型的快速迭代。\n",
    "2. 稀疏向量内积乘法运算快，计算结果方便存储，用以扩展。\n",
    "3. 离散后的特征对异常数据有很强的鲁棒性。\n",
    "4. 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个以后，每个变量都有单独的权重，相当于模型引入了非线性，能够提升模型的表达能力。\n",
    "5. 可以简化逻辑回归模型的作用，降低了模型过拟合的风险。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.随机森林如何处理缺失值？随机森林如何评估特征重要性？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "处理缺失值\n",
    "\n",
    "1. 对于训练集，同一个标签下的数据，如果是分类变量缺失，用众数补上；如果是连续性变量缺失，用中位数补上。\n",
    "\n",
    "2. 先用np.roughfix补上缺失值，然后构建随机森林并计算proximity matrix，则如果是分类变量缺失，用没有缺失的观测实例的proximity中的权重进行投票；如果是连续性变量，用proximity矩阵进行加权平均的方法补缺失值。\n",
    "\n",
    "衡量变量重要性的方法有两种，Decrease GINI 和 Decrease Accuracy：\n",
    "1. Decrease GINI： 对于回归问题，直接使用argmax(Var−VarLeft−VarRight)作为评判标准，即当前节点训练集的方差Var减去左节点的方差VarLeft和右节点的方差VarRight。\n",
    "2. 对于一棵树$ T_b(x)$，我们用OOB样本可以得到测试误差1；然后随机改变OOB样本的第$j$列：保持其他列不变，对第$j$列进行随机的上下置换，得到误差2；至此，我们可以用误差1-误差2来刻画变量$j$的重要性。基本思想就是，如果一个变量$j$足够重要，那么改变它会极大的增加测试误差；反之，如果改变它测试误差没有增大，则说明该变量不是那么的重要。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 请说明梯度下降算法如何实现，以及它与牛顿法的不同？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "目标函数$J(\\theta)$关于参数$\\theta$的梯度就是目标函数上升最快的方向，对于最小化优化问题，只需将参数延梯度方向相反的方向前进一个步长，就可以实现目标函数的下降，该步长称为学习率$\\eta$。参数更新公式为$$ \\theta = \\theta - \\eta * \\nabla_\\theta J(\\theta) $$牛顿法的基本思想是利用迭代点$x_k$处的一阶导数(梯度)和二阶导数(Hessen矩阵)对目标函数进行二次函数近似，然后把二次模型的极小点作为新的迭代点，并不断重复这一过程，直至求得满足精度的近似极小值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 简要谈下您理解的的机器学习领域的正则化？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "正则化是结构风险最小化策略的实现，是在经验风险上加上一个正则化项（regularizer）或罚项（penalty term）。正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值就越大。正则化的一般形式如下：$$ \\mathop{\\min}_{f \\in F} \\ \\ \\ \\frac{1}{N}\\sum_{i=1}^NL(y_i, f(x_i)) + \\lambda J(f)$$第1项为经验风险，第2项为正则化项，$ \\lambda \\geq 0 $为调整两者之间关系的系数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6 带核的SVM为什么能分类非线性问题？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "带核的SVM使用线性分类的方法解决非线性分类的方法，首先使用一个变换将原空间的数据映射到新空间，然后在新空间里利用线性分类的学习方法从训练数据中学习分类模型。核技巧就可以通过一个非线性变换将输入空间对应于一个特征空间，使得在输入空间中的超曲面对应于特征空间中的超平面模型，即带核的SVM能分类非线性问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7 请举例有哪些常用核函数，以及核函数的条件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "多项式核函数：$$K(x, z) = (x * z + 1)^p$$高斯核函数：$$K(x, z) = exp(-\\frac{||x -z||^2}{2\\sigma^2})$$字符串核函数等。\n",
    "\n",
    "核函数条件：只要一个对称函数所对应的核矩阵半正定，它就能作为核函数使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.什么是偏差与方差？当你模型受到低偏差和高方差困扰时，如何解决？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "偏差指算法的预测的平均值与真实值的关系（即算法的拟合能力），方差描述的是同一个算法在不同的数据集上的预测值和所有数据集上的平均预测值之间的关系（即算法的稳定性）。低偏差和高方差即造成了过拟合，可以采用解决过拟合的方法解决此困扰。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.请说明熵、联合熵、条件熵、相对熵、互信息的定义（要求公式），以及您对这些定义的理解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "熵表示信息量的期望，反映不确定性：$$H(x) = -\\sum_i^n P(x_i)log_2 P(x_i)$$联合熵可以看做X,Y的并集：$$H(X, Y) = -\\sum_{i=1}^n\\sum_{j=1}^n P(x_i, y_j) log_2P(x_i, y_j)$$条件熵：$$H(Y|X) = \\sum_{i=1}^n P(x_i)H(Y|X=x_i)$$相对熵刻画两个分布之间的差异，又叫KL散度：$$D(p||q) = \\sum_{i=1}^n P(x_i) log \\frac{p(x_i)}{q(x_i)}$$互信息表示变量之间相关性的度量：$$I(x, y) = -\\sum_{i=1}^n\\sum_{j=1}^n P(x_i, y_j) log\\frac{P(x_i, y_j)}{P(x_i)P(y_j)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 请简要说明您对EM算法的理解，并列举有哪些常用的采用EM 算法求解的模型？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "EM算法是一种迭代优化策略，由于它的计算方法中每一次迭代都分两步，其中一个为期望步（E步），另一个为极大步（M步），所以算法被称为EM算法（Expectation Maximization Algorithm）。首先根据己经给出的观测数据，估计出模型参数的值；然后再依据上一步估计出的参数值估计缺失数据的值，再根据估计出的缺失数据加上之前己经观测到的数据重新再对参数值进行估计，然后反复迭代，直至最后收敛，迭代结束。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>####答卷结束####</h1></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
