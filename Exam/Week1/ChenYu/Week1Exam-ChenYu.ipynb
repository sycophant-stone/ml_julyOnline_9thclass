{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习九期第一周(机器学习基础)考试\n",
    "#### 考试说明:\n",
    "- 起止时间：请同学在2018年6月25日至7月2日期间完成，最晚提交时间（7月1日24时之前）结束，<b>逾期不接受补考,该考试分数计入平时成绩</b>\n",
    "- 考试方式：请同学<font color=red><b>拷贝</b></font>该试卷至自己姓名的目录后，将文件更名，即上同学姓名拼音后进行作答。格式：Week1Exam-WangWei.ipynb\n",
    "- 提交格式：请同学新建自己姓名全拼的文件夹，将该试卷，数据文件，zip文件等相关考试文件，放置此目录下。将该目录<b>移动</b>至/0.Teacher/Exam/1/目录下\n",
    "- 注意事项：为确保同学们真正了解自身对本周课程的掌握程度，<font color=red><b>请勿翻阅，移动，更改</b></font>其它同学试卷。如发现按0分处理\n",
    "- 请同学在下方同学姓名处填写自己的姓名，批改人和最终得分不用填写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名: 陈宇  \n",
    "- 批改人：   \n",
    "- 最终得分:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问答题(共10题，每题10分，共计100分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.请分别解释统计机器学习中的输入/输出空间，特征空间，假设空间与参数空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "输入/输出空间：输入与输出所有可能取值的集合\n",
    "\n",
    "特征空间：所有特征向量存在的空间，每一维对应于一个特征\n",
    "\n",
    "假设空间：由输入空间到输出空间的映射的集合\n",
    "\n",
    "参数空间：所有参数向量的集合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.什么是损失函数，并举例有哪些常用的损失函数？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "损失函数：度量一次预测错误程度的函数\n",
    "\n",
    "常用的损失函数：\n",
    "\n",
    "(1) 0-1损失函数 (0-1 loss function)\n",
    "$$\n",
    "\\begin{eqnarray}L(Y,f(X))=\n",
    "\\begin{cases}\n",
    "1, &Y≠f(X)\\cr 0, &Y=f(X)\\end{cases}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "(2) 平方损失函数(quadratic loss function)\n",
    "$$\n",
    "L(Y,f(X))=(Y-f(X))^2\n",
    "$$\n",
    "\n",
    "(3) 绝对损失函数(absolute loss function)\n",
    "$$\n",
    "L(Y,f(X))=|Y-f(X)|\n",
    "$$\n",
    "\n",
    "(4) 对数损失函数(logarithmic loss function)\n",
    "$$\n",
    "L(Y,P(Y|X))=-logP(Y|X)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3请结合公式进行说明结构风险最小化的含义？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "结构风险最小化(structural risk minimization, SRM)是为了防止过拟合而提出来的策略，结构风险最小化等价于正则化(regularization)。结构风险\n",
    "\n",
    "在经验风险上加上表示模型复杂度的正则化项(regularizer)或惩罚项(penalty term)。在假设空间、损失函数以及训练数据集确定的情况下，结构风险\n",
    "\n",
    "的定义是：\n",
    "$$\n",
    "R_{srm}(f)=\\frac{1}{N}\\sum_{i=1}^NL(y_i,f(x_i))+λJ(f)\n",
    "$$\n",
    "\n",
    "其中$J(f)$为模型的复杂度，是定义在假设空间上的泛函，模型$f$越复杂，复杂度$J(f)$就越大；反之，模型越简单，复杂度$J(f)$就越小。也就是\n",
    "\n",
    "说，复杂度表示了对复杂模型的惩罚，$λ≥0$是系数，用以权衡经验风险和模型复杂度。结构风险小需要经验风险与模型复杂度同时小。结构风险小的\n",
    "\n",
    "模型往往对训练数据以及未知的测试数据都有较好的预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.谈谈什么是生成式模型与判别式模型，以及各自特点？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "生成式模型：由数据学习到联合概率分布$P(X,Y)$，然后求出条件分布概率$P(Y|X)$获得的预测模型：\n",
    "$$\n",
    "P(Y|X)=\\frac{P(X,Y)}{P(X)}\n",
    "$$\n",
    "\n",
    "判别式模型：由数据直接学习决策函数$f(X)$或者条件概率分布$P(Y|X)$获得的预测模型\n",
    "\n",
    "特点：\n",
    "\n",
    "(1)生成式模型表示的是给定输入$X$产生输出$Y$的生成关系，它可以还原出联合概率分布$P(X,Y)$；生成式模型的收敛速度更快，即当样本容量增大的\n",
    "\n",
    "时候，学到的模型可以更快地收敛于真实模型；而且当存在隐变量时，仍可以使用生成式模型。\n",
    "\n",
    "(2)判别式模型关心的是对于给定输入$X$，应该预测什么样的输出$Y$,往往学习的准确率更高；由于直接学习$P(Y|X)$或$f(X)$，可以对数据进行各种\n",
    "\n",
    "程度上的抽象、定义特征并使用特征，因此可以简化学习问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.请解释范数，并写出L1,L2范数的定义公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "从宏观上来讲，范数是将向量映射到非负值的函数，从直观上来说，范数是向量x到原点的距离。\n",
    "\n",
    "但是范数并不局限于上述的公式，对任意的函数只要满足以下的性质，则可以称该函数是范数：\n",
    "\n",
    "(1) 对于$f(x)=0$，则一定有$x=0$。\n",
    "\n",
    "(2) $f(x+y)<=f(x)+f(y)$。\n",
    "\n",
    "(3) 对任意的标量$a$，一定有$f(ax)=|a|f(x)$。\n",
    "\n",
    "\n",
    "L1范数：$L_1=||x||_1=\\sum_i|x_i|$\n",
    "\n",
    "L2范数: $L_2=||x||_2=(\\sum_ix_i^2)^{\\frac{1}{2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. 什么是信息熵，它的公式是？什么是信息增益?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "信息熵(Entropy)是表示随机变量不确定性的度量。\n",
    "\n",
    "假设$X$是一个具有有限个值的离散型随机变量，服从以下的概率分布： $P(X=x_i)=p_i, i=1,2,...,n$\n",
    "\n",
    "那么随机变量$X$的熵定义为：$H(X)=-\\sum_{i=1}^np_ilogp_i$\n",
    "\n",
    "在此基础上，条件熵$H(Y|X)$表示在已知随机变量X的取值条件下，随机变量Y的不确定性。其数学定义为X给定的条件下，Y的条件概率分布的熵对X的数\n",
    "\n",
    "学期望。数学表达式如下：$H(Y|X)=\\sum_{j=1}^kH(Y|X=x_j)$  其中$p_j=p(X=x_j), j=1,2,...,m$\n",
    "\n",
    "最后引出信息增益。信息增益表示在特征$X$给定的条件下，使得类$Y$的信息不确定性减少的程度。数学精确表述为：特征$A$对数据集$D$的信息增益\n",
    "\n",
    "$G(D,A)$定义为集合$D$的信息熵与特征$A$给定的条件下$D$的条件熵$H(D|A)$之差，即$G(D,A)=H(D)-H(D|A)$，信息论上也称之为互信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. 贝叶斯公式是什么？您如何理解贝叶斯思想？它与频率派有哪些区别？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "贝叶斯公式：\n",
    "$$\n",
    "P(A|B)=\\frac{P(B|A)P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "贝叶斯公式首先是由条件概率公式和乘法公式展开得到的。$P(B|A)$称为似然函数，表示的是在$A$确定下的数据$B$出现的情况，$P(A)$称为先验概\n",
    "\n",
    "率。而贝叶斯学派与频率学派争论的焦点就在于先验分布。贝叶斯学派认为先验分布可以是主观的，它不需要有频率解释。而频率学派则认为，只有在\n",
    "\n",
    "先验分布有一种不依赖主观的意义，且能根据适当的理论或以往的经验决定时，才允许在统计推断中使用先验分布，否则就会丧失客观性。\n",
    "\n",
    "(1)频率论先建立无效模型，然后计算在此无效模型的前提下得到从实际数据中得来的参数的可能性，假如这个可能性很小，我们认为无效模型不成立，\n",
    "\n",
    "从而选择备择模型；而贝叶斯论关注于在当前数据的前提下，某个模型成立的概率，得到的是具体的概率值，而该概率值不用于对某个假说的判断。\n",
    "\n",
    "(2) 频率论对概率的解释是：一个事件在一段较长的时间内发生的频率；贝叶斯论对概率的解释是人们对某事件是否发生的认可程度。\n",
    "\n",
    "(3) 贝叶斯论善于利用过去的知识和抽样数据，而频率论仅仅利用抽样数据。因此贝叶斯推论中前一次得到的后验概率可以作为后一次的先验概率。\n",
    "\n",
    "(4) 对置信区间的不同解释：频率论中95%置信区间解释为：100次抽样计算得到的100个置信区间中有95个包含了总体参数，5个没有，而不能解释成\n",
    "\n",
    "在一次抽样中有95%的可能性包含总体参数。这是由于经典统计中总体的参数是被当作一个恒定值的，不能从概率的角度解释； 贝叶斯论的置信区间恰\n",
    "\n",
    "好可以解释成概率的形式，因为贝叶斯分析中，总体参数是个随机变量，而非恒定值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.请从最大似然的角度去解释逻辑回归,以及逻辑回归的损失函数是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "假设有一个二分类问题，输出为$y∈\\{0,1\\}$\n",
    "\n",
    "而线性模型产生的预测值是$z=ω^Tx+b$,区间范围为$(-\\infty,+\\infty)$,我们希望能够将这样的$z$值转化为对应的0/1值。于是这里就产生了sigmoid\n",
    "\n",
    "函数$φ(z)=\\frac{1}{1+e^{-z}}$首先进行区间映射$(-\\infty,+\\infty)=>(-1,1)$，$φ(z)$可视为类1的后验概率估计$p(y=1|x)$，最终的分类结果表\n",
    "\n",
    "示为$$\n",
    "\\begin{eqnarray}\\widehat{y}=\n",
    "\\begin{cases}\n",
    "1, &if φ(z)≥0.5\\cr 0, &otherwise\\end{cases}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "因此有如下模型定义：\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\begin{cases}\n",
    "p(y=1|x;ω)=φ(z)\\cr p(y=0|x;ω)=1-φ(z)\\end{cases}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "\n",
    "写成一般形式为$p(y|x;ω)=φ(z)^y(1-φ(z))^{(1-y)}$\n",
    "\n",
    "利用最大似然估计有$L(ω)=\\prod_{i=1}^np(y^{(i)}|x^{(i)};ω)=\\prod_{i=1}^n[φ(z^{(i)})^{y^{(i)}}(1-φ(z^{(i)}))^{(1-y^{(i)})}]$\n",
    "\n",
    "取对数$l(ω)=lnL(ω)=\\sum_{i=1}^n[y^{(i)}ln(φ(z^{(i)}))+(1-y^{(i)})ln(1-φ(z^{(i)}))]$\n",
    "\n",
    "$l(ω)$为对数似然函数，应取最大值，故可令逻辑回归的损失函数为：\n",
    "$$\n",
    "J(ω)=-l(ω)=-\\sum_{i=1}^n[y^{(i)}ln(φ(z^{(i)}))+(1-y^{(i)})ln(1-φ(z^{(i)}))]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.试析Min-Max与Z-Score这两种数据缩放各自特点，和为什么树形结构不需要做缩放？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Min-Max标准化： 对于每个属性$x_j$，$j=1,2,...,d$，根据下式计算标准化后的属性值：\n",
    "$$\n",
    "\\widehat{x}_j^{(i)}=\\frac{x_j^{(i)}-min x_j}{max x_j-min x_j},i=1,2,...,N;j=1,2,...,d\n",
    "$$\n",
    "$$\n",
    "\\widehat{X}^{(i)}=(\\widehat{x}_1^{(i)},\\widehat{x}_2^{(i)},...,\\widehat{x}_d^{(i)})^T,i=1,2,...,N\n",
    "$$\n",
    "\n",
    "其中$max x_j=max\\{x_j^{(1)},x_j^{(2)},...,x_j^{(N)}\\}$为属性$x_j$的最大值；$min x_j=min\\{x_j^{(1)},x_j^{(2)},...,x_j^{(N)}\\}$为属\n",
    "\n",
    "性$x_j$的最小值。标准化之后，样本$X^{(i)}$的所有属性值都在$[0,1]$之间。\n",
    "\n",
    "Z_Score标准化： 对于每个属性$x_j$，$j=1,2,...,d$，先计算该属性的平均值和标准差：\n",
    "$$\n",
    "μ_j=\\frac{1}{N}\\sum_{i=1}^Nx_j^{(i)}\n",
    "$$\n",
    "$$\n",
    "σ_j=\\sqrt{\\frac{1}{N}\\sum_{i=1}^N(x_j^{(i)}-μ_j)^2}\n",
    "$$\n",
    "\n",
    "然后根据下式计算标准化后的属性值：\n",
    "$$\n",
    "\\widehat{x}_j^{(i)}=\\frac{x_j^{(i)}-μ_j}{σ_j},i=1,2,...,N;j=1,2,...,d\n",
    "$$\n",
    "$$\n",
    "\\widehat{X}^{(i)}=(\\widehat{x}_1^{(i)},\\widehat{x}_2^{(i)},...,\\widehat{x}_d^{(i)})^T,i=1,2,...,N\n",
    "$$\n",
    "\n",
    "标准化后，样本集的所有属性的均值都为0，标准差均为1。\n",
    "\n",
    "对于树型结构，数值缩放不影响分裂点位置。因为第一步都是按照特征值进行排序的，排序的顺序不变，那么所属的分支以及分裂点就不会有不同。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. 请解释下随机森林和Xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "随机森林：随机森林指的是利用多棵树对样本进行训练并预测的一种分类器，随机森林指的是利用多棵树对样本进行训练并预测的一种分类器。\n",
    "\n",
    "Xgboost：XGBoost是一种特殊的提升树模型，准确地说它是一种梯度提升决策树。传统GBDT算法的目标函数只有损失函数一项，而XGBoost在此基础上进\n",
    "\n",
    "行了改进，增加了正则项以防止模型过度复杂。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>####答卷结束####</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本周课程意见反馈(非必答)\n",
    "请同学围绕以下两点进行回答：\n",
    "- 自身总结：您自己在本周课程的学习，收获，技能掌握等方面进行总结，包括自身在哪些方面存在哪些不足，欠缺，困惑。作为将来回顾学习路径时的依据。\n",
    "- 课程反馈：也可以就知识点，进度，难易度，教学方式，考试方式等等进行意见反馈，督促我们进行更有效的改进，为大家提供更优质的服务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
