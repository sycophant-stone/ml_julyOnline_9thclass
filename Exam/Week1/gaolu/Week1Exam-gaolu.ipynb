{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习九期第一周(机器学习基础)考试\n",
    "#### 考试说明:\n",
    "- 起止时间：请同学在2018年6月25日至7月2日期间完成，最晚提交时间（7月1日24时之前）结束，<b>逾期不接受补考,该考试分数计入平时成绩</b>\n",
    "- 考试方式：请同学<font color=red><b>拷贝</b></font>该试卷至自己姓名的目录后，将文件更名，即上同学姓名拼音后进行作答。格式：Week1Exam-WangWei.ipynb\n",
    "- 提交格式：请同学新建自己姓名全拼的文件夹，将该试卷，数据文件，zip文件等相关考试文件，放置此目录下。将该目录<b>移动</b>至/0.Teacher/Exam/1/目录下\n",
    "- 注意事项：为确保同学们真正了解自身对本周课程的掌握程度，<font color=red><b>请勿翻阅，移动，更改</b></font>其它同学试卷。如发现按0分处理\n",
    "- 请同学在下方同学姓名处填写自己的姓名，批改人和最终得分不用填写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名:gao lu\n",
    "- 批改人：   \n",
    "- 最终得分:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问答题(共10题，每题10分，共计100分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.请分别解释统计机器学习中的输入/输出空间，特征空间，假设空间与参数空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- 输入空间：所有输入可能的取值的集合\n",
    "- 输出空间：所有输出可能的取值的集合\n",
    "- 特征空间：每个具体的输入数据也叫一个实例通常由特征向量表示.所有特征向量存在的空间\n",
    "- 假设空间：由输入空间到输出空间映射的集合就是假设空间.假设空间的确定意味着学习范围的确定. \n",
    "\n",
    "- 参数空间：\n",
    "    - 非概率模型中，使得输入空间$X$中的变量$x$在通过假设空间$\\mathcal{F}$中的函数$f$后得到输出空间$Y$中的$y$成立的函数f的所有参数值（向量）组成的空间\n",
    "    - 概率模型中，使得输出空间$Y$中的变量$y$，在满足输入空间$X$中的变量$x$的条件下，使得$P(y|x)$成立的所有概率模型的参数向量组成的空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.什么是损失函数，并举例有哪些常用的损失函数？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "损失函数为统计学习定义了一种准则，度量模型一次预测的好坏，以学习或选择最优模型。具体来说，监督学习问题是在假设空间$\\mathcal{F}$中选取模型$f$作为决策函数，对于给定的输入$X$，由$f(X)$给出相应的输出$Y$，这个输出的预测值$f(X)$与真实值$Y$的差异，我们定义一个损失函数（loss function），也称作代价函数(cost function)来表示，以度量预测错误的程度。损失函数是$f(X)$和$Y$的非负实值函数，记作$L(Y,f(X))$。\n",
    "\n",
    "常用的损失函数有：\n",
    "- 0-1损失函数：(打不出来不等号，所以用绝对值>0来表示了...)\n",
    "$$L(Y,f(X))=\\{ \\begin{array}{ll} 1, & \\textrm{|Y-f(X)|>0}\\\\ 0, & \\textrm{Y-f(X)=0} \\end{array} $$\n",
    "- 平方损失函数：\n",
    "$$L(Y,f(X))=(Y-f(X))^2$$\n",
    "- 绝对损失函数：\n",
    "$$L(Y,f(X))=|Y-f(X)|$$\n",
    "- 对数损失函数或对数似然损失函数：\n",
    "$$L(Y,f(X))=-\\log{P(Y|X)}$$\n",
    "- hinge loss:\n",
    "$$L(y_i,f(x_i))=\\sum_{j \\neq y_i} \\max(0, f(x_i, \\mathbf{W})_j - f(x_i, \\mathbf{W})_{y_i}+\\Delta)$$\n",
    "其中，$x_i$表示训练集中第$i$个样本数据；$f(x_i, \\mathbf{W})$表示在参数$\\mathbf{W}$下的得分结果向量，其中第$j$类的得分记作$f(x_i, \\mathbf{W})_j$。\n",
    "- 交叉熵损失函数:\n",
    "$$L(y, f(x_i))=-\\log{\\frac{e^{f_{y_i}(x_i)}}{\\sum_j e^{f_j(x_i)}}}$$\n",
    "其中，$f_j(x_i)$表示在参数$W^{(j)}$下的得分结果向量。\n",
    "\n",
    "- Logistic回归损失函数：\n",
    "$$L(y_i, h_\\theta(x_i))=y_i\\log h_\\theta(x_i) + (1-y_i) \\log (1-h_\\theta(x_i))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3请结合公式进行说明结构风险最小化的含义？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "结构风险最小化（structural risk minimization, SRM）一种防止过拟合的策略，等价于正则化（regularization）。结构风险在经验风险上加上表示模型复杂度的正则化项（regularizer）或罚项（penalty term）。在假设空间、损失函数以及训练数据集确定的情况下，结构风险的定义是：\n",
    "$$R_{srm}(f)=\\frac{1}{N}\\sum_{i=1}^N L(y_i, f(x_i)) + \\lambda J(f)$$\n",
    "其中$J(f)$为模型的复杂度，是定义在假设空间上的泛函数。模型$f$越复杂，复杂度$J(f)$越大；模型$f$越简单，复杂度$J(f)$越小。复杂度表示了对复杂模型的惩罚。$\\lambda \\geq 0$是权衡经验风险和模型复杂度的系数。结构风险小需要经验风险与模型复杂度同时小。结构风险小的模型往往对训练数据以及未知的测试数据都有较好的预测。\n",
    "结构风险最小化策略认为结构风险最小的模型是最优的模型。所以求解最优化模型，也就是求解最优化问题：$\\min_{f\\in\\mathcal{F}} R_{srm}(f)$。\n",
    "\n",
    "正则化是结构风险最小化策略的实现，可以是模型参数向量的范数。从贝叶斯估计角度来看，正则化项对应于模型的先验概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.谈谈什么是生成式模型与判别式模型，以及各自特点？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "生成方法学到的模型称为生成模型。生成方法由数据学习联合概率分布$P(X,Y)$，然后求出条件概率分布$P(Y|X)$作为预测模型，即生成模型：$P(Y|X)=\\frac{P(X,Y)}{P(X)}$。这样的方法称为生成方法，是因为模型表示了给定输入$X$产生输出$Y$的生成关系。典型的生成模型有：朴素贝叶斯法、HMM。\n",
    "\n",
    "判别方法学到的模型称为判别模型。判别方法由数据直接学习决策函数$f(X)$或条件概率分布$P(Y|X)$作为预测模型，即判别模型。判别方法关心的是，给定输入$X$，应该预测什么样的输出$Y$。典型的判别模型包括：KNN、感知机、决策树、Logistic回归模型、最大熵模型、SVM、提升方法、CRF等。\n",
    "\n",
    "- 生成模型的特点：\n",
    "    1. 生成方法可以还原出联合概率分布$P(X,Y)$，而判别方法不能；\n",
    "    2. 生成方法的学习收敛速度更快，即当样本容量增加时，学到的模型可以更快地收敛于真实模型；\n",
    "    3. 当存在隐变量时，仍然可以用生成方法，而判别方法就不能用。\n",
    "    4. 由生成模型可以得到判别模型，但由判别模型得不到生成模型。\n",
    "    \n",
    "\n",
    "- 判别模型的特点：\n",
    "    1. 判别方法直接学习的是条件概率$P(Y|X)$或决策函数$f(X)$，直接面对预测，通常学习的准确率更高；\n",
    "    2. 由于直接学习$P(Y|X)$或$f(X)$，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.请解释范数，并写出L1,L2范数的定义公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "范数是具有长度/距离概念的函数。在数学上，范数包括向量范数和矩阵范数，向量范数表征向量空间中向量的大小，矩阵范数表征矩阵引起变化的大小。\n",
    "在机器学习中，我们通常用范数作为正则化项来定义模型的复杂度。\n",
    "\n",
    "- L1范数：$\\|x\\|_1=\\sum_i |x_i|$\n",
    "- L2范数：$\\|x\\|_2=\\sqrt{\\sum_i x_i^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. 什么是信息熵，它的公式是？什么是信息增益?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "信息熵是度量样本集合纯度最常用的一种指标。假定当前样本集合$D$中第$k$类样本所占的比例为$p_k(k=1,2,...,|\\mathcal{Y}|)$, 则$D$的信息熵定义为：\n",
    "$Ent(D)=-\\sum_{k=1}^{|\\mathcal{Y}|} p_k\\log_2{p_k}$。\n",
    "\n",
    "$Ent(D)$的值越小，$D$的纯度越高。\n",
    "\n",
    "信息增益表示得知特征A的信息而使得类别的信息的不确定性减少的程度。\n",
    "- 离散属性$a$有$V$个可能的取值$\\{a^1, a^2, ..., a^V\\}$。用$a$对样本集$D$进行划分, 产生$V$个分支结点, 其中第$v$个分支结点包含了$D$中所有在属性$a$上取值为$a^v$的样本, 记为$D^v$。\n",
    "- 属性$a$对样本集$D$进行划分所获得的的信息增益：$$G(D,a)=Ent(D)-\\sum_{v=1}^V \\frac{|D^v|}{|D|}Ent(D^v)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. 贝叶斯公式是什么？您如何理解贝叶斯思想？它与频率派有哪些区别？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "贝叶斯公式：$$P(Y|X)=\\frac{P(X|Y)P(Y)}{P(X)}$$.\n",
    "它由联合概率公式推导出来：$$P(Y,X)=P(X|Y)P(Y)=P(Y|X)P(X)$$.\n",
    "其中，$P(Y)$为先验概率，$P(Y|X)$为后验概率，$P(X|Y)$为似然函数，$P(Y,X)$为联合概率。\n",
    "\n",
    "在统计学习中，Y表示样本（类别标签），X表示参数（具有某种特征）。贝叶斯思想，是将参数视作随机变量，把样本视作固定的，着眼点在于参数空间，重视参数的分布，通过参数的先验分布结合样本信息得到参数的后验分布。也就是说，贝叶斯方法把计算[具有某特征的条件下属于某类]的概率转换成计算[属于某类的条件下具有某种特征]的概率。\n",
    "\n",
    "频率派把需要推断的参数视作固定且未知的，而样本是随机的，着眼点在于样本空间，有关的概率计算都是针对样本的分布。频率派并不关心参数空间的所有细节，，他们相信数据都是在这个空间里的“某个”参数值下产生的（虽然你不知道那个值是什么），所以他们的方法论一开始就是从“哪个值最有可能是真实值”这个角度出发的。于是就有了最大似然（maximum likelihood）以及置信区间（confidence interval）这样的东西，他们关心的就是我有多大把握去圈出那个唯一的真实参数。而贝叶斯学派恰恰相反，他们关心参数空间里的每一个值，他们认为参数空间里的每个值都有可能是真实模型使用的值，区别只是概率不同而已。于是他们才会引入先验分布（prior distribution）和后验分布（posterior distribution）这样的概念来设法找出参数空间上的每个值的概率。\n",
    "比如，有一个后验分布是双峰的，频率学派的方法会去选这两个峰当中较高的那一个对应的值作为他们的最好猜测，而贝叶斯学派则会同时报告这两个值，并给出对应的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.请从最大似然的角度去解释逻辑回归,以及逻辑回归的损失函数是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "在二分类问题中，我们设给定的训练数据集$T=\\{(x_1,y_1), (x_2,y_2), ...,(x_N,y_N)\\}, x_i\\in\\mathbf{R}^n, y_i\\in\\{0,1\\}$. \n",
    "假定概率分布：$P(Y=1|x)=\\pi(x), P(Y=0|x)=1-\\pi(x)$.\n",
    "\n",
    "则似然函数为：$$\\Pi_{i=1}^N[\\pi(x_i)]^{y_i}[1-\\pi(x_i)]^{1-y_i}$$\n",
    "对数似然函数为：\n",
    "$$L(\\theta)=\\sum_{i=1}^N [y_i \\log\\pi(x_i) + (1-y_i)\\log(1-\\pi(x_i))]\\\\\n",
    "=\\sum_{i=1}^N [y_i \\log\\frac{\\pi(x_i)}{1-\\pi(x_i)} + \\log(1-\\pi(x_i))] \\\\\n",
    "=\\sum_{i=1}^N [y_i (\\theta^T x_i) - \\log(1+\\exp(\\theta^T x_i))$$\n",
    "\n",
    "对$L(\\theta)$求极大值，得到参数$\\theta$的估计值。\n",
    "这样，问题就变成了以对数似然函数为目标的最优化问题，逻辑回归中通常采用的方法是梯度下降法或拟牛顿法。\n",
    "\n",
    "假设$\\theta$的极大似然估计值是$\\hat{\\theta}$，那么学到的逻辑回归模型为：\n",
    "$$P(Y=1|x)=\\frac{e^{\\theta^T x}}{1+e^{\\theta^T x}} \\\\\n",
    "P(Y=0|x)=\\frac{1}{1+e^{\\theta^T x}}$$\n",
    "\n",
    "##### Logistic回归损失函数：\n",
    "$$L(y_i, h_\\theta(x_i))=y_i\\log h_\\theta(x_i) + (1-y_i) \\log (1-h_\\theta(x_i))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.试析Min-Max与Z-Score这两种数据缩放各自特点，和为什么树形结构不需要做缩放？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Min-Max\n",
    "min-max(Min-max normalization)也叫离差标准化，是对原始数据的线性变换，使结果落到[0,1]区间，并且经标准化的数据都是没有单位的纯数量。转换函数如下：\n",
    "$$x*=\\frac{x-\\min}{\\max-\\min}$$\n",
    "其中max为样本数据的最大值，min为样本数据的最小值。离差标准化是消除量纲（单位）影响和变异大小因素的影响的最简单的方法。这种方法有一个缺陷就是当有新数据加入时，可能导致max和min的变化，需要重新定义。\n",
    "\n",
    "##### Z-score\n",
    "Z-score(zero-mean normalization)也叫标准差标准化，经过处理的数据符合标准正态分布，即均值为0，标准差为1，其转化函数为：\n",
    "$$x*=\\frac{x-\\mu}{\\sigma}$$\n",
    "其中$\\mu$为所有样本数据的均值，$\\sigma$为所有样本数据的标准差。经标准化的数据都是没有单位的纯数量。对变量进行的标准差标准化可以消除量纲（单位）影响和变量自身变异的影响。但有人认为经过这种标准化后，原来数值较大的的观察值对分类结果的影响仍然占明显的优势，应该进一步消除大小因子的影响。尽管如此，它还是当前用得最多的数据标准化方法。标准化后的数据包括负值，如果需要处理后的数据均为正，则不适用。\n",
    "\n",
    "\n",
    "##### 树形结构不需要做缩放\n",
    "因为第一步都是按照特征值进行排序的，排序的顺序不变，那么所属的分支以及分裂点就不会有不同。对于线性模型，比如说LR，我有两个特征，一个是(0,1)的，一个是(0,10000)的，这样运用梯度下降时候，损失等高线是一个椭圆的形状，这样我想迭代到最优点，就需要很多次迭代，但是如果进行了归一化，那么等高线就是圆形的，那么SGD就会往原点迭代，需要的迭代次数较少。\n",
    "树模型是不能进行梯度下降的，因为树模型是阶跃的，阶跃点是不可导的，并且求导没意义，所以树模型（回归树）寻找最优点是通过寻找最优分裂点完成的。而数值缩放，不影响分裂点位置。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. 请解释下随机森林和Xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### 随机森林\n",
    "随机森林是由多个决策树构成(M个）， 每个决策树单独对数据进行预测， 最终结果取这M个决策树中类别最多的那一类。不同分类器是相互独立的。 \n",
    "\n",
    "在随机森林中，对每个决策树，加入了两个随机特征：\n",
    "1. 对样本进行随机采样：对于N个样本的训练集，采取有放回抽样，这样可以保证M个决策树的样本不同，防止over-fitting.\n",
    "2. 对特征进行随机采样：机森林中的子树的每一个分裂过程并未用到所有的待选特征，而是从所有的待选特征中随机选取一定的特征，之后再在随机选取的特征中选取最优的特征。这样能够使得随机森林中的决策树都能够彼此不同，提升系统的多样性，从而提升分类性能。\n",
    "\n",
    "##### Xgboost\n",
    "Xgboost扩展和改进了GDBT，相比于GDBT，Xgboost算法更快，准确率也相对高一些。\n",
    "1. Gradient Boosting Decision Tree (GBDT)是以决策树（CART）为基学习器的GB算法，是Adaboost的回归版本，把残差作为下一轮的学习目标，最终的结果由有加权和值得到，不是简单的多数投票：\n",
    "$$G(x)=\\sum a_m G_m(x)$$\n",
    "2. Xgboost是GDBT的高效实现，Xgboost的基学习器可以是CART，也可以是线性分类器：\n",
    "    - 在目标函数中加上了正则化项，当基学习器是CART时，正则化项与树的叶子结点的数量T和叶子结点的值有关：\n",
    "    $$L(\\phi)=\\sum_i l(\\hat{y_i},y_i)+\\sum_k \\Omega(f_k)$$，其中$\\Omega(f)=\\gamma T + \\frac{1}{2}\\lambda||\\omega||^2$。\n",
    "    - GB中使用损失函数对$f(x)$的一阶导数计算出伪残差用于学习生成$f_m(x)$，Xgboost不仅使用到了一阶导数，还使用二阶导数。\n",
    "    \n",
    "        第t次的loss：\n",
    "        $$L^{(t)}=\\sum_{i=1}^n l(y_i, \\hat{y}_i^{(t-1)}+f_t(x_t))+\\Omega(f_t)$$\n",
    "\n",
    "        对上式做二阶泰勒展开：g为一阶导数，h为二阶导数\n",
    "        $$L^{(t)}\\simeq \\sum_{i=1}^n [l(y_i, \\hat{y}_i^{(t-1)}) + g_i f_t(x_i) + \\frac{1}{2}h_i f_t^2(x_i)]+\\Omega(f_t)$$\n",
    "        其中,$g_i = \\partial_{\\hat{y}_i^{(t-1)}}l(y_i, \\hat{y}_i^{(t-1)})$, $h_i=\\partial^2_{\\hat{y}_i^{(t-1)}}l(y_i, \\hat{y}_i^{(t-1)})$.\n",
    "\n",
    "    - CART回归树中寻找最佳分割点的衡量标准是最小化均方差，Xgboost寻找分割点的标准是：遍历所有特征，求出每个特征对应的最佳分裂点，把增益最大的特征和分裂点作为下一次特征分裂:\n",
    "    $$Gain_{split}=\\frac{1}{2}[\\frac{(\\sum_{i\\in I_L} g_i)^2}{\\sum_{i\\in I_L}h_i + \\lambda} + \\frac{(\\sum_{i\\in I_R} g_i)^2}{\\sum_{i\\in I_R}h_i + \\lambda} - \\frac{(\\sum_{i\\in I} g_i)^2}{\\sum_{i\\in I}h_i + \\lambda}]-\\gamma$$\n",
    "    \n",
    "    - 为防止算法贪婪，将树长到最大的深度，然后递归地从底部向上剪枝。\n",
    "    \n",
    "Xgboost算法的步骤和GB基本相同，都是首先初始化为一个常数，GB是根据一阶导数，Xgboost是根据一阶导数$g_i$和二阶导数$h_i$，迭代生成基学习器，相加更新学习器。\n",
    "\n",
    "Xgboost与gdbt除了上述三点的不同，Xgboost在实现时还做了许多优化：\n",
    "\n",
    "    a.在寻找最佳分割点时，考虑传统的枚举每个特征的所有可能分割点的贪心法效率太低，xgboost实现了一种近似的算法。大致的思想是根据百分位法列举几个可能成为分割点的候选者，然后从候选者中根据上面求分割点的公式计算找出最佳的分割点。\n",
    "    b. Xgboost考虑了训练数据为稀疏值的情况，可以为缺失值或者指定的值指定分支的默认方向，这能大大提升算法的效率。\n",
    "    c. 特征列排序后以块的形式存储在内存中，在迭代中可以重复使用；虽然boosting算法迭代必须串行，但是在处理每个特征列时可以做到并行。\n",
    "    d. 按照特征列方式存储能优化寻找最佳的分割点，但是当以行计算梯度数据时会导致内存的不连续访问，严重时会导致cache miss，降低算法效率。论文中中提到，可先将数据收集到线程内部的buffer，然后再计算，提高算法的效率。\n",
    "    e. Xgboost 还考虑了当数据量比较大，内存不够时怎么有效的使用磁盘，主要是结合多线程、数据压缩、分片的方法，尽可能的提高算法的效率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>####答卷结束####</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本周课程意见反馈(非必答)\n",
    "请同学围绕以下两点进行回答：\n",
    "- 自身总结：您自己在本周课程的学习，收获，技能掌握等方面进行总结，包括自身在哪些方面存在哪些不足，欠缺，困惑。作为将来回顾学习路径时的依据。\n",
    "- 课程反馈：也可以就知识点，进度，难易度，教学方式，考试方式等等进行意见反馈，督促我们进行更有效的改进，为大家提供更优质的服务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$\\phi \\Phi \\emptyset$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
