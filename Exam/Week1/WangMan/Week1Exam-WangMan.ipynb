{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习九期第一周(机器学习基础)考试\n",
    "#### 考试说明:\n",
    "- 起止时间：请同学在2018年6月25日至7月2日期间完成，最晚提交时间（7月1日24时之前）结束，<b>逾期不接受补考,该考试分数计入平时成绩</b>\n",
    "- 考试方式：请同学<font color=red><b>拷贝</b></font>该试卷至自己姓名的目录后，将文件更名，即上同学姓名拼音后进行作答。格式：Week1Exam-WangWei.ipynb\n",
    "- 提交格式：请同学新建自己姓名全拼的文件夹，将该试卷，数据文件，zip文件等相关考试文件，放置此目录下。将该目录<b>移动</b>至/0.Teacher/Exam/1/目录下\n",
    "- 注意事项：为确保同学们真正了解自身对本周课程的掌握程度，<font color=red><b>请勿翻阅，移动，更改</b></font>其它同学试卷。如发现按0分处理\n",
    "- 请同学在下方同学姓名处填写自己的姓名，批改人和最终得分不用填写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名:王曼\n",
    "- 批改人：   \n",
    "- 最终得分:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问答题(共10题，每题10分，共计100分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.请分别解释统计机器学习中的输入/输出空间，特征空间，假设空间与参数空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "输入/输出空间：将输入与输出的所有取值的集合分别称为输入和输出空间。\n",
    "\n",
    "特征空间：每个具体的输入是一个实例，通常由特征向量表示，所有特征向量所存在的空间为特征空间。\n",
    "\n",
    "假设空间：监督学习的目的是找到由输入到输入的映射，也就是模型，模型属于由输入到输出空间的映射的集合，该集合就是假设空间，\n",
    "         假设空间确定意味着学习范围确定。\n",
    "\n",
    "参数空间：假设要求解的参数为$\\theta$,则$\\theta$的取值范围即为参数空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.什么是损失函数，并举例有哪些常用的损失函数？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数：用来度量预测值f(X)和真实值Y的不同程度，用L(Y,f(X))，损失函数越小，模型的性能越好。\n",
    "\n",
    "① 0-1损失函数：$$L(Y,f(X))=1,Y\\not=f(X);L(Y,f(X))=0,Y=f(X)$$\n",
    "② 平方损失函数：$$L(Y|f(X))=\\sum_N (Y−f(X))^2$$\n",
    "③ 对数损失函数：$$L(Y,P(Y|X))=−logP(Y|X)$$\n",
    "④ 绝对值损失函数：$$L(Y,f(X))=|Y-f(X)|$$\n",
    "⑤ Hinge损失函数：$$L(y)=max(0,1-t\\cdot y)$$其中，y表示预测值，t为目标值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3请结合公式进行说明结构风险最小化的含义？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "期望损失：由于模型遵循联合分布$P(X,Y)$，所以损失函数的期望为：$$R_{exp}(f)=E_p[L(Y,f(x))]=\\int_{xy}{L(y,f(x))P(x,y)dxdy}$$\n",
    "\n",
    "这是理论上模型f(X)关于联合分布P(X,Y)的平均意义下的损失，称为风险函数或期望损失，学习的目标就是选择期望风险最小化的模型，但是一方面不知道联合分布P(X,Y)，所以无法求得期望风险，监督学习成为一个病态问题。\n",
    "\n",
    "经验风险：给定一个训练数据集$Y={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}$，模型f(X)关于训练数据集的平均损失称为经验风险或经验损失，记作$R_{emp}$:$$R_{emp}(f)=\\frac{1}{N}\\sum_{i=1}^NL(y_i,f(x_i))$$\n",
    "\n",
    "两者的联系：根据大数定律，当样本容量N趋于无穷时，经验风险趋于期望风险，但现实情况下样本数量有限，所以要对经验风险进行矫正。\n",
    "\n",
    "监督学习的两个策略：经验风险最小化、结构风险最小化\n",
    "\n",
    "经验风险最小化（empirical risk minimization，ERM）：\n",
    "\n",
    "在假设空间、损失函数及训练数据集确定的情况下，经验风险函数$ R_{emp}(f)=\\frac{1}{N}∑_{i=1}{N}L(y_i,f(x_i))$就可以确定，ERM的策略认为，经验风险最小的模型就是最优模型，按照经验风险最小化求最优模型就是求解最优化问题：$$min\\frac{1}{N}\\sum_{i=1}^N L(y_i ,f(x_i )) $$\n",
    "\n",
    "- 当样本容量足够大时，ERM能保证有很好的学习效果;\n",
    "- 当样本容量很小时，ERM学习效果就未必很好，会产生“过拟合”现象\n",
    "\n",
    "结构风险最小化（structural risk minimization，SRM）:是为了防止过拟合提出的策略，等价于正则化（regularization）。\n",
    "\n",
    "结构：在经验风险上附加表示模型复杂度的正则化项或罚项，$$R_{srm} (f)=\\frac{1}{N}\\sum_{i=1}^N L(y_i ,f(x_i ))+λJ(f) $$\n",
    "\n",
    "J(f):模型复杂度，模型越复杂，值越大，该项表示对模型复杂度的惩罚\n",
    "$\\lambda$：系数，用于权衡经验风险和模型复杂度，结构风险最小则需要经验风险和模型复杂度同时小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.谈谈什么是生成式模型与判别式模型，以及各自特点？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "监督学习的任务就是学习一个模型，应用该模型对给定的输入预测相应的输出，这个模型的一般形式为决策函数$f(X)$或条件概率分布$P(Y|X)$。\n",
    "\n",
    "监督学习方法又可以分别生成方法和判别方法，其所生成的模型分别为生成模型和判别模型。\n",
    "\n",
    "- 生成模型：由数据学习联合概率分布$P(X,Y)$，然后求出条件概率分布$P(Y|X)$作为预测的模型，即为生成的模型：$$P(Y|X)=\\frac{P(X,Y)}{P(X)}$$\n",
    "\n",
    "之所以称为生成模型，是因为模型表示了给定输入X产生Y的生成关系。\n",
    "    \n",
    "- 判别模型：由数据直接学习决策函数$f(X)$，或求解条件概率分布$P(Y|X)$作为预测模型。也可以称为条件模型或概率模型，利用正负例的分类标签，\n",
    "\n",
    "求得判别模型的边缘分布，目标函数直接对应于分类准确率。其关心的是给定的输入X，应该预测什么样的输出Y。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.请解释范数，并写出L1,L2范数的定义公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "范数包括向量范数和矩阵范数，向量范数表示向量空间中向量的大小，矩阵范数表示矩阵变化大小。\n",
    "\n",
    "- L0范数：向量中非0元素的个数,$$||x||_0= (i|x_i\\not=0)$$\n",
    "\n",
    "- L1范数：向量x中非零元素的绝对值之和，$$||x||_1=\\sum_i|x_i|$$\n",
    "\n",
    "由于L1范数的天然性质，对L1优化的解是一个稀疏解，因此L1范数也被叫做稀疏规则算子。通过L1可以实现特征的稀疏，去掉一些没有信息的特征。\n",
    "\n",
    "- L2范数：向量元素的平方之和再开方（也就是模值），$$||x||_2=\\sqrt{\\sum_i|x_i|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. 什么是信息熵，它的公式是？什么是信息增益?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "信息熵：信息熵是度量样本集合的“纯度”的指标，假定当前样本集合D中第k类样本所占的比例为$p_k$，则D的信息熵为：\n",
    "$$Ent(D)=-\\sum_{k=1}^{|y|}p_klog_2p_k$$\n",
    "\n",
    "信息增益：以信息熵为基础，计算当前划分对信息熵所造成的影响，即划分前后信息熵的变化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. 贝叶斯公式是什么？您如何理解贝叶斯思想？它与频率派有哪些区别？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "贝叶斯公式：$$P(A|B)=\\frac{P(B|A)P(A)}{P(B)}$$\n",
    "\n",
    "我们把P(A)称为”先验概率”（Prior probability）：即在B事件发生之前，我们对A事件概率的一个判断。\n",
    "\n",
    "P(A|B)称为”后验概率”（Posterior probability）：即在B事件发生之后，我们对A事件概率的重新评估。\n",
    "\n",
    "P(B|A)/P(B)称为”可能性函数”（Likelyhood）：这是一个调整因子，使得预估概率更接近真实概率。\n",
    "\n",
    "所以，条件概率可以理解成下面的式子：后验概率=先验概率∗调整因子 \n",
    "\n",
    "\n",
    "贝叶斯的思想：先预估一个先验概率，然后加入实验结果，看该实验到底是增强还是削弱了先验概率，由此得到更加接近真实的后验概率。也就是当我们要预测一个事物发生的概率，要首先根据已有的经验和知识推断一个先验概率，然后在新证据不断累积的情况下调整该概率，得到一个事件发生的概率。\n",
    "\n",
    "贝叶斯派：\n",
    "\n",
    "① 认为参数是随机变量，而样本X是固定的，由于样本是固定的，所以重点研究参数的分布\n",
    "\n",
    "② 先验概率对参估计有重要的作用\n",
    "\n",
    "频率派：\n",
    "\n",
    "① 认为模型的参数是固定的，一个模型无数次抽样之后，所有的参数都是一样的\n",
    " \n",
    "② 任何模型都不存在先验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.请从最大似然的角度去解释逻辑回归,以及逻辑回归的损失函数是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "极大似然估计：给定了一系列结果之后，估计哪一组参数的可能性最大，也就是判定使用哪一组参数得到该结果的可能性最大。\n",
    "\n",
    "逻辑回归：对线性分类模型的结果利用sigmoid函数映射为概率,处理二分类问题。\n",
    "\n",
    "\n",
    "逻辑回归的假设函数：$$h_\\theta(X)=g(X^T\\theta)=\\frac{1}{1+e^{-X^T\\theta}}$$\n",
    "\n",
    "意义：样本X在使用权重$\\theta$的时候，结果为1的概率\n",
    "\n",
    "即$$P(Y_i=1|X_i,\\theta)=h_\\theta (X)$$ $$P(Y_i=0|X_i,\\theta)=1-h_\\theta (X)$$\n",
    "\n",
    "我们可知逻辑回归实际模型为二项分布或伯努利分布。\n",
    "\n",
    "由于$Y_i$的特殊性，上述模型等价于$$P(Y_i|X_i,\\theta)=(h_\\theta (X))^{Y_i}(1-h_\\theta (X))^{1-Y_i}$$\n",
    "\n",
    "等式两边取对数，可以将连乘变为加法：$$P(Y|X,\\theta)=\\sum_1^m Y_i log(h_\\theta (X_i ))+(1−Y_i )log(1−h_\\theta(X_i)) $$\n",
    "\n",
    "逻辑回归的损失函数：\n",
    "$$Cost(h_\\theta (X_i),Y_i)=\\begin{cases}−log(h_\\theta (X_i )),&\\text{$Y_i=1$} \\\\−log(1-h_\\theta (X_i )),&\\text{$Y_i=0$}\\end{cases}$$\n",
    "\n",
    "对大小为m的数据集取平均：\n",
    "$$J(\\theta)=\\frac{1}{m}\\sum_{i=1}^mCost(h_\\theta (X_i),Y_i)=-\\frac{1}{m}[\\sum_{i=1}^mY_i log(h_\\theta (X_i ))+(1-Y_i) log(1-h_\\theta (X_i ))]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.试析Min-Max与Z-Score这两种数据缩放各自特点，和为什么树形结构不需要做缩放？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Min-Max:对原始数据进行线性变换，映射到[0,1]之间，也称为离差标准化，缺陷是当有新数据的加入时，需要更新min和max\n",
    "\n",
    " $$x^*=\\frac{x-min}{max-min}$$\n",
    " \n",
    " 在不涉及距离度量、协方差计算、不要求分布为正态分布的时候可以使用该归一化方法。\n",
    " \n",
    " \n",
    "- Z-Score:该方法基于原始数据的均值和方差，适用于最大值和最小值未知或有超出离群数据的情况，处理后的数据符合标准正态分布，均值为0，标准差为1。\n",
    "\n",
    " $$x^*=\\frac{x-\\mu}{\\delta}$$\n",
    " \n",
    " 在分类、距离等算法中，需要使用距离来度量相似性的时候，或利用PCA进行降维的时候利用该方法表现更好。\n",
    " \n",
    "\n",
    "- 为什么树形结构不需要缩放：\n",
    "\n",
    " 树形结构是根据特征的概率分布选择一个让数据最能区分开的特征进行当前分裂，归一化之后顺序不变，所以该特征所属分支和分裂的位置不会改变。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. 请解释下随机森林和Xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Bagging：为了防止模型在学习的过程中，将数据中的噪声点当做特征点进行学习，从而导致的过拟合现象，使用有放回抽样的方法，从总数为m的原始训练集中抽取小于m的子样本集，采样T次得到T个不同的子集，用该T个子集分别训练得到T个弱学习器，之后通过对T个学习器的结果进行投票（分类）或平均（回归）的方法来确定最终的输出。\n",
    "\n",
    "随机森林：基于CART树模型的Bagging优化版本\n",
    "\n",
    "- 对训练数据进行采样，得到T个包含m个样本的采样集$D_m$\n",
    "\n",
    "- 训练T个决策树，在训练模型节点的时候，在节点数所有样本特征中选择一部分样本特征，在这些随机选择的部分样本特征中选择一个最优的特征来做决策树的左右子树划分。\n",
    "\n",
    "- 用于分类：投票决定；用于回归：平均输出\n",
    "\n",
    "Xgboost（eXtreme Gradient Boosting）：所应用的算法就是gradient boosting decision tree，既可以用于分类也可以用于回归问题中。\n",
    "\n",
    "Boosting：每一步都产生一个弱预测模型，然后加权累加到总模型中，没一步弱模型的生成的依据都是损失函数的负梯度方向，这样若干步之后就会达到逼近损失函数局部最小值的目标。Boost是一个累加模型，由若干个基函数机器权值乘积之和的累加。\n",
    "\n",
    "Gradient Boosting：就是通过加入新的弱学习器，来努力纠正前面所有弱学习器的残差，最终这样多个学习器相加在一起用来进行最终预测，准确率就会比单独的一个要高,之所以称为 Gradient，是因为在添加新模型时使用了梯度下降算法来最小化的损失。但是该过程是一个串行过程，计算复杂度高，效率低。\n",
    "\n",
    "Xgboost：\n",
    "\n",
    "- 能利用cpu的多线程，适当的改进了gradient boosting，增加了剪枝，控制了模型的复杂度，降低了模型的方差，是模型更简单且防止了过拟合。\n",
    "\n",
    "- 同时支持CART树和线性分类器作基分类器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>####答卷结束####</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本周课程意见反馈(非必答)\n",
    "请同学围绕以下两点进行回答：\n",
    "- 自身总结：您自己在本周课程的学习，收获，技能掌握等方面进行总结，包括自身在哪些方面存在哪些不足，欠缺，困惑。作为将来回顾学习路径时的依据。\n",
    "- 课程反馈：也可以就知识点，进度，难易度，教学方式，考试方式等等进行意见反馈，督促我们进行更有效的改进，为大家提供更优质的服务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
