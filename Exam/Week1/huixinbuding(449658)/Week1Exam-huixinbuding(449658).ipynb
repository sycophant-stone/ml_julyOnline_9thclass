{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习九期第一周(机器学习基础)考试\n",
    "#### 考试说明:\n",
    "- 起止时间：请同学在2018年6月25日至7月2日期间完成，最晚提交时间（7月1日24时之前）结束，<b>逾期不接受补考,该考试分数计入平时成绩</b>\n",
    "- 考试方式：请同学<font color=red><b>拷贝</b></font>该试卷至自己姓名的目录后，将文件更名，即上同学姓名拼音后进行作答。格式：Week1Exam-WangWei.ipynb\n",
    "- 提交格式：请同学新建自己姓名全拼的文件夹，将该试卷，数据文件，zip文件等相关考试文件，放置此目录下。将该目录<b>移动</b>至/0.Teacher/Exam/1/目录下\n",
    "- 注意事项：为确保同学们真正了解自身对本周课程的掌握程度，<font color=red><b>请勿翻阅，移动，更改</b></font>其它同学试卷。如发现按0分处理\n",
    "- 请同学在下方同学姓名处填写自己的姓名，批改人和最终得分不用填写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名: 卉心布丁（449658）\n",
    "- 批改人：   \n",
    "- 最终得分:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问答题(共10题，每题10分，共计100分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.请分别解释统计机器学习中的输入/输出空间，特征空间，假设空间与参数空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "输入/输出空间：即输入/输出可能的取值的集合。输入空间中的每条记录就是一个样本(sample)\n",
    "特征空间：特征是对原始数据的抽象，特征提取就是从原始数据空间（输入空间）提取特征，将原始特征映射到更高维的空间（特征空间）。\n",
    "假设空间，是由所有的假设组成的空间。而我们的学习过程，就是在假设空间进行搜索的过程，搜索目标就是找到与训练集“匹配”的假设。\n",
    "参数空间：所有参数可能取值组成的空间。我们通过在参数空间中搜索，找出一组是的我们损失函数最小的参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.什么是损失函数，并举例有哪些常用的损失函数？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "损失函数，或者说是代价函数，是预测值与真实值之间的非负实值函数。\n",
    "常用的有：\n",
    "（1）0-1 损失函数\n",
    "<img src=\"http://latex.codecogs.com/gif.latex?\\mathit{L(Y,f(X))&space;=&space;\\left\\{\\begin{matrix}1&space;,&space;Y\\neq&space;f(x)&space;\\\\&space;0,&space;Y&space;=&space;f(x)&space;\\end{matrix}}\" title=\"\\mathit{L(Y,f(X)) = \\left\\{\\begin{matrix}1 , Y\\neq f(x) \\\\ 0, Y = f(x) \\end{matrix}}\" />\n",
    "（2）平方损失函数\n",
    "<img src=\"http://latex.codecogs.com/gif.latex?\\mathit{L(Y,f(X))&space;=&space;(Y-f(X))^2}\" title=\"\\mathit{L(Y,f(X)) = (Y-f(X))^2}\" />\n",
    "（3）绝对损失函数\n",
    "<img src=\"http://latex.codecogs.com/gif.latex?\\mathit{L(Y,f(X))&space;=&space;\\left&space;|Y-f(X)&space;\\right&space;|}\" title=\"\\mathit{L(Y,f(X)) = \\left |Y-f(X) \\right |}\" />\n",
    "（4）对数损失函数\n",
    "<img src=\"http://latex.codecogs.com/gif.latex?\\mathit{L(Y,f(X))&space;=&space;-\\log&space;P(Y|X)}\" title=\"\\mathit{L(Y,f(X)) = -\\log P(Y|X)}\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3请结合公式进行说明结构风险最小化的含义？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "结构风险最小化（SRM），是为了防止过拟合而提出的策略。SRM等价于正则化。结构风险在经验风险上加上表示模型复杂的正则化项或罚项。\n",
    "\n",
    "$$\\ R_{srm}(f) = \\frac{1}{N}\\sum_{i=1}^{N}L(y_{i},f(x_{i}))+ \\lambda J(f) $$\n",
    "\n",
    "其中，$ \\frac{1}{N}\\sum_{i=1}^{N}L(y_{i},f(x_{i})) $ 为经验风险, $ \\ J(f)$ 我模型复杂度，定义在假设空间上的泛函。模型$\\ f $ 越复杂，复杂度 $ \\ J(f)$ 越大。即：$ \\ J(f)$ 表示对模型复杂的惩罚。$ \\lambda$ 用于权衡经验风险与模型复杂度。\n",
    "结构风险小，需要经验风险和模型复杂度同时小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.谈谈什么是生成式模型与判别式模型，以及各自特点？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "监督学习方法分为生成方法和判别方法。所学到的模型分别为生成模型和判别模型。\n",
    "生成方法是有数据学习联合概率分布 $ P(X,Y)$ ,然后求出条件概率分别 $P(Y|X)$ 作为预测的模型，即生成模型为：\n",
    "$$ P(Y|X) = \\frac{P(X,Y)}{P(X)} $$ \n",
    "这个方法之所以称为生成方法，是因为模型表示了给定输入$ X$产生输出$Y$的生成关系。\n",
    "典型的生成模型有：朴素贝叶斯法和隐马尔可夫模型\n",
    "\n",
    "判别方法是有数据直接学习决策函数$f(X)$或者条件概率分布$P(Y|X)$作为预测的模型，即为判别模型。判别模型关心的是，对给定的输入$X$,应该预测什么样的$Y$.\n",
    "\n",
    "生成方法的特点：生成方法可以还原出联合概率分布$P(X,Y)$,而判别方法不能。生成方法的学习收敛速度更快，即当样本增加时，学到的模型可以更快地收敛于真实模型。\n",
    "判别方法的特点：判别方法直接学习的是条件概率$P(X,Y)$或者决策函数$f(X)$，直接面对预测，往往学习的准确率更高。由于直接学习$P(X,Y)$和$f(X)$，可以对数据进行各种程度上的抽象、定义特征并使用特征。可以简化学习问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.请解释范数，并写出L1,L2范数的定义公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "范数是一个函数，在矢量空间内所有矢量赋予非零的正长度或大小。\n",
    " \n",
    "$L_{1}$范数为：$ \\left \\|\\omega   \\right \\| = \\left | x_1\\right | +\\left | x_2 \\right |+...+ \\left | x_n \\right |$\n",
    "\n",
    "$L_{2}$范数为：$ \\left \\|\\omega  \\right\\|^2 =  (\\left | x_1\\right |^2 + \\left | x_2\\right |^2 +...+\\left | x_n\\right |^2)^\\frac{1}{2}   $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. 什么是信息熵，它的公式是？什么是信息增益?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "信息熵：表示随机变量不确定性的度量。设$X$是一个取值有限个值的离散随机变量，其概率分别为：\n",
    "$$  P(X=x_i)=p_i $$\n",
    "则随机变量$X$的信息熵为：\n",
    "$$ H(X)=-\\sum_{i=1}^{n}p_ilogP_I $$\n",
    "一般来讲，熵越大，随机变量的不确定性越大\n",
    "\n",
    "信息增益：表示得知特征$X$的信息而使得类$Y$的信息的不确定性减少的程度。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. 贝叶斯公式是什么？您如何理解贝叶斯思想？它与频率派有哪些区别？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "贝叶斯公式为：\n",
    "$$ P(B_i|A) = \\frac{P(B_i)P(A|B_i)}{\\sum_{j=1}^{n}P{(B_j)}P{(A|B_j)}}    $$\n",
    "\n",
    "贝叶斯决策是利用概率的不同分布决策与相应的决策代价之间的定量折中。决策问题可以用概率的形式来描述，并且假设所有有关的概率结构均已知。贝叶斯思维是根据后验概率来进行决策的。\n",
    "\n",
    "* 频率派认为抽样是无限的.在无限次抽样当中,对于决策的规则可以很精确;而贝叶斯思维则认为世界无时无刻不在改变，未知的变量和事件都有一定的概率。这种概率会随时改变这个世界的状态(前面提到的后验概率是先验概率的修正)。\n",
    "* 频率派认为模型的参数是固定的, 一个模型在无数次的抽样过后, 所有的参数都应该是一样的; 而贝叶斯思维则认为数据应该是固定的. 我们的规律从我们对这个世界的观察和认识中得来. 我们看到的即是真实的, 正确的. 应该从观测的事物来估计参数.\n",
    "* 频率派认为任何模型都不存在先验; 而先验在贝叶斯思维当中有着重要的作用.\n",
    "* 频率派主张的是一种评价范式. 它没有先验, 更加的客观. 贝叶斯思维主张的是一种模型方法. 通过建立未知参数的模型. 在没有观测到样本之前, 一切参数都是不确定的. 使用观测的样本值来估计参数. 得到的参数带入模型使当前模型最佳的拟合观测到的数据.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.请从最大似然的角度去解释逻辑回归,以及逻辑回归的损失函数是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "逻辑回归，采用的$ h_\\theta(X) = g(X^T\\theta) = \\frac{1}{1+exp{(-X^T\\theta)}}$ ,如果直接采用线性回归那样用$J(\\theta) = \\frac{1}{2m}\\sum_{i=0}^{m}(Y_i-\\tilde{Y}_{i})^2 $ 。则很有可能不是一个凸函数，无法使用梯度下降法。\n",
    "\n",
    "而对于$ h_\\theta(X)$,代表是的样本$Y$在权重$\\theta$的时间结果为1的概率。即$ P(Y_i =1|X_i,\\theta) = h_\\theta(X) $,则可以得到,$ P(Y_i =0|X_i,\\theta) = 1-h_\\theta(X) $。那么，也可得到：\n",
    "$$  P(Y_i |X_i,\\theta) = (h_\\theta(X))^{Y_i}(1-h_\\theta(X))^{1-Y_i} $$\n",
    "同样，因为样本之间是独立的，那也得到：\n",
    "$$ P(Y |X,\\theta) =  \\prod_{i=1}^{m}P(Y_i |X_i,\\theta) $$\n",
    "同时两边取对数，则可以得到：\n",
    "$$ P(Y |X,\\theta) = \\sum_{i=1}^{m}Y_ilog(h_\\theta(X_i))+(1-Y_i)log(1 - h_\\theta(X_i))   $$\n",
    "\n",
    "而，逻辑回归的损失函数为：\n",
    "$$ cost(h_\\theta(X_i), Y_i) =  \\left\\{\\begin{matrix}\n",
    " -log(h_\\theta(X_i))&  Y_i = 1\\\\ \n",
    " -log(1-h_\\theta(X_i)) & Y_i = 0 \n",
    "\\end{matrix}\\right. $$\n",
    "可以看到，逻辑回归的本质，其实就是极大似然函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.试析Min-Max与Z-Score这两种数据缩放各自特点，和为什么树形结构不需要做缩放？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Min-Max与Z-Score是两种数据标准化（Normalization）的方法。\n",
    "Min-Max变换公式为：\n",
    "$$ y_i = \\frac{x_i - min_{1\\leq j\\leq n}{x_j}}{max_{1\\leq j\\leq n}{x_j}-min_{1\\leq j\\leq n}{x_j}} $$\n",
    "\n",
    "这个也叫离差标准化，是对原始数据的线性变换，使结果映射到$[0,1]$区间。\n",
    "\n",
    "Z_Score变换公式为：\n",
    "$$ y_i = \\frac{x_i - \\bar{x}}{s},这里\\bar{x}=\\frac{1}{n}\\sum_{i=1}^{n}x_i, s=\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n}(x_i-\\bar{x})^2} $$\n",
    "\n",
    "这种方法是基于原始数据的均值和标准差进行数据的标准化。将原始值$x$使用z-score标准化到${x}'$。\n",
    "z-score标准化方法适用于属性最大值和最小值未知的情况，或有超出取值范围的离群数据的情况。\n",
    "\n",
    "树形结构的分割是一系列的水平线和垂直线，不存在两者的交换，在考虑一个维度时，不需要考虑另一个维度，因此，它不受缩放的影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. 请解释下随机森林和Xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "随机森林是由多个决策树构成（M个），每个决策树单独对数据进行预测，最终结果去和M个决策树中类别最多的那一类。不同的分类器是相互独立的。\n",
    "在随机森林中，对每个决策树，加入两个随机特征：\n",
    "1. 随机选择样本：对与$N$个样本的训练集，采用**采用有放回抽样**，boostraping $N$个样本，这样可以保证M个决策树的样本不一样，防止over-fitting.\n",
    "2. 对待选特征进行随机选取。与数据集类似，随机森林中的子树的每一个分裂过程**并未用到所有的待选特征**，而是从所有的待选特征中随机选取一定的特征，之后再在随机选取的特征中选取最优的特征。这样能够使得随机森林中的决策树都能够彼此不同，提升系统的多样性，从而提升分类性能。\n",
    "\n",
    "关于Xgboost\n",
    "\n",
    "1. Xgboost，是以CART为基分类器的GB算法，其建模思路：就是在每轮迭代中生成一棵新的回归树，并综合所有回归树的结果，使预测值越来越逼近真实值。\n",
    "2. Xgboost能自动利用cpu的多线程，而且适当改进了gradient boosting，加了剪枝，控制了模型的复杂程度。\n",
    "3. Xgboost的工具支持并行，这样大大减少了计算时间。\n",
    "4. 抽样方面，Xgboost借鉴了随机森林的做法，支持列抽样，并且在代价函数中加入了正则项，用于控制模型的复杂度。降低过拟合，减少了计算量。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>####答卷结束####</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本周课程意见反馈(非必答)\n",
    "请同学围绕以下两点进行回答：\n",
    "- 自身总结：您自己在本周课程的学习，收获，技能掌握等方面进行总结，包括自身在哪些方面存在哪些不足，欠缺，困惑。作为将来回顾学习路径时的依据。\n",
    "- 课程反馈：也可以就知识点，进度，难易度，教学方式，考试方式等等进行意见反馈，督促我们进行更有效的改进，为大家提供更优质的服务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$\\phi \\Phi \\emptyset$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
