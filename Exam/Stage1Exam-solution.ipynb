{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习第九期第一阶段考试-参考答案\n",
    "#### 参考答案说明:\n",
    "\n",
    "- 来源：来自于网络搜索，面试，题库，笔记，书籍整理，当期及往期优秀同学贡献等途径\n",
    "- 使用：该答案供参考，非唯一固定答案"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问答题(共10题，每题10分，共计100分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.请说明线性分类器与非线性分类器有哪些区别，具体应用场景有哪些不同？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### 线性分类器：   \n",
    "线性分类是指用一个超平面能将正负样本区分开，表达式为$f(x)=wx+b$，对于二维的情况，超平面可以理解为一条直线，如一次函数。线性分类器的分类算法是基于一个线性的预测函数，决策的边界是平的，比如直线和平面。   \n",
    "   \n",
    "   \n",
    "##### 非线性分类器：   \n",
    "分类界面没有限制，可以是一个曲面，或者是多个超平面的组合，在二维的情况下可以是一个曲线。\n",
    "\n",
    "\n",
    "##### 两者区别：   \n",
    "线性分类器速度快、编程方便，但是可能拟合效果不会很好；非线性分类器编程复杂，但是效果好拟合能力强。   \n",
    "\n",
    "\n",
    "##### 应用场景   \n",
    "特征比数据量还大时，选择线性分类器，因为维度高的时候，数据一般在维度空间里面会比较稀疏，很有可能线性可分。\n",
    "对于维度很高的特征，选择线性分类器，理由同上。\n",
    "对于维度极低的特征，选择非线性分类器，因为低维空间可能很多特征都跑到一起了，导致线性不可分。\n",
    "\n",
    "\n",
    "##### 常见应用:   \n",
    "常见的线性分类器有：LR,贝叶斯分类，单层感知机、线性回归   \n",
    "常见的非线性分类器：决策树、RF、GBDT、多层感知机   \n",
    "SVM两种都有（看线性核还是高斯核）   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.逻辑回归中为什么常常要对特征进行离散化?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- 简化计算：   \n",
    "离散化后数据表示为稀疏向量，内积乘法运算速度快，计算结果方便存储，容易扩展。\n",
    "\n",
    "\n",
    "- 克服异常值对模型的扰动：   \n",
    "离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据会给模型造成很大的干扰。\n",
    "\n",
    "\n",
    "- 提升模型表达：   \n",
    "逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力\n",
    "\n",
    "\n",
    "- 引入新特征：   \n",
    "离散化后可以进行特征交叉，由M+N个变量变为M\\*N个变量，引入非线性，提升表达能力。\n",
    "\n",
    "\n",
    "- 稳定模型表现：   \n",
    "特征离散化后，模型会更稳定。比如对用户年龄进行离散化处理（也可以认为是一种聚类）使得特征对模型的影响更加稳定。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.随机森林如何处理缺失值？随机森林如何评估特征重要性？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### RandomForest包里有两种补全缺失值的方法：\n",
    "\n",
    "方法一（na.roughfix）简单粗暴，对于训练集,同一个类别标签下的数据，如果是分类变量缺失，用众数补上，如果是连续型变量缺失，用中位数补。   \n",
    "方法二（rfImpute）这个方法计算量大，至于比方法一好坏不好判断。先用na.roughfix补上缺失值，然后构建森林并计算proximity matrix，再回头看缺失值，如果是分类变量，则用没有缺失的观测实例的proximity中的权重进行投票。如果是连续型变量，则用proximity矩阵进行加权平均的方法补缺失值。然后迭代4-6次，这个补缺失值的思想和KNN有些类似。\n",
    "\n",
    "\n",
    "#### 随机森林如何评估特征重要性？   \n",
    "- 思路：   \n",
    "随机森林评估每个特征在随机森林中的每颗树上做了多大的贡献，然后取个平均值，最后比一比特征之间的贡献大小。  \n",
    "\n",
    "- 方法：   \n",
    "   1) Decrease GINI： 对于回归问题，直接使用argmax(Var−VarLeft−VarRight)作为评判标准，即当前节点训练集的方差Var减去左节点的方差VarLeft和右节点的方差VarRight。      \n",
    "   2) Decrease Accuracy：对于一棵树Tb(x)，我们用OOB(out-of-bag)样本可以得到测试误差1；然后随机改变OOB样本的第j列：保持其他列不变，对第j列进行随机的上下置换，得到误差2。至此，我们可以用误差1-误差2来刻画变量j的重要性。基本思想就是，如果一个变量j足够重要，那么改变它会极大的增加测试误差；反之，如果改变它测试误差没有增大，则说明该变量不是那么的重要。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 请说明梯度下降算法如何实现，以及它与牛顿法的不同？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "梯度下降法是用当前位置负梯度方向作为搜索方向，该方向为当前位置的最快下降方向，越接近目标值，步长越小，前进越慢。最终求解在给定模型给定数据之后的目标函数在参数空间中搜索找到的极值。\n",
    "\n",
    "其迭代过程为：$\\theta^n=\\theta^{n-1}-\\alpha \\nabla J(\\theta)$\n",
    "\n",
    "#### 与牛顿法的不同\n",
    "梯度下降的目的是直接求解目标函数极小值，而牛顿法则变相地通过求解目标函数一阶导为零的参数值，进而求得目标函数最小值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 简要谈下您理解的的机器学习领域的正则化？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "在机器学习模型中的正则化是指结构风险最小化策略的实现，是在经验风险上加一个正则化项或罚项。   \n",
    "正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值就越大。   \n",
    "\n",
    "比如，正则化项可以是模型参数向量的范数。不管是L1还是L2，都是针对模型中参数过大的问题引入惩罚项。   \n",
    "\n",
    "而在深度学习中，要优化的变成了一个个矩阵，参数变得多出了几个数量级，过拟合的可能性也相应的提高了。而要惩罚的是神经网络中每个神经元的权重大小，从而避免网络中的神经元走极端抄近路。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6 带核的SVM为什么能分类非线性问题？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "SVM通过核函数映射函数将样本的原始特征映射到一个使样本线性可分的更高维的空间中。将在低维线性不可分问题转化为高维空间线性问题,并通过核函数降低两点之间内积精确计算阶段的成本，最终得到超平面是高维空间的线性分类平面。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7 请举例有哪些常用核函数，以及核函数的条件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "多项式核:$K(x_1, x_2)=(x_1 * x_2 + c)^{d}$\n",
    "\n",
    "高斯核:$K(x_1, x_2)=exp(-\\frac{\\gamma * ||x_1 - x_2||^2}{2\\sigma^2})$\n",
    "\n",
    "sigmoid核：$K(x_1, x_2) = tanh(\\beta*x_i^Tx_j+\\theta)$\n",
    "\n",
    "核函数必须是对称函数，核矩阵半正定。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.什么是偏差与方差？当你模型受到低偏差和高方差困扰时，如何解决？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 方差Variance和偏差Bias ：\n",
    "偏差：  指某种模型的学习与表达能力在给定数据集上出现的误差，表现为模型选择上的不正确。   \n",
    "方差：  指某种模型在给定数据集上表现的的不稳定性，如受到数据分布，超参数等影响而表现出的模型不稳定。\n",
    "\n",
    "#### 模型受到低偏差和高方差困扰时的解决思路：\n",
    "说明模型选择问题不大，模型是有可能受到了欠/过拟合的影响，可以思考在数据维度，特征选择，正负样本分布，集成学习上加以改善解决。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.请说明熵、联合熵、条件熵、相对熵、互信息的定义（要求公式），以及您对这些定义的理解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "熵（entropy）：用来衡量整个系统的总体信息量\n",
    "$$ H(X) = -\\sum_{i=1}^{n}P(X_{k}) logP(X_{k})$$ \n",
    "\n",
    "\n",
    "联合熵：对于服从联合分布P(x,y)的一对离散型随机变量，联合熵定义为  \n",
    "$$ H(X,Y)=\\sum_{x\\in X}\\sum_{y\\in Y}P(x,y)\\log{P(x,y)=-E[logP(X,Y)]}$$\n",
    "联合熵大于或等于这两个变量中任一个的独立熵，少于或等于独立熵的和。该不等式有且只有在和均为统计独立的时候相等。\n",
    "这表明，两个变量关联之后不确定性会增大，但是又由于相互有制约关系，不确定程度小于单独两个变量的不确定度之和。 \n",
    "\n",
    "\n",
    "条件熵：就是在事件X的前提下，事件Y的熵：  \n",
    "$$ H(X|Y)=\\sum_{x\\in X,y\\in Y}P(x,y)logP(X|Y)$$  \n",
    "可以看出在Y的条件下，X的不确定度是多少。\n",
    "\n",
    "\n",
    "相对熵（KL距离)：两个概率密度函数p(x)和q(x)之间的相对熵定义为  \n",
    "$$D(p||q)=\\sum_{x\\in X}p(x)log\\frac{p(x)}{q(x)} =E_{p}[\\log\\frac{p(x)}{q(x)}]$$  \n",
    "描述两个随机变量距离的度量也叫交叉熵。相对熵越大，两个函数差异越大；反之，相对熵越小，两个函数差异越小。\n",
    "\n",
    "\n",
    "互信息：给定两个随机变量X，Y，联合密度函数为P(x,y)，其边际函数分别为p(x),q(x)，则互信息为P(x,y) 与p(x)q(x)之间的相对熵。  \n",
    "$I(X,Y) =\\sum_{x\\in X}\\sum_{y\\in Y}log\\frac{P(x,y)}{q(x)p(y)}=H(X)-H(X|Y)$    \n",
    "互信息是在了解Y的前提下，消除X的不确定度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 请简要说明您对EM算法的理解，并列举有哪些常用的采用EM 算法求解的模型？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 对EM的理解：\n",
    "有时因为样本的产生和隐含变量有关（隐含变量是不能观察的），而求模型的参数时一般采用最大似然估计，由于含有了隐含变量，所以对似然函数参数求导是求不出来的，这时可以采用EM算法来求模型的参数的（对应模型参数个数可能有多个）\n",
    "\n",
    "- EM算法一般分为2步：\n",
    "    - E步：选取一组参数，求出在该参数下隐含变量的条件概率值；   \n",
    "    - M步：结合E步求出的隐含变量条件概率，求出似然函数下界函数（本质上是某个期望函数）的最大值。   \n",
    "    - 重复上面2步直至收敛。\n",
    "  \n",
    "#### 常用的采用EM算法求解的模型：\n",
    "高斯混合模型， K−Means，混合朴素贝叶斯模型，因子分析模型等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>####答卷结束####</h1></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
